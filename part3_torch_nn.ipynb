{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TORCH.NN\n",
    "---\n",
    "> 신경망은 데이터에 대한 연산을 수행하는 계층(layer)/모듈(module)로 구성되어 있습니다.  \n",
    "> torch.nn 네임스페이스는 신경망을 구성하는데 필요한 모든 구성 요소를 제공합니다.  \n",
    "> PyTorch의 모든 모듈은 nn.Module 의 하위 클래스(subclass) 입니다.   \n",
    "> 신경망은 다른 모듈(계층; layer)로 구성된 모듈입니다. 이러한 중첩된 구조는 복잡한 아키텍처를 쉽게 구축하고 관리할 수 있습니다.  \n",
    "\n",
    "\n",
    "### 1. nn.Module\n",
    "---\n",
    "nn.Module는 모든 신경망 모듈의 기본 클래스이다.\n",
    "\n",
    "모든 모델은 이 클래스의 하위 클래스로 지정해야 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용 예시\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5)\n",
    "        self.conv2 = nn.Conv2d(20, 20, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        return F.relu(self.conv2(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Dezero와의 연관성\n",
    "---\n",
    "\n",
    "사실 Dezero에서 Module을 찾아볼 순 없었다.\n",
    "\n",
    "그래서 직접 코드를 뜯어본 결과\n",
    "\n",
    "> 코드 : https://pytorch.org/docs/stable/generated/torch.nn.Module.html\n",
    "\n",
    "가장 유사한 것이 Dezero의 Layer 클래스이다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self._params = set()\n",
    "\n",
    "    def __setattr__(self, name, value): # object에 존재하는 속성의 값을 바꾸거나 새로운 속성을 생성할때마다 호출되는 특수 메서드\n",
    "        if isinstance(value, (Parameter, Layer)):\n",
    "            self._params.add(name)\n",
    "        super().__setattr__(name, value)\n",
    "\n",
    "    def __call__(self, *inputs): #함수 형태로 불리는 순간 forward 실행\n",
    "        outputs = self.forward(*inputs) # 입력값을 순전파로 받아서 출력값 생성\n",
    "        if not isinstance(outputs, tuple):\n",
    "            outputs = (outputs, )\n",
    "        self.inputs = [weakref.ref(x) for x in inputs]\n",
    "        self.outputs = [weakref.ref(y) for y in outputs]\n",
    "        return outputs if len(outputs) > 1 else outputs[0]\n",
    "\n",
    "    def forward(self,inputs): #순전파는 이 클래스를 상속받은 유도 클래스에서 정의해야함\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def params(self): # 파라미터를 반환함\n",
    "        for name in self._params:\n",
    "            obj = self.__dict__[name]\n",
    "\n",
    "            if isinstance(obj, Layer):\n",
    "                yield from obj.params()\n",
    "            else:\n",
    "                yield obj\n",
    "\n",
    "    def cleargrads(self): # 파라미터 초기화\n",
    "        for param in self.params():\n",
    "            param.cleargrad()\n",
    "\n",
    "class Model(Layer): #그림 그리는 기능 추가\n",
    "    def plot(self, *inputs, to_file='model.png'):\n",
    "        y = self.forward(*inputs)\n",
    "        return utils.plot_dot_graph(y, verbose=True, to_file=to_file) #verbose 가 True면 ndarray 인스턴스의 형상과 타입도 계산 그래프에 표시해줌\n",
    "\n",
    "class MLP(Model):\n",
    "    def __init__(self, fc_output_sizes, activation = F.sigmold):\n",
    "        # fc_output_sizes는 신경망을 구성하는 완전연결계층들의 출력 크기를 튜플 또는 리스트로 지정합니다. \n",
    "        # activation은 활성화 함수를 지정합니다\n",
    "        super().__init__()\n",
    "        self.activation = activation\n",
    "        self.layers = []\n",
    "\n",
    "        for i, out_size in enumerate(fc_output_sizes):\n",
    "            layer = L.Linear(out_size)\n",
    "            setattr(self,'l' + str(i), layer) #l1 = L.Linear(out_size) 처럼 객체에 변수 저장중 \n",
    "            self.layers.append(layer)\n",
    "\n",
    "    def forward(self,x):\n",
    "        for i  in self.layers[:-1]:\n",
    "            x = self.activation(l(x))\n",
    "        return self.layers[-1](x)\n",
    "\n",
    "\n",
    "class Linear(Layer): # Layer를 상속받아 사용하는 클래스 예시\n",
    "    def __init__(self, out_size, nobias=False, dtype=np.float32, in_size=None):\n",
    "        super().__init__() # 필수다 이거 안해주면 기초 클래스의 생성자를 가져오지 않는다는 건데 보통 기초 클래스에 속성을 정의하니까 그거 통쨰로 안쓴다고 하는 것과 다른게 없다\n",
    "        self.in_size = in_size\n",
    "        self.out_size = out_size\n",
    "        self.dtype = dtype\n",
    "\n",
    "        self.W = Parameter(None, name='W')\n",
    "        if self.in_size is not None: # 만일 in_size가 지정되어 있지 않다면 나중으로 연기\n",
    "            self._init_W()\n",
    "        if nobias:\n",
    "            self.b = None\n",
    "        else:\n",
    "            self.b = Parameter(np.zeros(out_size, dtype=dtype), name = 'b')\n",
    "\n",
    "    def _init_W(self):\n",
    "        I, O = self.in_size, self.out_size\n",
    "        W_data = np.random(I,O).astype(self.dtype) * np.sprt(1/I)\n",
    "        self.W.data = W_data\n",
    "\n",
    "    def forward(self,x):\n",
    "        if self.W.data is None:\n",
    "            self.in_size = x.shape[1]\n",
    "            self._init_W()\n",
    "\n",
    "        y = F.linear(x, self.W, self.b)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "왜 우리는 Layer 클래스를 만들었을까?\n",
    "\n",
    "간단하게 말하자면 신경망은 매개변수(경사하강법 등의 최적화 기법에 의해 갱신되는 변수, '가중치'와 '편향'이 이에 해당)를 다뤄야 하는데 \n",
    "\n",
    "신경망 층이 깊어지면 깊어질수록 매개변수가 어마어마하게 많아질거다\n",
    "\n",
    "그런데 그 매개변수를 모두 코드에서 따로 관리해주자니 너무 복잡하다.\n",
    "\n",
    "따라서 매개변수를 담는 구조를 따로 만들어서 매개변수 관리를 자동화 하는 과정을 위해 만든게 Layer 클래스이다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. pytorch의 module\n",
    "---\n",
    "\n",
    "module 클래스를 상속받아 정의할때 유도 클래스는 반드시 __init__과 forward()는 반드시 정의하도록 되어 있다. (당연한 것이 모듈을 받은 모델이 함수처럼 실행될때 forward()를 이용하니까....)\n",
    "\n",
    "예를 들면 다음과 같이 이용한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\luili\\Desktop\\coding_study\\Pytorch_study\\필기\\part3_nn_Module.ipynb 셀 7\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/luili/Desktop/coding_study/Pytorch_study/%ED%95%84%EA%B8%B0/part3_nn_Module.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mNeuralNetwork\u001b[39;00m(nn\u001b[39m.\u001b[39mModule):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/luili/Desktop/coding_study/Pytorch_study/%ED%95%84%EA%B8%B0/part3_nn_Module.ipynb#X21sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/luili/Desktop/coding_study/Pytorch_study/%ED%95%84%EA%B8%B0/part3_nn_Module.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m         \u001b[39msuper\u001b[39m(NeuralNetwork, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "근데 파이토치는 파이토치 나름의 복잡한 시스템을 가진다.\n",
    "> Module에 관한 파이토치 글 : https://pytorch.org/docs/stable/generated/torch.nn.Module.html\n",
    "\n",
    "어마어마한 수의 메서드를 같이 상속받지만 보통은 __init__과 forward()만 받는다고 생각하면 편하다.\n",
    "\n",
    "참고로 반.드.시 super().__init__을 해줘야 하는데 간단하게 Dezero의 예시만 보더라도 self._params = set()을 가져오기 위함이었다.\n",
    "\n",
    "파이토치는 보다 복잡한 시스템으로 모듈을 만들기 때문에 정상적인 모듈을 만들고 싶다면 그냥 기초클래스의 생성자를 죄다 가져와야 정상적인 모듈이 만들어진다.\n",
    "\n",
    "참고로 생성자 안가져오면 'AttributeError: cannot assign module before Module.__init__() call' 이란 오류가 뜨면서 제발 가져오라고 한다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. torch.nn의 구성\n",
    "---\n",
    "\n",
    "> https://pytorch.org/docs/stable/nn.html\n",
    "\n",
    "torch.nn\n",
    "\n",
    "- Containers\n",
    "\n",
    "- Convolution Layers\n",
    "\n",
    "- Pooling layers\n",
    "\n",
    "- Padding Layers\n",
    "\n",
    "- Non-linear Activations (weighted sum, nonlinearity)\n",
    "\n",
    "- Non-linear Activations (other)\n",
    "\n",
    "- Normalization Layers\n",
    "\n",
    "- Recurrent Layers\n",
    "\n",
    "- Transformer Layers\n",
    "\n",
    "- Linear Layers\n",
    "\n",
    "- Dropout Layers\n",
    "\n",
    "- Sparse Layers\n",
    "\n",
    "- Distance Functions\n",
    "\n",
    "- Loss Functions\n",
    "\n",
    "- Vision Layers\n",
    "\n",
    "- Shuffle Layers\n",
    "\n",
    "- DataParallel Layers (multi-GPU, distributed)\n",
    "\n",
    "- Utilities\n",
    "\n",
    "- Quantized Functions\n",
    "\n",
    "- Lazy Modules Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 다양한 Layer\n",
    "---\n",
    "1. nn.Flatten\n",
    "\n",
    "    Flatten 람수는 완전연결계층에 이미지 입력을 위하여 이미지 데이터를 변환해주는 함수이다.\n",
    "\n",
    "    이미지 데이터는 일반적으로 다차원 데이터로 이루어져 있다. 입력 이미지를 평탄하게 즉, 1차원 배열로 만드는 함수이다.\n",
    "\n",
    "    API - torch.nn.Flatten(start_dim=1, end_dim=- 1)\n",
    "    \n",
    "    + parameters\n",
    "\n",
    "        - start_dim (int) : 첫번째 축 (default = 1)\n",
    "\n",
    "        - end_dim (int) : 마지막 축 (default = -1)\n",
    "\n",
    "    + Examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(32, 1, 5, 5)\n",
    "# With default parameters\n",
    "m = nn.Flatten()\n",
    "output = m(input)\n",
    "output.size()\n",
    "# torch.Size([32, 25])\n",
    "# With non-default parameters\n",
    "m = nn.Flatten(0, 2)\n",
    "output = m(input)\n",
    "output.size()\n",
    "torch.Size([160, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. nn.Linear\n",
    "\n",
    "    Linear 계층 저장된 가중치와 편향을 사용하여 입력에 선형 변환을 적용하는 모듈\n",
    "\n",
    "    Dezero에서 만든 그 모델과 같은 역할을 한다. 다만 다른것은 Dezero의 Linear는 입력값을 딱히 안줘도 호출되는 순간 자동으로 입력값을 설정하게끔 했지만 파이토치에는 그 기능이 없다.\n",
    "\n",
    "    API - torch.nn.Linear(in_features, out_features, bias=True, device=None, dtype=None)\n",
    "    \n",
    "    + parameters\n",
    "\n",
    "        - in_features (int) – 입력 셈플의 크기\n",
    "\n",
    "        - out_features (int) – 출력 샘플의 크기\n",
    "\n",
    "        - bias (bool) - False로 설정하면 레이어는 가산 편향을 학습하지 않습니다. Default : True\n",
    "\n",
    "    + examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Linear(20, 30) # 가중치를 (20 x 30)으로 설정해서 (a x 20)이 들어오면 가중치를 곱해 (a x 30)으로 변환함\n",
    "input = torch.randn(128, 20)\n",
    "output = m(input)\n",
    "print(output.size()) # torch.Size([128, 30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. nn.Sequential\n",
    "\n",
    "    > nn.Sequential 은 순서를 갖는 모듈의 컨테이너입니다.  \n",
    "    > 데이터는 정의된 것과 같은 순서로 모든 모듈들을 통해 전달됩니다.  \n",
    "    > 순차 컨테이너(sequential container)를 사용하여 아래의 seq_modules 와 같은 신경망을 빠르게 만들 수 있습니다.  \n",
    "\n",
    "    방금전까지만 해도 '아 내가 따로 클래스 정의해서 레이어를 모아야 하나?' 라는 고민을 했는데 바로 해결되었다\n",
    "\n",
    "    API - torch.nn.Sequential(*args: Module)\n",
    "        - torch.nn.Sequential(arg: OrderedDict[str, Module])\n",
    "\n",
    "    + Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "          nn.Conv2d(1,20,5),\n",
    "          nn.ReLU(),\n",
    "          nn.Conv2d(20,64,5),\n",
    "          nn.ReLU()\n",
    "        )\n",
    "\n",
    "model = nn.Sequential(OrderedDict([\n",
    "          ('conv1', nn.Conv2d(1,20,5)),\n",
    "          ('relu1', nn.ReLU()),\n",
    "          ('conv2', nn.Conv2d(20,64,5)),\n",
    "          ('relu2', nn.ReLU())\n",
    "        ]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 대단한 착각을 하시고 계시군요\n",
    "---\n",
    "지금 정리해둔게 완전 엉망이다...\n",
    "\n",
    "자 정리하겠다.\n",
    "\n",
    "계층은 계산 그래프(매개변수와 함수를 활용한)와 매개변수를 품고 있는 것이다.\n",
    "\n",
    "Dezero에서 Layer라는 클래스를 만들어 규칙에 맞춰서 다양한 계층을 생성할 수 있다. \n",
    "\n",
    "근데 Dezero에서 계층에서 그림 그리는 기능만 추가했더니 그걸 신경망 모델의 틀이라고 하고 있다.\n",
    "\n",
    "그렇다!\n",
    "\n",
    "계층이라고 했긴 했는데, 신경망 모델 또한 계층인 것이다.(!!!)\n",
    "\n",
    "계층에 다양한 계층을 넣어서 더 큰 계층을 만들고 그것을 신경망이라고 할 수 있는것!\n",
    "\n",
    "그렇기에 신경망을 제작하는 것과 계층을 만드는 방법은 다른게 없다.\n",
    "\n",
    "물론 신경망과 계층은 서로 구현 형식이 살짝 다르다.\n",
    "\n",
    "계층은 파라미터(매개변수)를 관리하기 위해서 계산그래프와 파라미터가 있는 것이고\n",
    "\n",
    "신경망은 파라미터가 없다\n",
    "\n",
    "딱 그 정도 차이이다. \n",
    "\n",
    "![](./%EC%9D%B4%EB%AF%B8%EC%A7%80/image_21.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "수식(함수)라고 했는데 이미 텐서에서 사용하는 +, - , *, / 는 이미 Function이라는 클래스를 상속받아서 순전파와 역전파가 있는 클래스이다. 즉, 보통의 연산자가 아니란 것\n",
    "\n",
    "그렇기 때문에 계층에서 따로 역전파(backward)를 정의하지 않아도 자동적으로 역전파가 흘러간다. 순전파만 형성해도 알아서 역전파가 흘러갈 것이란 뜻(순전파를 흘려야 계산그래프가 형성되기 때문...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('Pytorch_studing')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0 (default, Nov 15 2020, 08:30:55) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d060f0828fc2b117241290c294bd7cf3e183c40a44f56e1a2627712ee0c7af9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
