{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.optim\n",
    "---\n",
    "\n",
    "SGD등의 파라미터 최적화 알고리즘이 구현\n",
    "\n",
    "학습 단계(loop)에서 최적화는 세단계로 이뤄집니다\n",
    "\n",
    "1. optimizer.zero_grad()를 호출하여 모델 매개변수의 변화도를 재설정합니다. 기본적으로 변화도는 더해지기(add up) 때문에 중복 계산을 막기 위해 반복할 때마다 명시적으로 0으로 설정합니다.\n",
    "    - 변화도가 더해지는 이유는 Dezero에서 동일한 변수를 사용하여 덧셈을 하면 제대로 미분을 못하기 때문이다. 일반적으로 더해지게 해서 해결하는 것이다\n",
    "    #\n",
    "\n",
    "2. loss.backwards()를 호출하여 예측 손실(prediction loss)을 역전파합니다. PyTorch는 각 매개변수에 대한 손실의 변화도를 저장합니다.\n",
    "\n",
    "\n",
    "3. 변화도를 계산한 뒤에는 optimizer.step()을 호출하여 역전파 단계에서 수집된 변화도로 매개변수를 조정합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. (들어가기 앞서) 하이퍼파라미터\n",
    "---\n",
    "> 하이퍼파라미터(Hyperparameter)는 모델 최적화 과정을 제어할 수 있는 조절 가능한 매개변수입니다.  \n",
    "> 서로 다른 하이퍼파라미터 값은 모델 학습과 수렴율(convergence rate)에 영향을 미칠 수 있습니다.\n",
    "\n",
    "- 학습 시에는 다음과 같은 하이퍼파라미터를 정의합니다:\n",
    "\n",
    "    - 에폭(epoch) 수 - 데이터셋을 반복하는 횟수\n",
    "\n",
    "    - 배치 크기(batch size) - 매개변수가 갱신되기 전 신경망을 통해 전파된 데이터 샘플의 수\n",
    "\n",
    "    - 학습률(learning rate) - 각 배치/에폭에서 모델의 매개변수를 조절하는 비율. 값이 작을수록 학습 속도가 느려지고, 값이 크면 학습 중 예측할 수 없는 동작이 발생할 수 있습니다.\n",
    "\n",
    "### 2. Dezero의 Optimizer\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Optimizer:\n",
    "    def __init__(self):\n",
    "        self.target = None\n",
    "        self.hooks = []\n",
    "\n",
    "    def setup(self, target): #매개변수를 갖는 클래스(Model 또는 Layer)를 인스턴스 변수인 target으로 설정합니다\n",
    "        self.target = target\n",
    "        return self\n",
    "\n",
    "    def updata(self):\n",
    "        #None 이외의 매개변수를 리스트에 모아줌\n",
    "        params = [p for p in self.target.params() if p.grad is not None]\n",
    "\n",
    "        for f in self.hooks:\n",
    "            f(params)\n",
    "\n",
    "        for param in params:\n",
    "            self.update_one(param)\n",
    "\n",
    "    def update_one(self, param): # 구체적인 매개변수 갱신을 위한 함수\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def add_hook(self, f): #전처리를 수행하는 함수\n",
    "        self.hooks.append(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기본적으로 제공하는 Optimizer을 상속받아서 각자의 최적화 기법을 구현한다. \n",
    "\n",
    "pytorch의 경우도 마찬가지로 기본 클래스인 torch.optim.Optimizer(params, defaults)을 상속받아서 각자의 최적화 기법을 구현한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. pytorch의 다양한 최적화 기법\n",
    "---\n",
    "> 자세한 사항 : https://pytorch.org/docs/stable/optim.html\n",
    "\n",
    "- Adadelta : Adadelta 알고리즘을 구현합니다.\n",
    "\n",
    "- Adagrad : Adagrad 알고리즘을 구현합니다.\n",
    "\n",
    "- Adam : Adam 알고리즘을 구현합니다.\n",
    "\n",
    "- AdamW : AdamW 알고리즘을 구현합니다.\n",
    "\n",
    "- SparseAdam : 희소 텐서에 적합한 Adam 알고리즘의 지연 버전을 구현합니다.\n",
    "\n",
    "- Adamax : Adamax 알고리즘(무한대 표준에 기반한 Adam의 변형)을 구현합니다.\n",
    "\n",
    "- ASGD : 평균 확률적 경사 하강법을 구현합니다.\n",
    "\n",
    "- LBFGS : minFunc 에서 크게 영감을 받은 L-BFGS 알고리즘을 구현 합니다.\n",
    "\n",
    "- NAdam : NAdam 알고리즘을 구현합니다.\n",
    "\n",
    "- RAdam : RAdam 알고리즘을 구현합니다.\n",
    "\n",
    "- RMSprop : RMSprop 알고리즘을 구현합니다.\n",
    "\n",
    "- Rprop : 탄력적인 역전파 알고리즘을 구현합니다.\n",
    "\n",
    "- SGD : 확률적 경사 하강법을 구현합니다(선택적으로 모멘텀 포함)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 사용법\n",
    "---\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate) # 아담으로 옵티마이저 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파이토치 권고 코딩 스타일\n",
    "---\n",
    "```py\n",
    "# 데이터로더로부터 데이터와 정답을 받아옴\n",
    "for data, label in DataLoader():\n",
    "    # 모델의 예측값 계산\n",
    "    prediction = model(data)\n",
    "\n",
    "    # 손실 함수를 이용해 오차 계산\n",
    "    loss = LossFunction(prediction, label)\n",
    "\n",
    "    # 오차 역전파\n",
    "    loss.backward()\n",
    "\n",
    "    # 신경망 가중치 수정\n",
    "    optimizer.step()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('Pytorch_studing')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d060f0828fc2b117241290c294bd7cf3e183c40a44f56e1a2627712ee0c7af9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
