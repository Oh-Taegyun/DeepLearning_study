딥러닝 사례로는 비디오 게임이나, 바둑과 같은 보드 게임이 유명하지만, 그 외에 로봇 제어, 자율 주행 등 다양한 사례가 있다.

### 1. 보드 게임
---
바둑, 장기, 오셀로 등의 보드 게임은 다음과 같은 특성을 지니고 있다.

- 보드의 모든 정보를 알 수 있음(완전 정보)
- 한쪽이 승리하면 다른 쪽은 패배함(제로섬 zero-sum)
- 상태 전이에 우연이라는 요소가 없음(결정적)

이런 성질을 지닌 게임에서 중요한 것은 '수 읽기'이다. "내가 이 수를 두면 상대가 저 수를 둘 것이고, 그러면 나는 이 수로 받아칠 것이다"처럼 앞으로 일어날 일을 다양하게 예측해볼수록 더 좋은 수를 찾을 수 있다. 보드 게임에서 '수 읽기'는 다음 그림처럼 트리 구조로 표현할 수 있다.

![[Pasted image 20240630223354.png]]

그림과 같이 보드를 노드로, 다음 수를 에지(화살표)로 표현한 것을 게임 트리(game tree)라고 한다. 게임 트리를 모두 펼칠 수 있다면 가능한 모든 결과가 드러나기 때문에 최선의 수를 찾을 수 있다. 하지만 바둑이나 장기는 가능한 상태(돌이나 말을 배치하는 패턴)가 천문학적으로 많아서 게임 트리를 모두 전개하기가 현실적으로 불가능하다. 따라서 문제를 풀려면 게임 트리를 효율적으로 탐색해야 한다.

몬테카를로 트리 탐색(Monte Carlo Tree Search, MCTS)은 트리의 전개를 몬테카를로법으로 근사하는 기법이다. 예를 들어 돌들이 특정 형태로 놓여 있는 보드 상태가 얼마나 좋은지 평가할 때, 돌을 무작위로 두는 두 플레이어에게 승패가 결정될 때까지 계속 두게 한다. 이러한 시도(특정 상태의 보드에서 시작하는 게임)를 수차례 반복하여, 그 승률로 해당 보드 상태가 '얼마나 좋은가'를 근사적으로 나타낸다. 이처럼 승패가 결정될 때까지 무작위로 게임을 진행시키는 일을 플레이아웃(playout) 또는 롤아웃(rollout)이라고 한다.

몬테카를로 트리 탐색은 현재의 보드 상태에서 가능한 모든 수를 각각 평가하고, 그 결과를 토대로 다음 수를 결정하는 흐름으로 진행한다. 그리고 일반적으로는 이 기본 아이디어에 더하여 유망해 보이는 보드 상태로 전개한 후 평가한 결과를 피드백하는 메커니즘이 보강된다. 

### 2. 알파고
---
알파고(AlphaGo)는 몬테카를로 트리 탐색에 심층 강화 학습을 결합한 기법이다. 

알파고에는 두 개의 신경망이 사용되는데, 하나는 현재 바둑판에서 이길 확률을 평가하는 Value 신경망이고 다른 하나는 정책을 나타내는 Policy 신경망이다. Policy 신경망은 다음에 둘 수를 확률로 출력한다. 예를 들어 (1, 1) 위치에 둘 확률은 2.4%, (1, 2) 위치에 둘 확률은 0.2%와 같은 식이죠. 이 두 신경망으로 몬테카를로 트리 탐색을 더 정밀하게 수행할 수 있게 되었다.

알파고는 인간의 기보 데이터를 이용해 두 신경망을 학습시켰다. 그런 다음 셀프 플레이(self-play) 형태로 대국을 반복하고 여기서 수집한 경험 데이터를 사용해 학습을 더욱 강화했다.

> 셀프 플레이는 강화 학습의 틀 안에서 설명할 수 있다. 바둑을 두는 에이전트가 있고 에이전트의 복제체인 대전 상태가 '환경'역할을 한다. 이 둘이 상호작용하면서 최종적으로 승리 혹은 패배라는 보상을 얻는다. 이처럼 환경 속에서 상호작용하며 데이터를 수집한다는 점에서 알파고가 활용한 방식은 강화학습이라고 말할 수 있다.

### 3. 알파고 제로
---
알파고는 인간의 기보를 학습 데이터로 사용했다. 하지만 알파고 제로(AlphaGo Zero)는 학습 데이터를 전혀 사용하지 않고 셀프 플레이를 통한 강화 학습만으로 학습했다. 또한 알파고에서 사용하던 '도메인 지식(바둑 규칙)'을 이용하지 않은 점도 큰 특징이다. 이 외에도 다음과 같은 점을 개선했다.

- Policy 신경망과 Value 신경망을 하나의 신경망으로 표현
- 몬테카를로 트리 탐색으로 플레이아웃하지 않고, 신경망의 출력만으로 각 노드(상태)를 평가

이처럼 기존 알파고보다 단순하고 범용적으로 개선했다. 더구나 인간이 쌓아둔 기보 데이터 없이 강화 학습만으로 학습했음에도 놀랍게도 기보 데이터를 활용한 알파고를 압도하는 성능을 보여줬다. 실제로 알파고와의 대국에서 100대 0으로 압승을 거뒀다.

### 4. 로봇 제어
---
딥러닝은 로봇 제어와 같은 시스템에도 활용된다. 하나의 예로, 구글은 로봇 팔이 다양한 물건을 잡을 수 있도록 학습시키는 데 성공했다. 로봇은 위쪽에 장착된 카메라가 보내주는 영상을 해석하여 어떻게 행동할지를 결정한다. 행동의 결과로 물체를 잡으면 성공, 잡지 못하면 실패라는 보상을 준다. 이처럼 강화 학습의 틀 안에서 학습을 진행한다.

필요한 데이터는 그림과 같이 로봇 일곱 대를 수개월 동안 실제 운용하여 수집했다. 이때 QT-Opt라는 강화 학습 알고리즘을 이용했다. QT-Opt는 Q 러닝 기반이기 때문에 오프-정책 기법이다. 과거에 얻은 경험 데이터도 활용한다는 뜻이다. 로봇을 실제로 운용해야 하는 환경이라면 데이터 수집 비용이 많이 들기 때문에 과거 데이터를 활용할 수 있다는 건 커다란 장점이다.

논문에 따르면 QT-Opt는 미지의 물건도 96% 확률로 잡는 데 성공하여, 구글이 이전에 개발한 지도 학습 방식보다 실패 확률을 1/5 이하로 줄였다고 한다.


### 5. NAS
---
딥러닝의 아키텍처(신경망 구조)는 보통 사람이 직접 설계한다. 뛰어난 아키텍처를 설계하기 위해서는 경험이 필요하다. 수많은 시행착오를 거쳐야 한다. 그런데 최근에는 최적의 아키텍처를 컴퓨터가 자동으로 설계하는 연구가 활발히 진행되고 있다. 흔히들 NAS(Neural Architecture Search)라고 부르는 분야이다.

NAS를 수행하는 방법에는 베이지안 최적화(Bayesian optimization)와 유전 프로그래밍(genetic programming) 등 여러 가지가 있다. 그중 유력한 후보가 강화 학습이다. 붐을 일으킨 주인공은 『Neural Architecture Search with Reinforcement Learning』 논문이다. 이 논문에서는 강화 학습으로 신경망 구조를 자동으로 최적화하여 사람이 설계한 것 이상의 아키텍처를 발견하는 데 성공했습니다. 어떻게 해냈는지를 간략하게 알아보자.

핵심 아이디어는 신경망 아키텍처를 '텍스트'로 표현할 수 있다는 점에 주목한 것이다. 예를 들어 신경망의 아키텍처를 다음 예와 같은 텍스트로 정의할 수 있다(가상 포맷입니다).

![[Pasted image 20240630224904.png|300]]

또한 RNN(순환 신경망)을 사용하면 가변 길이의 텍스트를 생성할 수 있다. 이론적으로는 지금 예와 같은 텍스트를 출력하는 RNN도 고안할 수 있다. 그렇다면 다음 그림과 같이 RNN을 에이전트로 이용하여 강화 학습으로 최적의 아키텍처를 찾아낼 수 있다.

![[Pasted image 20240630224931.png]]

그림과 같이 RNN이 신경망의 아키텍처를 텍스트로 생성한다. 이 작업이 바로 에이전트의 행동에 해당한다. 그리고 생성된 아키텍처(신경망)를 검증 데이터로 학습시키고 마지막으로 인식 정확도를 측정한다. 이 인식 정확도가 보상으로 작용한다. 논문에서는 REINFORCE 알고리즘으로 RNN의 매개변수를 갱신하였고 그 결과 RNN은 점차 인식 정확도가 더 높은 아키텍처를 출력했다.

NOTE: 논문에서는 아키텍처의 범위를 제한하는 장치가 마련되어 있다. 예를 들어 합성곱 계층의 파라미터를 필터와 스트라이드 크기만으로 제한하고, RNN으로 두 매개변수의 값을 차례대로 출력하게 하는 식이다.

### 6. 자율 주행
---
자율주행 자동차를 만들기 위한 연구가 전 세계적으로 앞다투어 이루어지고 있다. 대부분은 딥러닝을 활용한 '지도 학습'으로 진행되고 있으나 한편으로는 강화 학습으로 접근하는 움직임도 보인다. 자율주행이라는 문제는 환경을 관찰하여 최적의 행동(브레이크 밟기, 핸들 돌리기 등)을 선택하는 일련의 결정이기 때문에 강화 학습에 잘 들어맞는다.

강화 학습으로 자율주행에 도전해볼 수 있는 환경으로 AWS DeepRacer[42]가 있다. DeepRacer는 딥러닝 기법으로 제어하는 자율주행 레이싱카입니다. 사용자는 딥러닝 알고리즘을 구현하고 보상 함수 등도 직접 설정한다. 그런 다음 시뮬레이터를 써서 AWS에서 학습시킨다. 최종적으로 만들어진 모델은 가상 레이스나 실물 자동차 레이스에서 이용할 수 있다. 실제로 '챔피언십 컵' 등의 레이스가 정기적으로 개최되어 상금을 걸고 경쟁을 벌이고 있다.



