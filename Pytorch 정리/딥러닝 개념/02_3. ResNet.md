![[07.jpg]]

신경망이 깊어지면 깊어질수록 기울시 소실 문제에 직면하게 된다. 합성곱을 사용하는 딥러닝의 경우 주어진 입력값에 대한 특징맵이 점점 작아지므로 문제가 될 수 있다.

이러한 기울기 소실 문제를 `ResNet`에서는 어떻게 해결했나? 바로 스킵 커넥션을 이용했다.

스킵 커넥션은 자기 자신을 미분하면 1이 나오기 때문에 신경망의 출력 부분에 입력을 더하는 방식으로 기울기를 최소 1로 확보하는 기법이다.

보면 합성곱의 출력값을 `F(x)`이고 여기에 미분하면 1이 나오는 `x`를 더해서 다음층에 입력하게 되는 입력값의 함수를 `H(x) = F(x) + x`가 된다.


### 1. 학습이 쉬워진다?
---
![[딥러닝 개념/image/08.png]]

스킵 커넥션은 학습이 쉬워진다는 장점 또한 가지게 된다. 

ResNet은 최적화해야 하는 함수는 `H(x) = F(x) + x`이다. 다른 관점에서 보면 `F(x) = H(x) - x`가 된다. 이는 입력과 출력값의 차이다. 

일반적인 스킵 커넥션이 없는 모델의 경우 최적화대상은 `H(x)`이지만, ResNet은 `F(x)`의 최적화이다. 헷갈리지만 당연하다. 학습해야하는 부분은 합성곱층인데, 기존 합성곱층의 출력값은 출력값인 `H(x)`이다. 그런데 ResNet의 경우는 `H(x)`에 `x`라는 부분도 있으므로 이를 제외한 `F(x)`이다.

x는 입력값이라 고정된 값이다. 따라서 `H(x)`와 x를 비슷하게 최적화 한다는 것이다. 즉, `F(x)`를 0으로 만드는 것이다. 입력 `x`가 출력 `H(x)`에 이미 매우 가까울 경우, `F(x)`는 작은 값이 된다. 즉, 신경망은 입력 `x`를 크게 변경하지 않고도 출력 `H(x)`에 매우 가까워질 수 있다. 이는 네트워크가 학습해야 하는 작업을 "잔차" 또는 "보정"으로 단순화하며, 이는 학습 과정을 효율적으로 만든다.

쉽게 말하면 기존의 학습은 알 수 없는 지점인 최적화 점을 향해서 간다면, ResNet은 0이라는 뚜렷한 목표가 생긴 것이다. 


### 2. 잔차 학습의 예시
---
예를 들어, 어떤 이미지 분류 문제를 해결하기 위한 신경망을 고려해 보자. 입력 `x`는 이미지이고, 최종 출력 `H(x)`는 분류 레이블(예: 고양이, 개 등)에 대한 예측이다.

- **전통적 신경망**: 이 네트워크는 입력 이미지 `x`로부터 직접적으로 레이블에 해당하는 출력 `H(x)`를 예측하려고 합니다. 네트워크가 깊어질수록, 이러한 직접적인 예측은 점점 더 어려워진다.

- **ResNet (잔차 학습)**: ResNet에서는 각 레이어(또는 레이어 그룹)가 입력 `x`와 출력 `H(x)` 사이의 잔차 `F(x)`를 학습한다. 즉, 네트워크는 원본 이미지 `x`에 어떤 작은 변경(`F(x)`)을 적용하여 최종 출력 `H(x)`에 도달하려고 한다. 이 작은 변경은 일반적으로 이미지에서의 미세한 특징들을 조정하는 것으로 이해될 수 있다. 
	- 이는 엄청난 발견이다. 원본 이미지와 신경망 출력값인 F(x)가 비슷한 값이 나온다는 것은 신경망의 추론 값을 원본 이미지에 가깝게 만든다는 것이고 이러한 것은 신경망을 이미지의 특징을 완벽하게 잡아낼수 있는 학습을 한다는 것을 의미한다.  


### 3. 파이토치로 디자인
---
참고로 nn.Sequential은 사용하지 말자. 이건 한번에 쭉 신경망을 흘리기 때문에 하나의 블록으로 묶이지 않는 이상 데이터 흐름을 제어할 수 없기 때문에 간단한 구조에만 적합하다.

![[10.jpg]]

##### 1. 다운 샘플링

스킵 커넥션에서 입력값과 합성곱의 결과를 더해주기 위해 입력값에 1x1 합성곱으로 채널 수를 맞춰주는 기법이다.

![[딥러닝 개념/image/11.png]]

위 이미지를 보듯 한개의 필터를 거치면 특징 맵의 채널이 64개 혹은 256개까지 늘어난다. 따라서 더하는 부분에 3채널인 이미지와 더할 수 없게 된다. 

3채널인 이미지의 특징을 손상시키지 않고 특징 맵과 채널을 맞춰주기 위해 사용하는 방법이 다운 샘플이다.

``` python
import torch
import torch.nn as nn

class BasicBlock(nn.Module):
   def __init__(self, in_channels, out_channels, kernel_size=3):
       super(BasicBlock, self).__init__()

       # ❶ 합성곱층 정의
       self.c1 = nn.Conv2d(in_channels, out_channels,
                           kernel_size=kernel_size, padding=1)
       self.c2 = nn.Conv2d(out_channels, out_channels,
                           kernel_size=kernel_size, padding=1)
                           
       self.downsample = nn.Conv2d(in_channels, out_channels,
                                   kernel_size=1)

       # ❷ 배치 정규화층 정의
       self.bn1 = nn.BatchNorm2d(num_features=out_channels)
       self.bn2 = nn.BatchNorm2d(num_features=out_channels)

       self.relu = nn.ReLU()

   def forward(self, x):
       # ❸스킵 커넥션을 위해 초기 입력을 저장
       x_ = x
       x = self.c1(x)
       x = self.bn1(x)
       x = self.relu(x)
       x = self.c2(x)
       x = self.bn2(x)

       # ➍합성곱의 결과와 입력의 채널 수를 맞춤
       x_ = self.downsample(x_)

       # ➎합성곱층의 결과와 저장해놨던 입력값을 더해줌
       x += x_
       x = self.relu(x)

       return x
```
