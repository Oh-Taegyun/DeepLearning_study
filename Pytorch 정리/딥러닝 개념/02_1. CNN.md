합성곱의 가장 중요한 점은 이미지를 처리하는게 아닌 **3차원 데이터**를 처리하는 것이다! 헷갈리지 말자


### 1. 완전연결 계층의 문제점
---
- 데이터의 형상이 무시됨 : 3차원 데이터를 1차원 데이터로 평탄화해야 함
    - 이미지(3차원)의 형상에는 공간적 정보가 담겨져 있음
    - 공간적으로 가까운 픽셀의 값은 비슷하거나, RGB의 각 채널은 서로 밀접연관 등의 패턴

- CNN의 합성곱계층은 형상을 유지 (3차원데이터 입력 / 출력)

- CNN에서는 합성곱 계층의 입출력 데이터를 특징 맵(feature map)이라고도 함 (입력특징맵 / 출력특징맵)

완전연결 계층은(Affine-ReLu 연결) ‘데이터의 형상이 무시’된다는 단점이 있다. 이미지는 통상 가로, 세로, 채널(색상)로 구성된 3차원 데이터이다. 

한편, 합성곱 계층은 형상을 유지한다. 이미지도 3차원 데이터로 입력받으며, 마찬가지로 다음 계층에도 3차 원 데이터로 전달한다. 그래서 CNN에서는 이미지처럼 형상을 가진 데이터를 제대로 이해할 (가능성이 있는) 것이다. CNN에서는 합성곱 계층의 입출력 데이터를 특징 맵이라고도 한다. 합성곱 계층의 입력 데이터를 입력 특징 맵, 출력 데이터를 출력 특징 맵이라고 하는 식



### 2. 합성곱 연산
---
![[딥러닝 개념/image/12.png]]
![[딥러닝 개념/image/13.png]]
![[딥러닝 개념/image/14.png]]

- 이미지 처리에서 말하는 필터 연산에 해당
- 예시 : 입력데이터는 세로,가로 방향의 형상가짐 (필터도 마찬가지)
- 입력은 (4,4), 필터는 (3,3), 출력은(2,2)의 형상
- 문헌에따라 필터를 커널이라고 지칭
- 합성곱연산은 필터의 윈도우를 일정 간격 이동해가며 입력데이터에 적용
- 입력과 필터에서 대응하는 원소끼리 곱한 후 그 총합 계산 (단일 곱셈-누산, FMA)
- 편향까지 포함한 합성곱 연산
    - 편향은 항상 하나(1,1)만 존재 = 모든 원소에 더함



### 3. 패딩
---
- 합성곱 연산을 수행하기 전 입력 데이터 주변을 특정 값 (ex.0)으로 채우는 것
- 예시 : (4,4)크기의 입력데이터에 폭이 1인 패딩 적용 

![[딥러닝 개념/image/15.png]]

- (4,4) 입력데이터에 패딩 추가되어 (6,6)이 됨
- 패딩은 출력크기를 조정할 목적으로 사용
    - 연산을 거칠 때 마다 출력의 형상은 계속해서 줄어들음
    - 출력크기가 1이 되어버리면 합성곱연산 불가
    - 입력데이터의 공간적크기를 고정한 채 다음계층에 전달가능


### 4. 스트라이드
---
- 필터를 적용하는 위치의 간격
- 스트라이드를 2로 하면 필터적용 윈도우가 두 칸씩 이동

![[딥러닝 개념/image/16.png]]

- 스트라이드를 키우면 출력크기는 작아짐
- 패딩, 스트라이드, 출력크기의 관계 수식화
    - 입력크기(H,W) / 필터크기(FH, FW) / 출력크기(OH, OW) / 패딩 P / 스트라이드 S
    - OH=(H+2P-FH)/S+1
    - OW=(W+2P-FW)/S+1
- 수식들은 정수로 나눠떨어지는 값이어야 함

![[딥러닝 개념/image/17.png|300]]


### 5. 3차원 데이터의 합성곱 연산
---
- 채널 쪽으로 특징 맵이 여러 개 있을 때
    - 입력데이터와 필터의 합성곱 연산을 채널마다 수행
    - 연산수행 결과 더해서 출력 얻음 

![[딥러닝 개념/image/18.png]]        

- 입력데이터의 채널 수와 필터의 채널 수가 같아야 함
- 필터 자체 크기는 원하는 값으로 설정가능 (모든 채널의 필터는 같은크기)


### 6. 블록으로 생각하기
---
![[딥러닝 개념/image/19.png]]

- 3차원 직육면체로 생각하면 이해 쉬움
- 3차원 데이터를 배열로 나타낼 때는 채널, 높이, 너비 순서 (C, H, W) / 필터 (C, FH, FW)
- 출력데이터는 한 장의 특징 맵 : 다수의 채널을 내보내려면 필터를 다수 사용 

![[딥러닝 개념/image/20.png]]

- 필터를 FN개 적용하면 출력 맵도 FN개 생성
- FN개의 맵을 모으면 (FN, OH, OW)인 블록 완성
- 합성곱 연산에서는 필터의 수도 고려해야 함
- 필터의 가중치 데이터는 4차원 데이터 (출력채널 수, 입력채널 수, 높이, 너비)
- 편향 추가

![[딥러닝 개념/image/21.png]]



### 7. 배치처리
---
![[딥러닝 개념/image/22.png]]

- 각 계층을 흐르는 데이터의 차원을 하나 늘려 4차원 데이터로 저장
- (데이터 수, 채널 수, 높이, 너비)
- 주의 : 신경망에 4차원 데이터가 하나 흐를 때마다 데이터 N개에 대한 합성곱 연산수행


### 8. 풀링계층
---
![[23.png]]

- 세로, 가로 방향의 공간을 줄이는 연산
- 최대 풀링(max pooling) : 최댓값을 구하는 연산
- 풀링의 원도우 크기와 스트라이드는 같은 값으로 설정하는 것이 보통
- 최대풀링의 예

##### 풀링 계층의 특징
1. 학습해야 할 매개변수가 없다
2) 채널 수가 변하지 않는다
	![[./image/24.png|500]]
풀링연산은 채널마다 독립적으로 계산하기 때문에 입력채널 그대로 출력

3) 입력의 변화에 영향을 적게 받는다 (강건하다)
	![[./image/25.png]]

입력 데이터의 차이를 풀링이 흡수해 사라지게 함

일반적으로 최댓값이던, 평균값이던 정보 손실은 발생한다. 하지만 평균값은 풀링의 커널 안에 포함되는 모든 픽셀의 정보를 담을 수 있기 때문에 최댓값보다 조금은 유리하다.

풀링은 일반적으로 데이터의 크기를 줄이면서도 중요한 정보를 유지하는 방법이다. 풀링을 하는 주요 이유는 다음과 같다.

1. **차원 축소(Dimensionality Reduction)**: 풀링은 입력 데이터의 크기를 줄인다. 이는 계산량을 감소시키고, 모델의 파라미터 개수를 줄여서 과적합(overfitting)의 위험을 감소시키는 데 도움이 된다

2. **불변성(Invariance) 증가**: 풀링은 이미지 내의 작은 위치 변화에 대해 더 강건해질 수 있도록 도와준다. 예를 들어, 맥스 풀링(max pooling)은 가장 두드러진 특징을 유지하면서 작은 변화나 왜곡이 있는 경우에도 동일한 결과를 내도록 한다.

4. **주요 특징 강조**: 풀링은 입력 데이터에서 중요한 특징을 유지하고 강조하는 역할을 한다. 특히 맥스 풀링은 가장 강한 신호를 선택함으로써, 특징의 중요한 부분을 강조한다.

5. **메모리 사용량 및 연산 시간 감소**: 데이터의 크기가 줄면, 처리해야 할 데이터 양이 감소하고, 이는 메모리 사용량과 연산 시간을 줄여든다. 이는 특히 큰 이미지나 깊은 네트워크에서 중요하다.

6. **특징의 추상화 및 간소화**: 풀링은 입력 데이터를 더 높은 수준의 추상화로 변환한다. 이 과정에서 모델은 더 간소화된 형태의 데이터를 사용하여 학습할 수 있으며, 이는 종종 더 일반화된 모델을 만드는 데 도움이 된다.

