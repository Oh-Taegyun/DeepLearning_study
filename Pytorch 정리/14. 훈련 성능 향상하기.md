최적적합 모델을 찾을려면 먼저 모델을 과대적합시켜야 한다. 그래야 손실 그래프를 보고 어느 지점에서 학습을 멈춰야 하는지 파악이 가능하기 때문, 

### 1. 과대적합이 안되요
---
모델을 과대적합시키는게 1번째 원칙이다. 하지만, 모델이 다음과 같은 문제를 지닐 수 있다.
1. 훈련이 안되요 
	- 시간이 지나도 훈련 손실이 줄어들지 않는다. 참고로 모델이 정답을 있는 그대로 외우는 수단까지 가지는 것을 감안한다면 말이 안되는 셈
2. 훈련은 잘 되는데 모델이 의미 있는 일반화를 못합니다.
3. 여전히 과소적합 상태 
	- 시간이 지남에 따라 훈련과 검증 손실이 모두 줄어들고 기준점을 넘어설 수 있지만 과대적합 되지 않을 것 같음

이것들을 차례대로 살펴보자

##### 1-1. 훈련이 안되요
---
경사 하강법 자체에 문제가 있는 경우를 말한다. 옵티마이저 선택, 모델 가중치의 초깃값 분포, 학습률, 배치 크기에 뭔가 문제가 생겼단 뜻

일반적으로는 파라미터를 고정하고 학습률과 배치크기를 튜닝하는것으로 해결 된다. 
1. 학습률을 높이거나 낮춘다. 높으면 최적 적합을 뛰어넘는 업데이트가 일어났다는 것이고, 낮추면 변화량이 너무 미미해서 멈춘 것처럼 보이는 것 
2. 배치 크기를 증가시킨다. 배치 샘플이 늘어나면 유익하고 잡음이 적은 (분산이 낮은) 그레디언트가 발생한다. 

##### 1-2. 훈련은 잘 되는데 모델이 의미 있는 일반화를 못합니다.
---
랜덤 분류기(그냥 랜덤으로 때려 맞추는 것)보다 좋지 못한 상태란 것이다. 즉, 답만 외우고 있단 소리
이 경우 난이도가 갑자기 최상이 되어버리는데, 이건 모델의 문제보다 데이터의 문제일 가능성이 높기 때문이다. 정답 레이블이 잘못 설정되어있거나 노이즈가 엄청 심하게 껴서 모든 데이터가 다 같아보이는 것

약간의 가능성으로 모델의 종류가 문제에 적합하지 않을 수 있다....는 가설도 있다. (RNN문제를 CNN으로 푼다던가) 근데 이 문제일 가능성은 거의 없는게 그냥 일반적인 ANN도 냅다 데이터 주면 어떻게든 학습을 하긴 한다. 문제는 그게 정확도가 낮아서 그렇지

##### 1-3. 여전히 과소적합 상태
---
![[Pasted image 20240414010436.png|500]]
시간이 지남에 따라 훈련과 검증 손실이 모두 줄어들고 기준점을 넘어설 수 있지만 과대적합 되지 않을 것 같은 경우이다. 

늘 말하지만 모델은 최악의 경우 모든 정답을 통째로 외워버린다는 선택지를 가지고 있다. 에폭숫자를 무한대로 늘려버리면 반드시 정답을 외워서 오버피팅이 되어야 한다. 

이 경우 모델이 추론할 수 있는 용량이 부족하다는 뜻이다. 모델이 표현능력이 현저하게 떨어진단 뜻이다. 

이 경우 감사하게 해결방법은 매우 쉬운데 그냥 모델의 크기를 늘려주면 그만이다. 