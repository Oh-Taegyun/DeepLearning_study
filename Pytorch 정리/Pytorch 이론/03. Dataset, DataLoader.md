Daatset을 설정한 뒤에 DataLoder로 설정한 Dataset을 끌고 온다. Dataset 은 샘플과 정답(label)을 저장하고, DataLoader 는 Dataset 을 샘플에 쉽게 접근할 수 있도록 순회 가능한 객체(iterable)로 감싼다.

그럼 Dataset은 왜 만들었을까?

데이터가 100만개라면, 거대한 데이터를 하나의 ndarray 인스턴스로 처리하면 모든 원소를 한꺼번에 메모리에 올려야 한다. 당연히 메모리는 부담스러운 상황일 것이다. 그렇기 때문에 이러한 문제에 대응이 가능하도록 Dataset 클래스를 만들었다.


### Dataset 구조
---
``` python
from torch.utils.data import Dataset

class MyDataset(Dataset):
    def __init__(self):
        pass
    def __getitem__(self, index):
        pass
    def __len__(self):
        pass
```

`__init__(self)` : 생성자. 여기에는 모델에 사용할 데이터를 담아두는 등 어떤 인덱스가 주어졌을 때 반환할 수 있게 만드는 초기 작업을 수행한다.

`__len__(self)` : 학습에 사용할 데이터의 총 개수라고 볼 수 있는데, 즉 얼마만큼의 인덱스를 사용할지를 반환하는 메서드이다.

`__getitem__(self, index)` : 메서드는 어떤 인덱스가 주어졌을 때 해당되는 데이터를 반환하는 메서드이다. numpy 배열이나 텐서 형식으로 반환한다. 보통 입력과 출력을 튜플 형식으로 반환하게 된다.


### DataLoader
---
``` python
DataLoader(dataset, batch_size=1, shuffle=False, sampler=None,
           batch_sampler=None, num_workers=0, collate_fn=None,
           pin_memory=False, drop_last=False, timeout=0,
           worker_init_fn=None)
```

Dataset과는 달리 DataLoader는 그닥 사용자가 수정할 이유가 없다. DataLoader는 그저 Dataset를 기반으로 데이터를 꺼내는 것일뿐 그 이상의 기능을 할 이유가 없다.


parameter 설명
1. dataset는 필수 입력 인자다.
2. bath_size = 1 : 배치 크기이다.
3. shuffle = False : 에포크별로 데이터셋을 뒤섞을건지 여부를 결정하는 것

나머지는 많이 사용하지는 않는다. 그 외 설명은 다음 글을 참고하길 바란다.

> DataLoader parameter 별 용도 : https://subinium.github.io/pytorch-dataloader/


``` python
# DataLoader 사용 예제
from torch.utils.data import DataLoader

train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)
test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)

for epoch in range(max_epoch):
    for x, t in train_dataloader:
        print(x.shape, t.shape)
        ...
```

> https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader 데이터로더에 대한 자세한 파라미터 값


