### 01. 퍼셉트론
---
![[Pytorch 이론/image/04.png|500]]

신경에 있는 뉴런의 형상을 구현화 한 것. 뉴런도 임계치가 넘어가는 전기신호를 받으면 1과 0으로 나타나고 그런 뉴런이 수백개 모여 신경을 이룬다.

### 02. 인공 신경망(ANN)
---
![[Pytorch 이론/image/05.png]]

퍼셉트론의 구조적 문제를 해결하기 위해 다중 퍼셉트론을 구현했다. 그리고 거기서 가중치와 활성화 함수를 적절하게 넣어 만든 것이 신경망이다. 그리고 이러한 계산을 행렬의 곱을 이용한다면

![[Pytorch 이론/image/07.png|300]]
이러한 식을 간단하게

![[Pytorch 이론/image/08.png|200]]
이렇게 표현할 수 있다.

따라서 이러한 이유로 신경망의 순전파(추론) 하는데에는 행렬 계산이 들어간다. 근데 학습을 시키지 않으면 역시 정확도가 낮다!

![[Pytorch 이론/image/06.png]]

### 03. 신경망의 학습
---
그럼 어떤 기준으로 학습시킬까? 그에 대한 답은 손실함수를 이용한 가중치를 학습시키는 방법이다. 신경망도 하나의 지표를 기준으로 최적의 매개변수 값을 탐색해야 하는데. 신경망 학습에서 사용하는 지표는 손실함수이다.

그럼 신경망 가중치를 손실함수로 어떻게 최적화 할까? 그에 대한 답이 그레디언트에 있다. 그레디언트는 각 장소에서 함수가 가장 빨리 증가하는 방향벡터이다. 그래서 다음과 같이 학습이 가능하다.

![[Pytorch 이론/image/09.png|200]]

즉 다음과 같이 손실함수에 대한 기울기를 구한다.

![[Pytorch 이론/image/10.png|300]]

그 다음 이걸 토대로 가중치를 학습시키면 된다.

![[Pytorch 이론/image/11.png|500]]

![[Pytorch 이론/image/12.png]]

![[Pytorch 이론/image/13.png]]

![[Pytorch 이론/image/14.png]]

중요한 것은 흐름이다! 손실 함수를 흔한 오차제곱합으로 썼을 뿐


### 4. 계산 그래프
---
![[Pytorch 이론/image/15.png]]

역전파를 통해 미분을 효율적으로 계산이 가능해서 이 방법을 채택한다. 중요한 요점은 신경망의 디자인, 왜 행렬의 계산인지, 손실함수의 의미, 그리고 손실 함수를 이용한 학습방법이다.

가장 단순한 신경망 디자인은 다음과 같다.

1. 신경망을 행렬식으로 디자인 한다
2. 원하는 손실함수를 정의한다.
3. 손실함수의 출력값을 역전파 하여 계산 그래프를 학습시킨다

``` python
for data, label in DataLoader():
    # 모델의 예측값 계산
    predictrion = model(data)
    # 손실 함수를 이용해 오차 계산
    loss = LossFunction(prediction, label)
    # 오차 역전파
    optimizer.step()
```




