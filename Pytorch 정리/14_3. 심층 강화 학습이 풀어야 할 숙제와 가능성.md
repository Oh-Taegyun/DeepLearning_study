심층 강화 학습은 로봇 제어와 같은 실제 시스템에 활용되고 있다. 하지만 현실 세계는 가상의 세계와 달리 제약이 많다. 예를 들어 로봇은 비싸고 대량으로 준비하기 어렵기 때문에 경험 데이터를 충분히 수집하기가 쉽지 않는다. 또한 강화 학습에서는 시행착오를 거치면서 경험 데이터를 수집하는데, 이 과정에서 로봇을 고장내거나 주변에 위험을 초래하는 행동은 피해야 한다. 이번 절에서는 이러한 문제들을 막아주는 유력한 해결책을 몇 가지 소개하겠다.

### 1. 시뮬레이터 활용
---
현실 세계의 제약 상당수는 시뮬레이터로 해결할 수 있다. 시뮬레이터 안에서는 에이전트와 환경의 상호작용을 빠르게 반복할 수 있다. 게다가 에이전트가 위험한 행동을 하더라도 아무런 문제가 없다. 하지만 현실 세계와 시뮬레이터 속 세계 사이에는 간극이 있다. 시뮬레이터는 실제 환경을 완벽하게 모방할 수 없기 때문에 시뮬레이터에서 학습한 모델(에이전트)이 실제 환경에서 기대한 대로 동작하지 않는 경우가 많다. 이 문제에 집중하는 연구 분야를 Sim2Real이라고 한다.

Sim2Real에는 여러 가지 유력한 기법이 있으나 그중 Domain Randomization(도메인 무작위화) 기법을 소개하겠다. 이 기법은 시뮬레이터에 무작위 요소를 추가하여 다양한 환경을 만들고, 그 다양한 환경 속에서 에이전트를 행동시키고 학습시킨다. 이런 식으로 일반화 성능을 높여 실제 환경에서도 더 잘 동작하리라 기대할 수 있다. 예를 들어 OpenAI는 손가락 다섯 개가 달린 로봇 손 하나만으로 루빅스 큐브를 맞추는 문제에 이 기법을 적용한 연구를 공개했다.

이 연구에서는 광원과 텍스처 등의 렌더링 이미지 매개변수나 마찰과 같은 역학적 매개변수에 무작위 값을 넣어 다양한 환경을 만들고 그 안에서 에이전트를 학습시킨다. 그 결과 현실 세계에서도 추가 학습 없이 큐브를 맞추는 데 성공했다.

### 2. 오프라인 강화 학습
---
![[Pasted image 20240630225915.png]]

과거에 수집한 경험 데이터를 잘 활용하는 방법도 있다. 예를 들어 자율주행이나 로봇 제어 등에서는 지금까지 사람이 조작한 많은 경험 데이터가 축적되어 있을 수 있다. 대화 시스템에서는 사람들이 주고받은 대화 이력을 쉽게 수집할 수 있다(챗봇 같은 대화 시스템을 강화 학습으로 구현하는 연구는 많습니다). 이러한 과거 경험 데이터, 즉 '오프라인 데이터'를 활용해서도 에이전트를 학습시킬 수 있다. 나아가 환경과 상호작용을 전혀 하지 않고도 오프라인 데이터만으로 최적의 정책을 추정하는 것도 생각할 수 있다. 이것이 바로 오프라인 강화 학습(offline reinforcement learning)이다.

왼쪽 그림과 같이 일반적인 강화 학습은 에이전트와 환경이 상호작용한다(‘온라인’ 강화 학습). 반면 오른쪽의 오프라인 강화 학습은 지금까지 수집된 경험 데이터(오프라인 데이터셋)만으로 학습한다. 오프라인 강화 학습은 실제 환경과의 상호작용이 일어나지 않는 것이 특징이다.

>오프라인 강화 학습은 오프-정책 기법과 관련이 있다. 오프 정책은 '행동 정책'통해 경험 데이터를 수집하고, 그 경험 데이터를 이용해 '대상 정책'을 갱신한다. 그러므로 오프-정책 기법을활용하여 오프라인 강화 학습을 구현할 수 있다. 다만 일반적인 오프-정책 기법은 대상 정책에 준하는 행동 정책을 만들고 그 위에서 환경과 상호작용하면서 에이전트를 학습시킨다. (Q러닝) 반면, 오프라인 강화 학습은 환경과의 상호작용이 전혀 없이 오프라인 경험 데이터만으로 학습한다.   

### 3. MDP가 지닌 유연성
---
현실에 존재하는 모든 문제를 MDP라는 문제 형식에 맞출 수는 없습니다. 하지만 놀랍도록 많은 문제가 MDP로 공식화할 수 있다고 알려져 있습니다. MDP는 환경과 에이전트가 '상태, 행동, 보상'이라는 세 가지 정보를 서로 '던져주는' 구조입니다. 이때 어떤 센서를 사용할지, 행동을 어떻게 제어할지, 보상을 어떻게 설정할지 등 세부 정보는 문제에 따라 유연하게 결정할 수 있습니다. 이러한 유연성 덕분에 MDP를 적용할 수 있는 범위는 생각보다 넓습니다.

또한 에이전트의 행동은 고차원적인 것부터 저차원인 것까지 생각할 수 있습니다. 예를 들어 로봇 제어 문제에서는 '쓰레기 버리기'나 '충전하기' 같은 행동을 고차원적인 결정이라고 생각할 수 있습니다. 한편 '모터에 3볼트 전류를 흘려보내기'와 같은 행동은 저차원적인 결정이라고 할 수 있습니다. 상태도 마찬가지로 높은 수준부터 낮은 수준까지 다양한 관점에서 생각할 수 있습니다.

시간(타임 스텝)의 단위도 에이전트가 결정하는 시점에 따라 다르게 설정할 수 있습니다. 예를 들어 단계 하나는 1밀리초가 될 수도 있고, 1분이 될 수도 있고, 1시간이나 하루, 심지어 한 달이 될 수도 있습니다. 이처럼 문제에 따라 상태, 행동, 시간 단위를 유연하게 바꿀 수 있기 때문에 MDP의 적용 범위가 그만큼 넓어집니다.

MDP에서 설정이 필요한 사항

현실의 문제를 해결하려면 어떻게 MDP로 공식화하는냐가 관건입니다. 새로운 문제를 MDP로 공식화하려면 다음과 같은 사항들을 결정해야 합니다.

- 해결하고자 하는 문제는 일회성 과제인가, 아니면 지속적 과제인가?
- 보상의 가치는? (보상 함수 설정)
- 에이전트가 취할 수 있는 행동은 무엇인가?
- 환경의 상태를 무엇으로 규정할 것인가?
- 수익의 할인율은?
- 어디까지를 환경으로, 어디까지를 에이전트로 할 것인가?

이 사항들은 문제에 따라 자연스럽게 결정되는 경우도 있고 설정하기 어려운 경우도 있습니다. 예를 들어 바둑과 같은 보드 게임은 간단합니다. 일회성 과제이고 보상은 최종적으로 이기면 1,










