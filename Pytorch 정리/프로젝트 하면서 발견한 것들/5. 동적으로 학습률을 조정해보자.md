> 출처 : https://gaussian37.github.io/dl-pytorch-lr_scheduler/
### learning rate 임의 변경
---
- 학습에 사용되는 learning rate를 임의로 변경하기 위해서는 SGD, Adam과 같은 optimizer로 선언한 optimizer 객체를 직접 접근하여 수정할 수 있습니다.
- 일반적인 환경인 1개의 optimizer를 사용한다면 `optimizer.param_groups[0]`을 통하여 현재 dictionary 형태의 optimizer 정보를 접근할 수 있습니다. 그 중 `lr`이라는 key를 이용하여 learning rate의 value값을 접근할 수 있습니다.
- 다음은 learning rate를 반으로 줄이는 작업을 나타내는 예시입니다.

```python
optimizer.param_groups[0]['lr'] /= 2
```

### 1. LambdaLR
---
- LambdaLR은 가장 유연한 learning rate scheduler이다. 어떻게 scheduling을 할 지 lambda 함수 또는 함수를 이용하여 정하기 때문.
- LmabdaLR을 사용할 때 필요한 파라미터는 `optimizer`, `lr_lambda`이다. 

``` python
def func(epoch):
    if epoch < 40:
        return 0.5
    elif epoch < 70:
        return 0.5 ** 2
    elif epoch < 90:
        return 0.5 ** 3
    else:
        return 0.5 ** 4

scheduler = LambdaLR(optimizer, lr_lambda = func)
```

- 이번에는 `torch.optim.lr_scheduler.LambdaLR`을 상속받아서 클래스를 생성하는 방법을 알아보도록 하자.

``` python
class WarmupConstantSchedule(torch.optim.lr_scheduler.LambdaLR):
    """ Linear warmup and then constant.
        Linearly increases learning rate schedule from 0 to 1 over `warmup_steps` training steps.
        Keeps learning rate schedule equal to 1. after warmup_steps.
    """
    def __init__(self, optimizer, warmup_steps, last_epoch=-1):

        def lr_lambda(step):
            if step < warmup_steps:
                return float(step) / float(max(1.0, warmup_steps))
            return 1.

        super(WarmupConstantSchedule, self).__init__(optimizer, lr_lambda, last_epoch=last_epoch)

--------------------

optimizer = torch.optim.SGD(model.parameters(), lr=0.05, momentum=0.9, weight_decay=1e-5)
scheduler = WarmupConstantSchedule(optimizer, warmup_steps=10)
for step in range(100):
    scheduler.step()
```

- 위와 같이 `LambdaLR`을 활용하면 lambda / function을 이용하여 scheduler를 정할 수 있고 또는 클래스 형태로도 custom 하게 만들 수 있다.

### 2. StepLR
---
- StepLR도 가장 흔히 사용되는 learning rate scheduler 중 하나이다. 일정한 Step 마다 learning rate에 gamma를 곱해주는 방식이다.
- StepLR에서 필요한 파라미터는 `optimizer`, `step_size`, `gamma` 이다.

```python
scheduler = StepLR(optimizer, step_size=200, gamma=0.5)
```

![[Pasted image 20240315001311.png]]
- 위 그래프에서 연두색 선은 50 epoch을 나타낸다. step_size가 200이므로 50번 씩 4번 step이 0.5배가 되는 것을 확인할 수 있다.

### 3. MultiStepLR
---
- `StepLR`이 균일한 step size를 사용한다면 이번에 소개할 `MultiStepLR`은 step size를 여러 기준으로 적용할 수 있는 StepLR의 확장 버전이다.
- StepLR과 사용방법은 비슷하며 StepLR에서 사용한 step_size 대신, `milestones`에 리스트 형태로 step 기준을 받는다. 

``` python
scheduler = MultiStepLR(optimizer, milestones=[200, 350], gamma=0.5)
```

![[Pasted image 20240315001325.png]]
- 위 그래프에서 연두색 선은 50 epoch을 나타낸다. 200 epoch과 350 epoch 선에서 learning rate가 0.5배가 된 것을 확인할 수 있다.

### 4. ExponentialLR
---
- 지수적으로 learning rate가 감소하는 방법도 많이 사용한다. 이번에 다룰 내용은 지수적으로 learning rate가 감소하는 ExponentialLR이다.
- 지수적으로 감소하기 때문에 하이퍼 파라미터는 감소율 `gamma` 하나이다. 따라서 다음과 같이 간단하게 사용할 수 있다.

```
scheduler = ExponentialLR(optimizer, gamma=0.95)
```

  
### 5. CosineAnnealingLR
---
- `CosineAnnealingLR`은 cosine 그래프를 그리면서 learning rate가 진동하는 방식이다. 최근에는 learning rate가 단순히 감소하기 보다는 진동하면서 최적점을 찾아가는 방식을 많이 사용하고 있다. 이러한 방법 중 가장 간단하면서도 많이 사용되는 방법이 `CosineAnnealingLR` 방식이다.
- `CosineAnnealingLR`에 사용되는 파라미터는 `T_max` 라는 반주기의 단계 크기값과 `eta_min` 이라는 최소값이다.

```
scheduler = CosineAnnealingLR(optimizer, T_max=100, eta_min=0.001)
```
  ![[Pasted image 20240315001503.png]]

### 6. CyclicLR
---
- `CyclicLR` 방법 또한 많이 사용하는 방법이다. 앞에서 설명한 CosineAnnealingLR은 단순한 cosine 곡선인 반면에 CyclicLR은 3가지 모드를 지원하면서 변화된 형태로 주기적인 learning rate 증감을 지원한다.
- 이 때 사용하는 파라미터로 `base_lr`, `max_lr`, `step_size_up`, `step_size_down`, `mode`가 있습니다. `base_lr`은 learning rate의 가장 작은 점인 lower bound가 되고 `max_lr`은 반대로 learning rate의 가장 큰 점인 upper bound가 된다. `step_size_up`은 base_lr → max_lr로 증가하는 epoch 수가 되고 `step_size_down`은 반대로 max_lr → base_lr로 감소하는 epoch 수가 된다.

```
scheduler = CyclicLR(optimizer, base_lr=0.001, max_lr=0.1, step_size_up=50, step_size_down=100, mode='triangular')
```

![[Pasted image 20240315001650.png]]

- 먼저 위 그래프를 살펴보겠습니다. 위 그래프틑 `triangular` 모드에 해당합니다. learning rate는 `base_lr=0.001` 부터 시작해서 `step_size_up=50 epoch` 동안 증가하여 `max_lr=0.1` 까지 도달합니다. 그 이후 `step_size_down=100 epoch` 동안 감소하여 다시 `base_lr` 까지 줄어듭니다. 이 작업을 계속 반복하게 됩니다.

![[Pasted image 20240315001708.png]]
- 논문의 그림을 빌리면 `triangular` 모드는 위 그림과 같습니다.


```
scheduler = CyclicLR(optimizer, base_lr=0.001, max_lr=0.1, step_size_up=50, step_size_down=None, mode='triangular2')
```

![[Pasted image 20240315001802.png]]

- 이번에는 `triangular2` 모드에 대하여 다루어 보겠습니다. 이 모드에서는 주기가 반복되면서 `max_lr`의 값이 반 씩 줄어드는 것을 볼 수 있습니다.
- 위 코드에서는 `step_size_down=None`을 입력하였는데 이 경우 `step_size_up`과 같은 주기를 같습니다. 따라서 상승 하강 주기가 모두 50 epoch이 됩니다.
- 이 모드의 경우 max_lr이 반씩 줄어들기 때문에 마지막에는 수렴하게 됩니다.

![[Pasted image 20240315001936.png]]
- 논문의 그림을 빌리면 `triangular2` 모드는 위 그림과 같습니다.

```
scheduler = CyclicLR(optimizer, base_lr=0.001, max_lr=0.1, step_size_up=50, step_size_down=None, mode='exp_range', gamma=0.995)
```
![[Pasted image 20240315001955.png]]
- `exp_range` 모드는 `triangular2`와 유사합니다. 대신 선형 증감이 아니라 지수적 증감 방식을 사용합니다. 따라서 지수식의 밑에 해당하는 `gamma`값을 따로 사용합니다.

![[Pasted image 20240315002003.png]]
- 논문의 그림을 빌리면 `exp_range` 모드는 위 그림과 같습니다.
