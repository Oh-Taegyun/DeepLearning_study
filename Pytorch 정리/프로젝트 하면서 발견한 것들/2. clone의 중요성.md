``` python
def J_M_i(Pulley_i, End_Effector_i, X):
    if X.grad is not None:
        X.grad.data.zero_()  # 이전 그래디언트 제거

    X.requires_grad_(True)
    L_i = f_i(Pulley_i, End_Effector_i, X)
    L_i.backward()
    return X.grad
```

이렇게 쓰면 다음과 같은 문제가 생긴다.

```
일반적으로 print(X.grad) 하면 다음과 같이 출력되는데
tensor([-0.8317, -0.4476, -0.3286, -0.0633, 0.0724, 0.1786]) 
tensor([-0.7977, 0.5294, -0.2888, -0.0100, 0.1132, 0.0171]) 
tensor([ 0.3413, 0.8486, -0.4042, -0.0066, -0.0929, 0.0332]) 
tensor([ 0.4537, -0.7749, -0.4402, -0.0453, -0.0195, 0.0140]) 
tensor([-0.5329, -0.2979, 0.7920, 0.0132, -0.1199, -0.0548]) 
tensor([-0.5408, 0.3703, 0.7552, 0.0498, -0.1038, 0.0035]) 

왜 자꾸 다음과 같이 리스트에 담길까?
[tensor([-0.5408, 0.3703, 0.7552, 0.0498, -0.1038, 0.0035]), 
tensor([-0.5408, 0.3703, 0.7552, 0.0498, -0.1038, 0.0035]), 
tensor([-0.5408, 0.3703, 0.7552, 0.0498, -0.1038, 0.0035]), 
tensor([-0.5408, 0.3703, 0.7552, 0.0498, -0.1038, 0.0035]), 
tensor([-0.5408, 0.3703, 0.7552, 0.0498, -0.1038, 0.0035]), 
tensor([-0.5408, 0.3703, 0.7552, 0.0498, -0.1038, 0.0035])]
```

무슨 이유일까?

리스트에 동일한 텐서 값이 여러 번 담기는 문제는 보통 루프 내에서 텐서를 참조하는 방식 때문에 발생할 수 있다. 

특히, PyTorch 텐서를 다룰 때는 불변성(immutable)과 가변성(mutable)의 개념이 중요하다. 

리스트에 텐서를 추가하는 과정에서, 실제로는 *"텐서의 참조"* 를 추가하는 것이므로, 만약 같은 텐서 객체를 반복적으로 추가하게 되면, 리스트에는 동일한 객체에 대한 참조가 여러 번 포함되게 되는 것이다.

이 문제가 발생하는 주된 이유 중 하나는 루프 내에서 생성되거나 업데이트되는 텐서가 올바르게 관리되지 않아, 마지막으로 업데이트된 텐서의 참조만 반복적으로 리스트에 추가되기 때문이다.

즉, 각 반복에서 새로운 텐서를 생성하거나 기존 텐서의 복사본을 만들어야 하지만, 실제로는 같은 텐서 객체를 계속 사용하고 있는 것이다.

이를 해결하기 위해 각 반복에서 텐서를 새로 생성하거나, 명시적으로 텐서의 복사본을 만들어 리스트에 추가해야 한다. 

예를 들어, 

```python
J_M = []
for i in range(8):  # 독립 케이블이 8개라고 가정
    result = J_M_i(Pulley[i], End_Effector[i], X).clone()  # 텐서의 복사본을 생성
    J_M.append(result)
```

`.clone()` 메소드로 텐서의 복사본을 생성하여, 각 반복마다 독립적인 텐서 객체가 리스트에 추가되도록 하면 된다. 

이렇게 하면 각 반복에서 생성된 텐서 값이 올바르게 리스트에 저장된다.

맨 처음 문제는 다음과 같이 하면 된다.

``` python
def J_M_i(Pulley_i, End_Effector_i, X):
    if X.grad is not None:
        X.grad.data.zero_()  # 이전 그래디언트 제거

    X.requires_grad_(True)
    L_i = f_i(Pulley_i, End_Effector_i, X)
    L_i.backward()
    return X.grad.clone()
```





