오프-정책은 에이전트가 행동 정책과 대상 정책을 따로 가지고 있었다. 두 정책이 역할을 분담해서 행동 정책으로는 '탐색'을 대상 정책으로는 '활용'을 수행하도록 하였다. 

문제는 오프-정책에서 사용하는 중요도 샘플링이다. 그런데 중요도 샘플링은 가급적이면 피해야 할 기법이다

### 1. 중요도 샘플링의 문제점
---
중요도 샘플링만 봐도 뭔가 억지스러운 느낌이 있지 않은가?

>분산을 작게 하려면 두 확률 분포 (b와 π)를 가깝게 만들면 된다. 

과연 여기에 해당하는 상황이 얼만큼 있을까?? 중요도 샘플링은 결과가 불안정하기 쉽다는 문제를 안고 있다. 특히 두 정책의 확률 분포가 다를수록 중요도 샘플링에서 사용하는 가중치 ρ도 변동성이 커진다. 이에 따라 SARSA의 갱신시겡 등장하는 목표도 변경되기에 Q함수의 갱신 역시 심하게 불안정해진다. 

이 문제를 해결해주는 게 Q러닝이다. 

### 2. Q 러닝
---
Q 러닝의 대표적인 특징은 다음 세 가지로 요약할 수 있다.
1. TD법
2. 오프-정책
3. 중요도 샘플링을 사용하지 않음

Q러닝을 도출하기 위해 먼저 벨만 방정식과 SARSA의 관계부터 확인해야 한다. 그 다음 벨만 최적 방정식과 연관된 형태로 Q러닝을 도출한다. 

즉, 벨만 방정식에서 SARSA를 도출하고, 벨만 최적 방정식에서 Q러닝을 도출하겠다.

>1. 벨만 방정식 -> SARSA
>2. 벨만 최적 방정식 -> Q러닝

### 3. 벨만 방정식과 SARSA
---
벨만 방정식(Q함수)은 다음과 같다.

![[Pasted image 20240621194554.png]]

여기서 중요한 점은 2가지이다. 
1. 환경의 상태 전이 확률 p(s'|s, a)에 따른 다음 단계의 '모든'상태 전이를 고려한다
2. 에이전트의 정책 π에 따른 다음 단계의 '모든'행동을 고려한다. 

![[Pasted image 20240621194938.png|500]]

위의 경우를 보면 알겠지만 벨만 방정식은 다음 상태와 다음 행동의 '모든'후보를 고려한다.  따라서 SARSA는 벨만 방정식의 '샘플링 버전'으로 볼 수 있다. 

'샘플링 버전'이란 모든 전이가 아닌 '샘플링된 데이터'를 사용한다는 뜻이다. 따라서 SARSA의 백업 다이어그램은 다음 그림과 같다.

![[Pasted image 20240626142034.png]]


![[Pasted image 20240621005213.png]]

S<sub>t+1</sub>은 p(s' | s, a)로부터 샘플링한다. 그리고 다음 행동 A<sub>t+1</sub>은 정책 π(a | s)로부터 샘플링한다. 이때 SARSA의 TD목표는 R<sub>t</sub> + γ Q<sub>π</sub>(S<sub>t+1</sub>, A<sub>t+1</sub>)가 된다. 이 목표 방향으로 Q함수를 조금만 갱신하면 된다.

그럼 이제 벨만 최적 방정식도 이런식으로 대응을 해보자

### 4. 벨만 최적 방정식과 Q러닝
---
가치 반복법은 최적 정책을 얻기 위한 '평가'와 '개선'이라는 두 과정을 하나로 묶은 기법이다. 가치 반복법의 중요한 점은 벨만 최적 방정식에 기반하여 '단 하나의 갱신식을 반복'함으로써 최적 정책을 얻을 수 있다는 사실이다. (물론 해보니까 벨만 최적 방정식이 나왔더라로 풀었다.) 

벨만 최적 방정식에 의한 갱신인 동시에 이를 '샘플링 버전'으로 만드는 방법을 알아보자

먼저 Q 함수의 벨만 최적 방정식은 다음과 같다.

![[Pasted image 20240626143845.png]]

여기서 q<sub>*</sub>(s, a)는 최적 정책 π<sub>*</sub>에서의 Q함수를 뜻한다. 벨만 방정식과 달리 벨만 최적 방정식은 max연산자를 사용한다. 이를 백업 다이어그램으로 표현해보자. 

![[Pasted image 20240626143626.png|500]]

행동 A<sub>t+1</sub>은 Q함수가 가장 큰 행동이다. 이를 샘플링 버전으로 다시 작성해보자

![[Pasted image 20240626144700.png|500]]

위 방법에 기반한 것이 Q러닝이다. Q러닝에서 추정치 Q(S<sub>t</sub>, A<sub>t</sub>)의 목표는 R<sub>t</sub>+γ max<sub>a</sub>Q(S<sub>t+1</sub>, a)가 된다. 이 목표 방향으로 Q함수를 갱신한다. 수식으로는 다음과 같다. 

![[Pasted image 20240626145101.png]]

위 식에 따라 Q함수를 반복해서 갱신하면 최적 정책의 Q 함수에 가까워진다. 

제일 중요한 점은 Q함수가 가장 큰 행동으로 A<sub>t+1</sub>을 선택한다는 것이다. 특별한 정책에 따라 샘플링 하지 않고 max연산자로 선택한다. 따라서 (오프-정책 기법임에도) 중요도 샘플링을 이용한 보정이 필요가 없다.

Q러닝을 정리하자면
1. 오프-정책 기법이다
2. 대상 정책과 행동 정책을 따로 가지며 행동 정책으로는 '탐색'을 수행한다.
3. 흔하게 사용되는 행동 정책은 현재 추정치인 Q함수를 ɛ-탐욕화한 정책이다
4. 행동 정책이 결정되면 그에 따라 행동을 선택하여 샘플 데이터를 수집한다.
5. 에이전트가 행동할 때마다 Q함수를 갱신한다. 


### 코드
---
``` python
def greedy_probs(Q, state, epsilon=0, action_size=4):
    qs = [Q[(state, action)] for action in range(action_size)]
    max_action = np.argmax(qs)

    base_prob = epsilon / action_size
    action_probs = {action: base_prob for action in range(action_size)}  
    # action_probs = {0: ε/4, 1: ε/4, 2: ε/4, 3: ε/4}
    action_probs[max_action] += (1 - epsilon)
    return action_probs
```

``` python
class QLearningAgent:
    def __init__(self):
        self.gamma = 0.9
        self.alpha = 0.8
        self.epsilon = 0.1
        self.action_size = 4

        random_actions = {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}
        self.pi = defaultdict(lambda: random_actions)
        self.b = defaultdict(lambda: random_actions)  # 행동 정책
        self.Q = defaultdict(lambda: 0)

    def get_action(self, state):
        action_probs = self.b[state]  # 행동 정책에서 가져옴
        actions = list(action_probs.keys())
        probs = list(action_probs.values())
        return np.random.choice(actions, p=probs)

    def update(self, state, action, reward, next_state, done):
        if done:  # 목표에 도달
            next_q_max = 0
        else:     # 그 외에는 다음 상태에서 Q 함수의 최댓값 계산
            next_qs = [self.Q[next_state, a] for a in range(self.action_size)]
            next_q_max = max(next_qs)

        # Q 함수 갱신
        target = reward + self.gamma * next_q_max
        self.Q[state, action] += (target - self.Q[state, action]) * self.alpha

        # 행동 정책과 대상 정책 갱신
        self.pi[state] = greedy_probs(self.Q, state, epsilon=0)
        self.b[state] = greedy_probs(self.Q, state, self.epsilon)


env = GridWorld()
agent = QLearningAgent()

episodes = 10000
for episode in range(episodes):
    state = env.reset()

    while True:
        action = agent.get_action(state)
        next_state, reward, done = env.step(action)

        agent.update(state, action, reward, next_state, done)
        if done:
            break
        state = next_state

# [그림 6-15] Q 러닝으로 얻은 Q 함수와 정책
env.render_q(agent.Q)
```

![[Pasted image 20240626192158.png]]

참고로 500번하면 다음과 같다.

![[Pasted image 20240626192252.png]]

중간에 0이 있는 이유는 간단하다 ɛ-그리디 정책은 대개 높은 기댓값을 가지는 정책만 선택하기 때문이다. 그러다가 어쩌다가 ɛ확률로 다른 길을 가보는 것이다.





