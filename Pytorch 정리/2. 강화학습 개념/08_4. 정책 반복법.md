최종 목표는 최적 정책을 찾는 것이다. 벨만 최적 방정식 또한 벨만 방정식처럼 계산량이 너무 많다. 심지어 Max 연산으로 인해 비선형 방정식이라 더더욱 연립방정식을 풀어내는데는 시간이 오래 걸린다.

하지만 직접 계산이 가능하다면, 시간이 좀 걸리더라도 분명 도움은 될 것이다. 문제가 있다면 그러한 경우를 직접 계산하는게 말이 안된다. 비현실적인 경우가 더 많단 뜻이다. 따라서 앞서 구한 정책을 평가하는 방법을 응응하면 어떨까?

딥러닝의 손실 함수처럼 "정책을 평가"하여 "개선"하자는 의미이다. 

> 정책을 개선한단 의미는, 행동 원칙을 바꾸겠단 것이다.
> 1. 결정적 행동 결정 정책 - 왼쪽을 오른쪽으로 바꾸겠다.
> 2. 확률적 행동 결정 정책 - 왼쪽의 확률을 바꾸겠다 (0.4 -> 0.8)

### 1. 정책 개선
---
다음 기호를 사용하여 정책 개선 방법을 설명해보자
1. 최적 정책 : μ<sub>*</sub>(s)
2. 최적 정책의 상태 가치 함수 : v<sub>*</sub>(s)
3. 최적 정책의 행동 가치 함수(Q 함수) : q<sub>*</sub>(s, a)

![[Pasted image 20240519192044.png]]

최적 정책은 다음과 같다고 할 때, "임의의 결정적 정책"에 대해 다음과 같이 적용해보자  

![[Pasted image 20240519192242.png]]

이때 각 개념을 다음 기호로 표기해보자
1. 현 상태의 정책: μ(s)
2. 정책 μ(s)의 상태 가치 함수 : v<sub>μ</sub>(s)
3. 새로운 정책: μ'(s)

이전에 말했지만, 이러한 국소적인 후보 중에서 최선의 행동을 보여주는 알고리즘은 "탐욕 정책" 이라고 했다. 탐욕화된 정책 μ'(s)에는 재미난 특징이 있는데, 모든 상태 s에서 μ(s)와 μ'(s)가 같다면(정책이 바뀌지 않는다면) 그 정책 μ(s)는 이미 최적 정책이라는 것이다. 

식으로 풀어쓴다면,
![[Pasted image 20240519192605.png]]

이 식 자체가 바로 최적 정책이 만족하는 식이다. 탐욕화를 수행해도 정책이 그대로라면, 이미 최적 정책이란 것이다.

만약 정책이 바뀐다면, 정책 μ' 가 정책 μ와 달라진다면 새로운 정책은 반드시 기존 정책보다 좋아야 한다 따라서 모든 상태에서 "v<sub>μ'</sub>(s) >= v<sub>μ</sub>(s)" 이란 것이다

당연하게 상태 가치 함수는 변경될 것이다. 왜냐면 과거와 다르게 최상의 결과를 뽑는 행동을 한 다음이다. 당연히 미래에 얻는 보상은 다르지 않겠는가? 

상태 가치 함수가 원래는 

"현재 상태 쌓아논 보상 + 무작위 미래 상태의 수익 기댓값" = 5 였는데 
"현재 상태 쌓아논 보상 + 가장 최고의 행동 + 무작위 미래 상태의 수익 기댓값" 은 당연히 5보단 크지 않겠는가? 

### 2. 평가와 개선 반복
---
정책 개선 과정은 다음 그림과 같다. 

![[Pasted image 20240525211109.png|300]]

1. 먼저 𝜋<sub>0</sub>​이라는 정책에서 시작한다. 정책 𝜋<sub>0</sub>​은 확률적일 수도 있으므로 𝜇<sub>0</sub>(𝑠)가 아닌 𝜋<sub>0</sub>(𝑠|𝑎)로 표기한다.
2. 다음으로 정책 𝜋<sub>0</sub>​의 가치 함수를 평가하여 V<sub>0</sub>​를 얻는다. 반복적 정책 평가 알고리즘을 이용하면 된다.
3. 그리고 가치 함수 V<sub>0</sub>​를 이용하여 탐욕화를 수행한다. 탐욕 정책은 언제나 하나의 행동을 선택하므로 결정적 정책인 𝜇<sub>1</sub>​을 얻을 수 있다. 
4. 해당 행동을 하고 난 뒤의 가치 함수를 다시 평가한다. 분명 좋아져야 하는데 안좋아지거나 그대로면 해당 반복을 종료한다. 
5. 1 ~ 4 과정을 반복한다.

