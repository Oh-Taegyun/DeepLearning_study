### 1. 복습
---
![[Pasted image 20240615190814.png]]

1. 환경 상태 전이 - 다음 상태는 어떻게 전이되는가?
	- 절대적
		![[Pasted image 20240302013134.png|200]]
	
	- 확률적
		![[Pasted image 20240302013417.png|200]]

3. 보상 함수 - 어떤 보상을 얻게 되는가? 
		![[Pasted image 20240302014507.png]]

5. 행동 정책 함수 - 어떤 행동 정책을 사용할 것인가?
	- 결정적
		![[Pasted image 20240302015248.png|150]]
		
	-  확률적 
		![[Pasted image 20240302015540.png|150]]

6. 상태 가치 함수 - 수익의 기댓값(평균)을 나타내는 방법
	![[Pasted image 20240302023224.png|450]]


### 1. MDP 예제
---
두 칸 짜리 그리드 월드로 한번 이 예제를 풀어보자 

![[Pasted image 20240302024511.png]]


### 2. 백업 다이어그램
---
'상태, 행동, 보상'의 전이를 표현한 그래프이다.

![[Pasted image 20240302024934.png]]

왼쪽 백업 다이어그램은 현재 상태에 관련없이 무조건 오른쪽으로 간다는 가정을 나타낸것이고, 오른쪽 백업 다이어그램은 오른쪽과 왼쪽 둘 다 고려했을때의 그림이다.

왼쪽 백업 다이어그램의 특징은 다음과 같다.
1. 행동 정책이 결정적이다.
2. 환경의 상태 전이는 결정적이다. (행동 정책이 결정적이기 때문)

오른쪽 백업 다이어그램의 특징은 다음과 같다.
0. 오른쪽, 왼쪽으로 이동하는 확률이 존재한다. 
1. 행동 정책이 확률적이다
2. 환경의 상태 전이는 확률적이다 (행동 정책이 확률적이기 때문)

지금은 상태 전이와 행동 정책이 결정적일때를 생각해보자

### 3.  최적 정책 찾기
---
환경 상태 전이가 결정적이므로 다음 수식을 따른다.

![[Pasted image 20240302013134.png|200]]

행동 정책이 결정적이므로 다음 수식을 따른다.

![[Pasted image 20240302015248.png|150]]

다행히도, 2칸짜리라 에이전트가 행동할 수 있는게 그렇게 많진 않아서 모든 경우를 다 구할 수 있다. 다음은 각 행동 정책이다. 상태와 행동이 각 2개씩이라 결정적 정책은 총 2^2 = 4개가 존재한다. 

![[Pasted image 20240506215636.png|300]]

이 네가지 중에서 최적 정책이 존재한다. 

에이전트의 목표는 '수익'을 극대화 하는 것이다. 이를 파악하기 위해 수익의 기댓값을 지표로 삼아야 하는데 이걸 구하는 수식이 "상태 가치 함수"라고 한다.

![[Pasted image 20240302021826.png]]

대충 이를 계산하면 다음과 같다. (아직 수식 구하는 방법은 모른다)

![[Pasted image 20240506220359.png]]
![[Pasted image 20240506220415.png]]

계속 이렇게 구하면 다음과 같은 그래프를 얻을 수 있다.   

![[Pasted image 20240506220451.png]]

즉, 상태 2의 정책이 가장 좋은 행동 정책임을 알 수 있다. 







