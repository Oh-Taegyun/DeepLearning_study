강화 학습 알고리즘은 크게 가치 기반 기법과 정책 기반 기법으로 나뉜다. 일반적으로 DQN이나, SARSA는 가치 기반 기법이며, 정책 경사법은 정책 기반 기법이다. 

그런데 둘 다 사용하는 기법. 즉, 가치 기반이자 정책 기반인 기법도 존재한다.

앞에서 설명한 '베이스라인을 적용한 REINFORCE'에서 베이스라인으로 가치 함수를 사용한다면, 가치 기반이자 정책 기반인 기법이다.

그 외에도 행위자-비평자가 존재한다.

### 1. 행위자-비평자 도출
---
먼저 '베이스라인을 적용한 REINFORCE'를 복습하자. 이 기법에서 목적 함수의 기울기는

![[Pasted image 20240630024110.png]]

이다. 이 식에서 G<sub>t</sub>는 수익, b(S<sub>t</sub>)는 베이스라인을 나타낸다. 베이스라인은 임의의 함수를 사용할 수 있다. 이제부턴 신경망으로 모델링한 가치 함수를 베이스라인으로 사용해보자.

1. w : 가치 함수를 나타내는 신경망의 모든 가중치 매개변수
2. V<sub>w</sub>(S<sub>t</sub>) : 가치 함수를 모델링한 신경망

그러면 목적 함수의 기울기는 다음 식으로 표현된다. 

![[Pasted image 20240630024406.png]]

위 식에는 문제가 하나 있다. 수익 G<sub>t</sub>는 목표에 도달해야 비로소 정해진다는 문제이다. 즉, 목표에 도달하기 전까지는 정책이나 가치 함수를 갱신할 수 없다. 사실 몬테카를로법에 기초한 기법 모두에 해당하는 단점이다. 

![[Pasted image 20240630024941.png]]

물론 TD법으로 대처가 가능하다. 위 그림과 같이 V<sub>w</sub>(S<sub>t</sub>)를 학습할 때 몬테카를로법에서는 수익 G<sub>t</sub>를 사용한다. 한편 TD법에서는 R<sub>t</sub> + γ V<sub>w</sub>(S<sub>t+1</sub>)을 사용한다.


>![[Pasted image 20240620231940.png]]
>위 식에서 V<sub>π</sub>는 가치 함수의 추정치이고, R<sub>t</sub>+ϒ v<sub>π</sub>(S<sub>t+1</sub>)은 목적지(목표)이다. 이 목적지를 RD목표라고 하며 TD법은 V<sub>π</sub>(S<sub>π</sub>)를 TD 목표 방향으로 갱신한다.
>
>가치 함수의 출력이 신경망으로 담당한다면, TD법의 목표인 R<sub>t</sub> + γ V<sub>w</sub>(S<sub>t+1</sub>)에 가까워지도록 학습해야 하므로, 평균 제곱 오차를 사용할 수 있다는 것

가치 함수를 신경망으로 모델링 하면 V<sub>w</sub>(S<sub>t</sub>)의 값이 R<sub>t</sub> + γ V<sub>w</sub>(S<sub>t+1</sub>)에 가까워지도록 학습한다. 구체적으로 설명하면, V<sub>w</sub>(S<sub>t</sub>)와 R<sub>t</sub> + γ V<sub>w</sub>(S<sub>t+1</sub>)의 평균 제곱 오차를 손실 함수로 사용하여 경사 하강법으로 신경망의 가중치를 갱신한다. 

이제 몬테카를로법에 기반한 식, 

![[Pasted image 20240630024406.png]]

을 TD법으로 바궈보자. 이를 위해 G<sub>t</sub>대신 R<sub>t</sub> + γ V<sub>w</sub>(S<sub>t+1</sub>)을 사용하면 다음 식이 만들어진다. 

![[Pasted image 20240630025342.png]]

위 식에 기반한 방법이 바로 행위자-비평자 이다. 여기서 정책 π<sub>θ</sub>와 가치 함수 V<sub>w</sub>는 모두 신경망이며 이 두 신경망을 병렬로 학습한다. 정확하게는 정책 π<sub>θ</sub>은 위 식에 따라 학습하고, 가치 함수 V<sub>w</sub>는 TD법에 따라 V<sub>w</sub>(S<sub>t</sub>)의 값이 R<sub>t</sub> + γ V<sub>w</sub>(S<sub>t+1</sub>)에 가까워 지도록 학습한다.

행위자-비평자의 '행위자'는 정책 π<sub>θ</sub>에 해당하고, '비평자'는 가치 함수 V<sub>w</sub>에 해당한다. 즉, 행위자가 정책 π<sub>θ</sub>에 따라 선택한 행동의 좋은 정도를 비평자가 V<sub>w</sub>를 기준으로 평가한다는 뜻이다. 

``` python
class PolicyNet(nn.Module): # 정책 신경망
    def __init__(self, action_size):
        super().__init__()
        self.l1 = nn.Linear(4, 128)
        self.l2 = nn.Linear(128, action_size)

    def forward(self, x):
        x = F.relu(self.l1(x))
        x = F.softmax(self.l2(x), dim=1)
        return x


class ValueNet(nn.Module): # 가치 신경망
    def __init__(self):
        super().__init__()
        self.l1 = nn.Linear(4, 128)
        self.l2 = nn.Linear(128, 1)

    def forward(self, x):
        x = F.relu(self.l1(x))
        x = self.l2(x)
        return x


class Agent:
    def __init__(self):
        self.gamma = 0.98
        self.lr_pi = 0.0002
        self.lr_v = 0.0005
        self.action_size = 2

        self.pi = PolicyNet(self.action_size)
        self.v = ValueNet()

        self.optimizer_pi = optim.Adam(self.pi.parameters(), lr=self.lr_pi)
        self.optimizer_v = optim.Adam(self.v.parameters(), lr=self.lr_v)

    def get_action(self, state):
        state = torch.tensor(state[np.newaxis, :])
        probs = self.pi(state)
        probs = probs[0]
        m = Categorical(probs)
        action = m.sample().item()
        return action, probs[action]
		
    def update(self, state, action_prob, reward, next_state, done):
        state = torch.tensor(state[np.newaxis, :])
        next_state = torch.tensor(next_state[np.newaxis, :])
		
        target = reward + self.gamma * self.v(next_state) * (1 - done)
        target.detach()
        v = self.v(state)
        loss_fn = nn.MSELoss()
        loss_v = loss_fn(v, target)
		
        delta = target - v
        loss_pi = -torch.log(action_prob) * delta.item()
		
        self.optimizer_v.zero_grad()
        self.optimizer_pi.zero_grad()
        loss_v.backward()
        loss_pi.backward()
        self.optimizer_v.step()
        self.optimizer_pi.step()
```

