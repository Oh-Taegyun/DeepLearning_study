밴디트 문제에서는 에이전트가 어떤 행동을 취하든 다음에 도전할 문제의 설정은 바뀌지 않았다. 하지만 바둑같은 경우 에이전트가 어떤 수를 두면 매 시간마다 상황이 시시각각 변하게 된다. 다라서 에이전트는 상황이 변하는 것을 고려해서 최선의 수를 두어야 한다.

비정상 문제와는 살짝 다른게 비정상 문제는 '보상의 확률 분포'는 에이전트가 어떤 행동을 취했느냐와는 관계가 없이 시간에 따라서만 변화한다. 앞으로 우리가 하는 것은 에이전트의 행동으로 환경이 변화하는 경우를 다루는 것이다.

### 1. 마르코프 결정 과정(MDP)이란?
---
마르코프 결정 과정(Markov Decision Process, MDP)은 수학적 프레임워크로, 순차적 의사 결정 문제를 모델링하는 데 사용된다. MDP는 다음의 네 가지 요소로 구성된다.

1. **상태 집합 (States, S)**: 에이전트가 있을 수 있는 모든 가능한 상태의 집합
2. **행동 집합 (Actions, A)**: 에이전트가 각 상태에서 취할 수 있는 모든 가능한 행동의 집합
3. **상태 전이 확률 (Transition Probabilities, P)**: 특정 상태에서 특정 행동을 취했을 때 다른 상태로 전이될 확률. P(s' | s, a)는 상태 s에서 행동 a를 취했을 때 다음 상태가 s′가 될 확률이다. 
4. **보상 함수 (Reward Function, R)**: 특정 상태 s에서 행동 a를 취했을 때 받는 보상을 나타낸다. R(s,a)는 상태 s에서 행동 a를 했을 때 얻는 즉각적인 보상이다. 

MDP는 다음과 같은 목적을 가지고 있다.

- **정책 (Policy, π)**: 각 상태에서 에이전트가 어떤 행동을 취할지 결정하는 함수이다. 최적 정책은 장기적인 보상을 최대화하는 행동을 결정한다.

MDP를 통해 에이전트는 다음의 절차를 따른다.

1. 현재 상태 s를 관찰한다.
2. 정책 π에 따라 상태 s에서 취할 행동 a를 선택한다.
3. 행동 a를 수행하고, 새로운 상태 s′로 전이된다.
4. 보상 R(s, a)를 받는다.
5. 새로운 상태 s′에서 다시 절차를 반복한다.

최적 정책을 찾기 위한 알고리즘으로는 가치 반복(Value Iteration)과 정책 반복(Policy Iteration) 등이 있다.

원 큐에 정리하자면 다음 그림이 나온다.

![[Pasted image 20240615190820.png]]

---

에이전트가 환경과 상호작용하면서 행동을 결정하는 과정을 말한다.

![[Pasted image 20240302011453.png]]

위 그림에서는 에이전트의 행동에 따라서 에이전트가 처하는 상황이 다르다. 이를 일반적으로 표현하면 환경 상태에 따른 에이전트의 상황이 다르다. 이 상황을 강화학습에서는 `state`라고 한다. 

강화학습에서 "상태"는 에이전트가 환경과 상호작용하는 과정에서 환경의 특정한 상태를 나타낸다. 이 상태는 에이전트가 결정을 내리고 행동을 선택하는 데 사용된다. 상태는 보통 관측 가능한 정보로 표현되며, 이 정보는 에이전트가 취할 수 있는 행동을 결정하는 데 중요한 역할을 한다. 상태는 환경의 내부 상태뿐만 아니라 에이전트의 관측을 통해 알 수 있는 외부 상태도 포함할 수 있다.

지금처럼은 무조건 왼쪽으로 가는게 이득인데 다음과 같을때는 어떠한가?

![[Pasted image 20240302011804.png|500]]

이럴때는 당연하게 오른쪽으로 무조건 이동하는게 이득이다. 


### 2. 에이전트와 환경의 상호작용
---
![[Pasted image 20240302012047.png|300]]

상태 S_t에서 시작해서 에이전트가 행동 A_t를 수행하여 보상 R_t를 얻고 다음 상태인 S_(t+1)로 전환된다. 이러한 에이전트와 환경의 상호작용은 실제로 다음과 같은 정리를 만들어낸다.

![[Pasted image 20240302012250.png]]

어찌보면 이것도 시간에 따라서 진행되는 것이므로 엄연히 `시계열 데이터`이다. 


### 3. 환경 상태 전이
---
환경 상태는 어떻게 전이되는가? 에 대해서 나타내는 수식이다.

2가지가 있다.
1. 절대적 상태 전이 함수
2. 확률적 상태 전이 함수

![[Pasted image 20240302012924.png]]

왼쪽 그림은 에이전트가 왼쪽으로 이동하려는 행동을 선택했다. 그 결과 에이전트는 반드시 왼쪽으로만 이동했다. 이러한 성질을 결정적이라고 한다.

다음 상태 `s'`는 현재 상태 `s`와 행동 `a`에 의해 '단 하나로' 결정된다. 따라서 함수로는 다음처럼 표현할 수 있다. 

![[Pasted image 20240302013134.png|200]]
`Next_State = f(State, Action)`

이러한 함수를 `상태 전이 함수`라고 한다. 

반면 오른쪽 그림은 확률적으로 이동하고 있다. 에이전트가 왼쪽으로 이동하려는 행동을 선택하더라도 0.9 확률로만 왼쪽으로 이동하고, 0.1의 확률로는 그 자리에 머물러 있는 것을 의미한다. 상태 전이가 결정적이여도 확률적으로 표현할 수 있다. 1.0의 확률로 왼쪽으로 이동한다고 기술하면 된다.

확률적 상태 전이를 표기하는 방법은 다음과 같이 서술할 수 있다.

![[Pasted image 20240302013417.png|200]]

상태 s에서 행동 a를 선택했을 때 s'로 전이될 확률(P)을 나타내는 식이다.

![[Pasted image 20240302013527.png]]

다음 상태를 정하는데 '현재' 상태 s와 행동 a만이 영향을 준다. 다시 말해서 상태 전이에서는 과거의 정보, 즉 지금까지 어떤 상태를 거쳐 왔고 어떤 행동들을 취해 왔는지는 신경쓰지 않는다.

이처럼 현재의 정보만 고려하는 성질을 **`마르코프 성질`** 이라고 한다.

MDP는 마르코프 성질을 만족한다고 가정하고 상태 전이(와 보상)를 모델링한다. 마르코프 성질을 도입하는 가장 큰 이유는 문제를 더 쉽게 풀기 위해서이다. 만약 마르코프 성질을 따른다고 가정하지 않는다면 과거의 모든 상태와 행동까지 고려해야 해서, 그 조합이 기하급수적으로 많아지기 때문이다.  


### 4. 보상 함수
---
보상이 `결정적`으로 주어진다고 할 때, 에이전트가 상태 s에서 행동 a를 수행하여 다음 상태 s'가 되었을 때 얻는 보상을 `r(s, a, s')`라는 함수로 정의한다. 

![[Pasted image 20240302014507.png]]

이렇게 정의할 수 있다.
`Reward = r(State, Action, Next_State)`


### 5. 정책 함수
---
마르코프 성질에 의할때, 정책에서 중요한 점은 에이전트는 '현재 상태'만으로 행동을 결정할 수 있다는 것이다. 

MDP의 마르코프 성질이라는 특성은 에이전트에 대한 제약이 아니라 '환경에 대한 제약'으로 볼 수 있다. 즉, 마르코프 성질을 만족하도록 '상태'를 관리하는 책임이 환경 쪽쪽에 있다는 것이다. 에이전트 관점에서 보면 최선의 선택에 필요한 정보가 모두 현재 상태에 담겨 있기 때문에 현재만을 바라보고 행동할 수 있다는 것이다.

이때 행동을 결정하는 방식은 2가지가 있다.
1. 결정적 행동 결정 정책
2. 확률적 행동 결정 정책

![[Pasted image 20240302015115.png]]

##### 1. 결정적 행동 결정 정책

![[Pasted image 20240302015248.png|150]]

현재 상태 's'를 전해주면 행동 'a'를 반환하는 함수이다.

##### 2. 확률적 행동 결정 정책

![[Pasted image 20240302015540.png|150]]

현재 상태 's'에서 행동 'a'를 취할 확률이다 다음과 같이 표기할 수 있다.

![[Pasted image 20240302015620.png|600]]

`Pi(Action | State)`

### 6. MDP의 목표
---
![[Pasted image 20240302015930.png]]

이러한 틀 안에서 최적 정책을 찾는 것이 MDP의 목표이다. 

MDP는 문제에 따라서 일회성 과제와 지속적 과제로 나뉜다. 

1. 일회성 과제
	끝이 '있는' 과제, 바둑을 예시로 들 수 있다. 언젠가 끝이 있고 결국 승리/패배/무승부 중 하나로 귀결된다

2. 지속적 과제
	끝이 '없는' 과제, 예를 들어 재고 관리와 같은 것이다. 판매량과 재고량을 보고 최적의 구매량을 결정해야 한다. 

### 7. 수익
---
![[Pasted image 20240302020755.png]]

이럴때 수익은 다음과 같이 정의할 수 있다. 

![[Pasted image 20240302020908.png|350]]

이떄 ϒ(0<ϒ<1)은 할인율이다. 왜 할인율을 붙여놨을까? 일단, 지속적 과제에서 수익이 무한대가 되지 않도록 조절하는 것이다. 수렴하지 않고 수익이 무한대로 늘어난다면, 우리는 최적의 정책을 찾을 수 없을것이다.

또한 할인율은 가까운 미래의 보상을 더 중요하게 보이도록 한다. 이는 생물의 모든 행동 원칙인데, 예를 들어서 지금 1만원 받기와 1년 뒤 2만원 받기는 누가 뭐래도 지금 1만원 받는게 더 큰 매력을 느낀다.


### 8. 상태 가치 함수
---
에이전트의 목표는 '수익'을 극대화 하는 것이다. 

주의할 점은 에이전트와 환경이 '확률적'으로 동작할 수 있다는 것이다. 에이전트는 다음 행동을 확률적으로 결정할 수 있고, 상태 역시 확률적으로 전이될 수 있다. 그렇다면 얻는 수익도 '확률적'으로 달라질 것이다. 

이렇게 확률적 동작에 대응하기 위해서 기댓값, 즉 '수익의 기댓값'을 지표로 삼아야 한다.

![[Pasted image 20240302021826.png]]

이 수식을 상태 가치 함수라고 한다. 

에이전트의 정책 π는 조건으로 주어진다. 정책 π가 바뀌면 에이전트가 얻는 보상도 바뀌고 총합인 수익도 바뀌기 때문이다. 이런 특징을 반영해서 식을 다음처럼 쓸 수 있다.

![[Pasted image 20240302023224.png|450]]


### 9. 최적 정책과 최적 가치 함수
---
강화 학습의 목표는 최적 정책을 찾는 것이다. 

그럼 최적 정책은 과연 무엇일까? 두 가지 정책 π와 π’가 있다고 가정하자. 이 두 정책에서 각각 상태 가치 함수 v(s)가 결정된다고 하자

![[Pasted image 20240302023640.png|500]]

이런 그래프에서는 어떤 부분에서는 π가 더 좋은 정책이고, 어떤 부분에서는 π’가 더 좋은 정책이다. 이런 경우 두 정책의 우열을 가릴 수 없다.

하지만 다음 그림의 경우

![[Pasted image 20240302023913.png|500]]

이 그림에서는 모든 경우에서 π’가 더 좋은 정책이다. 두 정책의 우열을 가릴려면 하나의 정책이 다른 정책보다 '모든 상태'에서 더 좋거나 최소한 똑같아야 한다. 만약 모든 상태에서 둘의 상태 가치 함수가 같다면 똑같이 좋은 정책이다.

MDP에서는 최적 정책이 적어도 하나는 존재한다는 사실이다. 그리고 그 최적 정책은 '결정적 정책'이다. 결정적 정책에서는 각 상태에서는 행동이 유일하게 결정된다. (어찌보면 당연하다 미래를 알고있다면, 과거를 맘대로 바꿀 수 있다면 어떤 선택이든 100% 좋은 선택만 따라가는 기적같은 사건이 있다고 생각할 수 있지 않은가?)

최적 정책의 상태 가치 함수를 최적 상태 가치 함수라고 한다. 









