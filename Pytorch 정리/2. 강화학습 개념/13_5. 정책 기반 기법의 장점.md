그렇다면 지금까지 살펴본 정책 기반 기법들의 장점은 무엇일까? 

### 1. 정책을 직접 모델링하기 때문에 효율적이다
---
우리가 궁극적으로 얻고자 하는 것은 최적 정책이다. 가치 기반 기법은 가치 함수를 추정하고 이를 바탕으로 정책을 결정한다. 반면, 정책 기반 기법은 정책을 ‘직접’ 추정한다. 문제에 따라서는 가치 함수의 형태가 복잡한 반면, 최적 정책은 단순할 수 있다. 이런 경우 정책 기반 기법이 더 빠르게 학습하리라 기대할 수 있다.

### 2. 연속적인 행동 공간에서도 사용할 수 있다
---
지금까지 살펴본 강화 학습 문제들은 모두 행동 공간이 이산적이라고 가정했다. 예를 들어 <카트 폴>은 왼쪽과 오른쪽 중 하나의 행동을 선택한다. 이러한 이산적 행동 공간에서는 (연속적이지 않은) 몇 가지 후보 중 하나의 행동을 선택한다.

한편, 연속적인 행동 공간도 생각할 수 있다. 예를 들어 OpenAI Gym의 <Pendulum시계추>는 막대의 중앙에 회전력 (torque) 토크을 가해 막대를 들어 올리는 과제이다. 이 과제에서의 행동은 ‘회전력을 어느 정도 가할 것인가’가 되며, 그 값은 가령 2.05나 -0.24 등의 연속적인 값이 될 수 있다.

가치 기반 기법은 행동 공간이 연속적이면 적용하기 어려워진다. 몇 가지 방법을 생각해볼 수 있는데, 그중 하나가 연속적인 행동 공간을 이산화하는 것이다. 그러나 어떻게 이산화할 것인가(양자화quantization)는 어려운 문제이며, 적합한 방법은 과제에 따라 다르다. 대부분의 문제에서는 적합한 이산화 방법을 시행착오를 거쳐 찾아내야 한다.

반면, 정책 기반 기법은 연속적인 행동 공간에도 간단하게 대응할 수 있다. 예를 들어 신경망의 출력이 정규분포라고 한다면 신경망은 정규분포의 평균과 분산을 출력할 수 있다. 그 평균과 분산을 바탕으로 샘플링하면 연속적인 값을 얻을 수 있다.

![[Pasted image 20240630030342.png]]

### 3. 행동이 선택될 확률이 부드럽게 변화한다
---
가치 기반 기법에서 에이전트는 주로 ε-탐욕 정책에 따라 행동한다. 그렇다면 기본적으로 Q 함수의 값이 가장 큰 행동이 선택되고, 이때 Q 함수가 갱신되면서 값이 최대가 되는 행동이 바뀌면 행동하는 방식도 급격하게 달라진다. 반면, 정책 기반 기법에서는 각 행동의 확률이 소프트맥스 함수에 의해 정해진다. 따라서 정책의 매개변수를 갱신하는 과정에서 각 행동의 확률이 부드럽게 변화한다. 이 덕분에 정책 경사법의 학습이 안정적으로 이루어질 수 있다.

이상이 정책 기반 기법의 장점입니다. 하지만 정책 기반 기법이 항상 좋은 것은 아니니 주의해야 한다. 과제에 따라 더 잘하기도 혹은 못하기도 한다. 또한 정책 기반이는 가치 기반이는 매우 다양한 구현 알고리즘이 존재한다. 그래서 알고리즘을 선택할 때는 이러한 점을 잘 고려해야 한다.


