에이전트의 구현 방법에는 '분포 모델'과 '샘플 모델'이 있다. 지금까지 구현한 방식은 분포 모델에 해당한다. 그런데 샘플 모델을 이용하면 더 간단하게 구현할 수 있다.

1. 분포 모델 : 확률 분포를 정확하게 알고 있다.
2. 샘플 모델 : 샘플링만 할 수 있다면 뭔들 못하겠는가?

확률적 행동을 구현하는 방법에는 '분포 모델'과 '샘플 모델'이 있다. 이전에 환경에는 분포 모델과 샘플 모델이 있다고 했는데, 에이전트도 마찬가지이다.

### 1. 분포 모델
---
분포 모델은 확률 분포를 명시적으로 유지하는 모델이다. 그래서 무작위로 행동하는 에이전트라면 분포 모델로 다음처럼 구현할 수 있다.

``` python
class QLearningAgent:
    def __init__(self):
        random_actions = {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}
        self.pi = defaultdict(lambda: random_actions)

    def get_action(self, state):
        action_probs = self.pi[state]  # 행동 정책에서 가져옴
        actions = list(action_probs.keys())
        probs = list(action_probs.values())
        return np.random.choice(actions, p=probs)
```

각 상태에서의 행동 확률 분포를 self.pi 변수에 유지한다. 그리고 실제 행동을 할 때는 이 확률 분포를 토대로 샘플링한다. 이것이 에이전트를 분포 모델로 구현하는 방법이다. 이처럼 확률 분포를 명시적으로 유지한다는 점이 분포 모델의 특징이다.

### 2. 샘플 모델
---
샘플 모델은 '샘플링이 가능하다'라는 조건만 만족하면 되는 모델이다. 확률 분포를 유지할 필요가 없기 때문에 분포 모델보다 간단하게 구현할 수 있다. 똑같이 무작위로 행동하는 에이전트를 샘플 모델로는 다음처럼 구현이 가능하다.

``` python
class RandomAgent:
    def get_action(self, state):
        return np.random.choice(4)
```

그냥 네 가지 행동 중 하나를 무작위로 선택만 하게 하면 되기 때문이다.

### 3. 샘플 모델 버전의 Q러닝
---

역할 측면에서 보면 에이전트의 정책은 두 가지이다. 
1. 대상 정책 : 평가와 개선의 대상으로서의 정책이다. 즉, 정책에 대해 평가한 다음 개선한다. 이러한 정책을 "대상 정책"이라고 한다. 

2. 행동 정책 : 다른 하나는 에이전트가 실제로 행동을 취할 때 활용하는 정책이다. 이 정책에 따라 '상태, 행동 보상'의 샘플 데이터가 생성된다. 이러한 정책이 "행동 정책"이다

``` python
class QLearningAgent:
    def __init__(self):
        self.gamma = 0.9
        self.alpha = 0.8
        self.epsilon = 0.1
        self.action_size = 4

        random_actions = {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}
        # self.pi = defaultdict(lambda: random_actions)
        # 대상 정책은 확률 분포로 유지되고 있다. 무엇보다 대상 정책을 더 이상 사용할 필요가 없다. 샘플 모델은 그냥 랜덤으로 행동하면 된다. 
        self.b = defaultdict(lambda: random_actions)  # 행동 정책
        self.Q = defaultdict(lambda: 0)

    def get_action(self, state):
		# 이전 코드에서는 update()에서 행동 정책을 개선했으나, 여기서는 바로 ɛ그리디 정책을 진행하도록 함
	    self.b[state] = greedy_probs(self.Q, state, self.epsilon)
		
        action_probs = self.b[state]  # 행동 정책에서 가져옴
        actions = list(action_probs.keys())
        probs = list(action_probs.values())
        return np.random.choice(actions, p=probs)

    def update(self, state, action, reward, next_state, done):
        if done:  # 목표에 도달
            next_q_max = 0
        else:     # 그 외에는 다음 상태에서 Q 함수의 최댓값 계산
            next_qs = [self.Q[next_state, a] for a in range(self.action_size)]
            next_q_max = max(next_qs)

        # Q 함수 갱신
        target = reward + self.gamma * next_q_max
        self.Q[state, action] += (target - self.Q[state, action]) * self.alpha

        # 행동 정책과 대상 정책 갱신이 더 이상 필요 없다. 
        # self.pi[state] = greedy_probs(self.Q, state, epsilon=0)
        # self.b[state] = greedy_probs(self.Q, state, self.epsilon)
```

이를 반영한다면 다음과 같이 만들 수 있다.

``` python
class QLearningAgent:
    def __init__(self):
        self.gamma = 0.9
        self.alpha = 0.8
        self.epsilon = 0.1
        self.action_size = 4
        self.Q = defaultdict(lambda: 0)

    def get_action(self, state):
        if np.random.rand() < self.epsilon:  # epsilon의 확률로 무작위 행동
            return np.random.choice(self.action_size)
        else:                                # (1 - epsilon)의 확률로 탐욕 행동
            qs = [self.Q[state, a] for a in range(self.action_size)]
            return np.argmax(qs)

    def update(self, state, action, reward, next_state, done):
        if done:
            next_q_max = 0
        else:
            next_qs = [self.Q[next_state, a] for a in range(self.action_size)]
            next_q_max = max(next_qs)

        target = reward + self.gamma * next_q_max
        self.Q[state, action] += (target - self.Q[state, action]) * self.alpha

```

특이한 점은 행동 정책인 `self.b`마저 삭제했다는 것이다. get_action()메서드에서는 self.b를 사용하지 않고 대신 Q함수를 이용해 ɛ-탐욕 정책에 다른 행동 선택을 직접 구현했다. 

보면 알겟지만 이번 코드에는 정책을 확률 분포로 유지하지 않는다. 더 정확하게는 정책 자체를 유지하지 않는다. 확률 분포를 유지할 필요가 없어서 코드가 훨씬 간결해졌다. 


### 4. 분포, 샘플 모델 장단점
---
둘은 무슨 차이가 있을까? 간단하게 두 코드를 놓고 비교해보자.

``` python
# 분포 모델
def greedy_probs(Q, state, epsilon=0, action_size=4):
    qs = [Q[(state, action)] for action in range(action_size)]
    max_action = np.argmax(qs)

    base_prob = epsilon / action_size
    action_probs = {action: base_prob for action in range(action_size)}  
    # action_probs = {0: ε/4, 1: ε/4, 2: ε/4, 3: ε/4}
    action_probs[max_action] += (1 - epsilon)
    return action_probs

class QLearningAgent:
    def __init__(self):
        self.gamma = 0.9
        self.alpha = 0.8
        self.epsilon = 0.1
        self.action_size = 4

        random_actions = {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}
        self.pi = defaultdict(lambda: random_actions)
        self.b = defaultdict(lambda: random_actions)  # 행동 정책
        self.Q = defaultdict(lambda: 0)

    def get_action(self, state):
        action_probs = self.b[state]  # 행동 정책에서 가져옴
        actions = list(action_probs.keys())
        probs = list(action_probs.values())
        return np.random.choice(actions, p=probs)

    def update(self, state, action, reward, next_state, done):
        if done:  # 목표에 도달
            next_q_max = 0
        else:     # 그 외에는 다음 상태에서 Q 함수의 최댓값 계산
            next_qs = [self.Q[next_state, a] for a in range(self.action_size)]
            next_q_max = max(next_qs)

        # Q 함수 갱신
        target = reward + self.gamma * next_q_max
        self.Q[state, action] += (target - self.Q[state, action]) * self.alpha

        # 행동 정책과 대상 정책 갱신
        self.pi[state] = greedy_probs(self.Q, state, epsilon=0)
        self.b[state] = greedy_probs(self.Q, state, self.epsilon)

# --------------------------------------------------------------

# 샘플 모델
class QLearningAgent:
    def __init__(self):
        self.gamma = 0.9
        self.alpha = 0.8
        self.epsilon = 0.1
        self.action_size = 4
        self.Q = defaultdict(lambda: 0)

    def get_action(self, state):
        if np.random.rand() < self.epsilon:  # epsilon의 확률로 무작위 행동
            return np.random.choice(self.action_size)
        else:                                # (1 - epsilon)의 확률로 탐욕 행동
            qs = [self.Q[state, a] for a in range(self.action_size)]
            return np.argmax(qs)

    def update(self, state, action, reward, next_state, done):
        if done:
            next_q_max = 0
        else:
            next_qs = [self.Q[next_state, a] for a in range(self.action_size)]
            next_q_max = max(next_qs)

        target = reward + self.gamma * next_q_max
        self.Q[state, action] += (target - self.Q[state, action]) * self.alpha
```

> 두 모델의 근본적인 차이점은 에이전트가 환경에 대한 정보를 가지고 있느냐 아니냐의 차이이다. 

둘다 최종적으로는 Q를 반복해서 정책을 평가하고 개선한 뒤에 최적 정책을 찾자는 취지이다. 하지만 환경 모델을 알고 있다면, 직접 행동하지 않고도 시뮬레이션 만으로 최적의 행동 전략을 예측할 수 있다. 

근데 해보면 알겠지만 둘 다 많은 시행착오를 거쳐야 한다. 즉, 현재의 그리드 월드 문제에서는 에이전트가 환경 모델을 가지고 있더라도 뭐 쓸때가 없어서 그냥 냅다 샘플 모델로 돌리는 것이나 그닥 차이가 없다는 것, 

분포 모델을 사용할 거면 그리드 문제라도 뭔가 여태껏 쌓아놓은 정보를 기반으로 뭔갈 더 해야한다. 

---
##### 1. 분포 모델 장단점
분포 모델에서는 환경의 동적인 특성을 명시적으로 모델링한다. 이 모델은 상태와 행동에 따른 결과 상태 및 보상의 분포를 추정하려고 한다.

#### 장점
1. **샘플 효율성**: 분포 모델은 주어진 데이터를 최대한 활용하여 환경의 동작을 예측한다. 이는 상대적으로 적은 데이터로도 좋은 성능을 낼 수 있게 해준다.
2. **계획 가능성**: "환경의 모델을 알고 있기 때문에" 미래의 여러 가능성을 시뮬레이션하여 최적의 행동 전략을 계획할 수 있다.

#### 단점
1. **모델 오류**: 환경을 정확하게 모델링하는 것은 매우 어려울 수 있으며, 모델의 오류가 결국 학습된 정책의 성능에 부정적인 영향을 미칠 수 있다.
2. **복잡성**: 정확한 모델을 개발하고 유지하기 위해 필요한 계산적 자원과 시간이 많이 소모된다.

---
##### 2. 샘플 모델 장단점
샘플 모델은 환경의 모델을 명시적으로 학습하지 않고, 경험을 통해 직접 최적의 정책을 학습한다.

#### 장점
1. **구현의 단순성**: 환경의 내부 동작을 모델링할 필요 없이, 보상을 최대화하는 방법을 학습한다.
2. **강건성**: 모델 오류의 영향을 받지 않으므로, 복잡하거나 예측하기 어려운 환경에서도 잘 작동할 수 있다.

#### 단점
1. **샘플 비효율성**: 새로운 상태나 행동에 대해 학습하기 위해서는 많은 수의 샘플이 필요하며, 이는 많은 시간과 자원을 요구할 수 있다.
2. **단기적 최적화**: 직접적인 경험만을 기반으로 학습하기 때문에, 단기적인 보상에 치우칠 수 있고 장기적인 최적화가 어려울 수 있다.

