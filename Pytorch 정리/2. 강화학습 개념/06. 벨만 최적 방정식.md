벨만 방정식은 어떤 정책 π에 성립하는 방정식이다. 하지만 우리가 최종적으로 찾으려는 것은 최적 정책이다. 최적 정책이란 모든 상태에서 상태 가치 함수가 최대인 정책이다. 

그럼 어떻게 해야 벨만 방정식을 최적 정책이라고 표현할 수 있을까?

### 1. 상태 가치 함수의 벨만 최적 방정식
---
![[Pasted image 20240518205255.png]]

만약 최적 정책을 다음과 같이 나타낸다면,

![[Pasted image 20240518205402.png]]

최적 정책의 가치 함수는 ![[Pasted image 20240518205531.png|50]]일때 최적 정책에 의해 선택되는 행동 a는 뭘까?? 왜 이런 고민을 하냐면, 최적 정책일때의 행동이 바로 에이전트가 최대의 이득을 얻는 행동이기 때문이다. 

>벨만 방정식은 "현재 상태 쌓아논 보상 + 미래 상태의 수익 기댓값"이라고 했는데, 만약 극적인 행동 정책으로 미래 상태의 수익 기댓값을 비정상적으로 높이면 어떻게 될까? 에 대한 궁금증이다. 
>판타지 세계로 이야기 하자면 미래를 알고 과거로 회귀해서 강해지는 경우 어떤 결과가 될까? 이다...

만약 세 개의 행동 후보가 있다고 할때 어떤 확률 분포로 행동을 선택해야 할까?

![[Pasted image 20240518205842.png|600]]

최적 정책이기 때문에 무조건 (a3)을 선택해야 한다. (그래야 상태 가치 함수 v*(s)가 큰 값을 얻기 때문이다) 

무조건 a3를 선택한다면 우리는 이 문제를 확률적 정책이 아닌 결정적 정책으로 볼 수 있다. (액션을 결정했으므로..... ) 이를 이용해 정책 함수를 살짝 바꿔보자

![[Pasted image 20240518210433.png]]

여기서 μ(s)는 이미 결정된 상수값이니까, 이 정책이 최대값을 가질라면 다음과 같다.

![[Pasted image 20240518210723.png]]

![[Pasted image 20240518212614.png]]

이 식이 바로 벨만 최적 방정식이다. 

기존의 벨만 방정식은 모든 지점에서 기대되는 수익의 기댓값이다. 즉, 모든 경우를 고려한 것이고 이 벨만 최적 방정식은 최적의 경우 1가지만 고려한 것이다. 

>1. 상태 가치 함수의 벨만 최적 방정식
	- 현재 상태에서, 모든 정책을 고려했을 때, 가장 최고의 시나리오로 구성된 최고의 수익
	- "현재 상태 쌓아논 보상 + 수 많은 미래 상태에서 가장 최고의 보상을 얻는 최댓값"

상태 가치 함수의 벨만 방정식 : "현재 상태 쌓아논 보상 + 무작위 미래 상태의 수익 기댓값"(밀를 모를때)
상태 가치 함수의 벨만 최적 방정식 : "현재 상태 쌓아논 보상 + 수 많은 미래 상태에서 가장 최고의 보상을 얻는 최댓값" (미래를 알고 행동할때)

미래를 알고 행동하는건 흔한 웹툰의 설정이지 않은가? 고수가 회귀해서 최고의 수련만 한다던가 그런거....

### 2. 행동 가치 함수의 벨만 최적 방정식
---
![[Pasted image 20240518212223.png]]

이렇게 유도할 수 있다. 

>2. 행동 가치 함수의 벨만 최적 방정식
	- 현재 상태에서, 모든 행동 중 1개의 행동만 고려했을 때, 가장 최고의 시나리오로 구성된 가장 최고의 수익

MDP에서는 결정적 최적 정책이 하나 이상 존재한다. 문제에 따라서 최적 정책이 여러개 일 수 있다(모든 문제에 대한 최선의 해답이 둘 이상 있을 수 있지 않은가?) 하지만, 그 가치 함수들의 값은 모두 동일해야한다. (미래를 아는 경우 무조건 성공할 수 밖에 없는 선택지는 여러개일 수 있지만, 그에 따른 보상은 모두 동등하다) 

따라서 최적 정책의 가치 함수와 최적 정책의 Q함수도 하나만 존재한다.

물론 밑에서 쓸 공식은

![[Pasted image 20240518204720.png]]
이긴 하다.

행동 가치 함수의 벨만 방정식 : "현재 상태 쌓아논 보상 + 임의의 1가지 행동 + 랜덤한 미래 상태의 수익 기댓값"
행동 가치 함수의 벨만 최적 방정식 : "현재 상태 쌓아논 보상 + 임의의 1가지 행동 + 가장 최고의 시나리오 보상을 얻는 최댓값"


### 3. 최적 정책 구하기
---
최적 행동 가치 함수를 이미 알고 있다고 가정하자, 그럼 상태 s에서의 최적 행동은 다음과 같이 구할 수 있다.

![[Pasted image 20240518215512.png]]

(각 행동의 최고의 행동 가치 함수 중 가장 행동을 골라 따른다. )

argmax는 최댓값을 만들어내는 인수를 반환한다. 최적 행동 가치 함수를 알고 있는 경우, 함수의 값이 최대가 되는 행동을 선택하면 된다. 그 행동을 선택하는 것이 바로 최적 정책이다.

>행동 가치 함수의 벨만 최적 방정식 : "현재 상태 쌓아논 보상 + 임의의 1가지 행동 + 가장 최고의 시나리오 보상을 얻는 최댓값"
>
>그럼 그 임의의 1가지 행동중 가장 최고인 행동을 하나 뽑으면 보상이 세계 최고 수준이 되겠네?에 대한 답변이다.

다음 식은 행동 가치 함수를 이용한 벨만 방정식이다.

![[Pasted image 20240518220230.png]]

만약 이게 최적 정책이라면 

![[Pasted image 20240518221035.png]]

이고 이것을 다시 위 식에 적용한다면,

![[Pasted image 20240518221205.png]]

최적 상태 가치 함수를 사용해서 최적 정책을 얻을 수 있다. 이 식은 탐욕 정책이라고 할 수 있다. 코딩 테스트의 그 그리드 맞다. 국소적인 후보 중에서 최선의 행동을 찾는다. 이번 처럼 벨만 최적 방정식에서는 현재 상태(s)와 다음 상태(s')만이 관련 있으며, 단순히 다음 상태만을 고려하여 가치가 가장 큰 행동을 선택한다. 


### 한눈에 보기
---
![[Pasted image 20240626141730.png]]]]