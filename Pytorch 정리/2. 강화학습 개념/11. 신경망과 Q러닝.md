그리드 문제야 뭐 상태 후보가 12개이고 행동 후보는 4개뿐이라서 Q함수 후보는 총 12 x 4 = 48개이다. 이 정도야 Q 함수를 테이블로 관리하는데 아무 문제도 없다. 

근데 만약 체스는 어떻게 할 것인가? 체스의 보드 배열 패턴은 10<sup>123</sup>이다. 즉, 상태가 이 수만큼 존재한다는 뜻이다. 이 많은 상태를 테이블로 관리하려면 슈퍼 컴퓨터 여러대는 필요할 것이다. 더 큰 문제는 테이블으 원소 하나하나를 독립적으로 평가하고 개선해야 한다는 점이다. 

이걸 가능하게 하려면 역시 신경망으로 인한 합리적인 추론 방법이 필요할 것이다.

### 1. 신경망의 전처리
---
범주형 데이터 (옷 사이즈 S/M/L, 혈액형 A/B/O/AB) 데이터를 원 핫 벡터로 만드는것은 기본적인 상식이다. 

즉, 그리드 월드의 상태또한 원-핫 벡터로 만들어야 한다. 

``` python
def one_hot(state):
    HEIGHT, WIDTH = 3, 4
    vec = np.zeros(HEIGHT * WIDTH, dtype=np.float32) # 각 원소를 담을 벡터 준비
    
    y, x = state 
    idx = WIDTH * y + x
    vec[idx] = 1.0 # 주어진 상태만 1
    
    return vec[np.newaxis, :] # 배치 처리를 위헤ㅐ 새로운 축 추가 (batch, 12)
```


### 2. Q 함수를 표현하는 신경망
---
이전에 Q함수를 테이블 (파이썬 코드에서는 딕셔너리 defaultdict)로 구현했다. 

``` python
Q = defaultdict(lambda: 0)
state = (2, 0)
action = 0

print(Q[state, action]) # 출력 결과 0.0
```

Q는 (state, action) 쌍의 데이터를 입력받아 Q 함수의 값을 출력한다. 즉 (state, action)쌍의 데이터 하나하나에 대해 Q함수의 값이 개별적으로 저장되어 있다.

이제 테이블로 표현된 Q 함수를 신경망으로 만들어보자, 그러려면 먼저 신경망의 입력과 출력을 명확하게 규정해야 한다. 

![[Pasted image 20240627131726.png]]

첫 번째 구조는 상태와 행동 두 가지를 입력으로 받는 신경망이다. 출력으로는 Q 함수의 값을 하나만 내보낸다. 

두 번째 구조는 상태만을 입력받아, 가능한 행동의 개수만큼 Q함수의 값을 출력하는 신경망이다. 예를 들어 행동의 가짓수가 4개라면 원소 4개짜리 벡터를 출력한다.

그런데 첫 번쨰 구조는 계산 비용 측면에서 문제가 있다.  어떤 상태에서 Q 함수의 최댓값을 구하는 계산 비용, 즉 수식으로 표현하면 max<sub>a</sub> Q(s,a)의 계산 비용이 커진다.

![[Pasted image 20240626145101.png]]

Q러닝에서는 max<sub>a</sub> Q(s,a)를 계산해야 한다. 상태 s에서 Q함수가 최대가 되는 행동(a)을 찾는 계산이다. 이 계산을 첫 번째 신경망 구조에서 수행하려면 행동 후보의 수만큼 신경망을 순전파하여 Q함수의 값을 구해야 한다. (그림을 보면 알겠지만....)

행동의 수가 4개라면 순전파를 총 4번 수행하여 가능한 행동 각각에 대한 Q함수를 구해야 하는 것이다. 반면, 두 번째 신경망 구조에서는 모든 행동에 대한 Q함수를 순전파 한 번으로 구할 수 있다. 

두 번째 구조를 구현하면 다음과 같다.

``` python
class QNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.l1 = nn.Linear(100)  # 중간층의 크기
        self.l2 = nn.Linear(4)    # 행동의 크기(가능한 행동의 개수)

    def forward(self, x):
        x = F.relu(self.l1(x))
        x = self.l2(x)
        return x
```

이것으로 Q함수를 신경망으로 대체할 수 있게 되었다. 

### 3. 신경망과 Q러닝
---
![[Pasted image 20240626145101.png]]

Q러닝에서 추정치 Q(S<sub>t</sub>, A<sub>t</sub>)의 목표는 R<sub>t</sub>+γ max<sub>a</sub>Q(S<sub>t+1</sub>, a)가 된다. 이 목표 방향으로 Q함수를 갱신한다. α는 목표 방향으로 얼마나 나아갈 것인지 조정한다

여기서 목표인  R<sub>t</sub>+γ max<sub>a</sub>Q(S<sub>t+1</sub>, a)를 T로 간소화 해보자

![[Pasted image 20240627133856.png]]

입력이 S<sub>t</sub>, A<sub>t</sub>일 때 출력이 T가 되도록 Q함수를 갱신하는 것으로 해석할 수 있다.

![[Pasted image 20240627145749.png]]

신경망 맥락에 대입하면, 입력이 S<sub>t</sub>, A<sub>t</sub>일 때 출력이 T가 되도록 학습시킨다는 뜻이다. 즉, T를 정답 레이블로 볼 수 있다. 또한 T는 스칼라 값이기 때문에 회귀 문제로 해석이 가능하다.

``` python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np

class QNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.l1 = nn.Linear(100)  # 중간층의 크기
        self.l2 = nn.Linear(4)    # 행동의 크기(가능한 행동의 개수)
	
    def forward(self, x):
        x = F.relu(self.l1(x))
        x = self.l2(x)
        return x

class QLearningAgent:
    def __init__(self):
        self.gamma = 0.9
        self.lr = 0.01
        self.epsilon = 0.1
        self.action_size = 4
		
        self.qnet = QNet()  # 신경망 초기화
        self.optimizer = optim.SGD(self.qnet.parameters(), lr=self.lr)  # 옵티마이저 생성

    def get_action(self, state_vec):
        if np.random.rand() < self.epsilon:
            return np.random.choice(self.action_size)
        else:
            qs = self.qnet(state_vec)
            return qs.argmax()

    def update(self, state, action, reward, next_state, done):
		# 다음 상태에서 최대가 되는 Q 함수의 값(next_q) 계산
        if done:  # 목표 상태에 도달
            next_q = torch.zeros(1)  # [0.]  # [0.] (목표 상태에서의 Q 함수는 항상 0)
        else:     # 그 외 상태
            next_q = self.qnet(next_state).max().detach()

        # 목표
        target = reward + self.gamma * next_q
        # 현재 상태에서의 Q 함수 값(q) 계산
        q = self.qnet(state)[action]

        # 목표(target)와 q의 오차 계산
        loss = nn.functional.mse_loss(target, q)

        # 역전파 → 매개변수 갱신
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        return loss.item()

env = GridWorld()
agent = QLearningAgent()

episodes = 1000  # 에피소드 수
loss_history = []

for episode in range(episodes):
    state = env.reset()
    state = one_hot(state)
    total_loss, cnt = 0, 0
    done = False

    while not done:
        action = agent.get_action(state)
        next_state, reward, done = env.step(action)
        next_state = one_hot(next_state)

        loss = agent.update(state, action, reward, next_state, done)
        total_loss += loss
        cnt += 1
        state = next_state

    average_loss = total_loss / cnt
    loss_history.append(average_loss)


# [그림 7-14] 에피소드별 손실 추이
plt.xlabel('episode')
plt.ylabel('loss')
plt.plot(range(len(loss_history)), loss_history)
plt.show()

# [그림 7-15] 신경망을 이용한 Q 러닝으로 얻은 Q 함수와 정책
Q = {}
for state in env.states():
    for action in env.action_space:
        q = agent.qnet(one_hot(state))[:, action]
        Q[state, action] = float(q.data)
        
env.render_q(Q)
```

next_q는 정답 레이블을 만들기 위해 사용된다. 지도 학습에서는 정답 레이블에 대한 기울기는 필요가 없다. 따라서 next_q를 계산 그래프에서 제외한다. 이렇게 하면 단순하게 next_q는 그냥 스칼라 값이 된다. 

위의 코드에서는 target 계산에 쓰이는 next_q의 값을 if문에서 설정했다. 그런데 if문을 사용하지 않고 다음처럼 구현할 수 있다.

``` python
def update(self, state, action, reward, next_state, done):
	done = int(done)
	next_qs = self.qnet(next_state)
	next_q = next_qs.max(axis = 1)
	next_q.detach()
	target = reward + (1 - done) * self.gamma * next_q
```


### 4. Q 함수를 신경망으로 대체한다고?
---
이 말이 뭔 말인가 싶을거다.

먼저 우리는 Q러닝에서 Q함수를 어떻게 구했을까? 그렇다 바로 TD법을 사용했다.

![[Pasted image 20240620230427.png|350]]
![[Pasted image 20240620231940.png]]
![[Pasted image 20240621005213.png]]

우리는 위의 TD법을 가치 함수로 하는 SARSA를 사용했다.  

그리고 벨만 방정식으로부터 SARSA를 도출해내어 보다 일반적인 관점에서 바라봤다. 벨만 방정식은 따로 정책을 반복해야 한다.

![[Pasted image 20240621003306.png]]

그래서 우리는 최적 정책을 찾는 공식으로 도출했다.

반면 벨만 최적 방정식의 매우 중요한 점은 최적 정책을 찾는 방안이 없더라도 스스로 최적 정책을 얻을 수 있다는 사실이었다. 

따라서 벨만 최적 방정식을 샘플링 버전으로 만들어 놓으니

![[Pasted image 20240626145101.png]]
가 된 것이다. 

문제는

![[Pasted image 20240621003306.png]]

이놈이다. 이거 max를 구하려면 Q를 알아야 한다. 즉, 모든 상태에 대한 Q를 어딘가에는 저장을 해둬야 한다는 것이다. 그래야 최고의 행동을 뽑을테니....

``` python
def greedy_probs(Q, state, epsilon=0, action_size=4):
    qs = [Q[(state, action)] for action in range(action_size)] # 이 부분이 문제
    max_action = np.argmax(qs) # 최적 행동 정책을 뽑음
    
    base_prob = epsilon / action_size
    action_probs = {action: base_prob for action in range(action_size)}  
    #{0: ε/4, 1: ε/4, 2: ε/4, 3: ε/4} 이렇게 됨
    action_probs[max_action] += (1 – epsilon) # 가장 좋은 정책이 뽑힐 확률은 (1-ε)+ε/4
	
    return action_probs
```

이걸 보면 알겠지만, Q 함수를 담은 딕셔너리에서 qs를 뽑아내고 있다. 즉, 모든 상황에 대한 Q함수를 가지고 있어야 한다는 것이다. 저 부분을 딥러닝으로 대체한 것이다. 

>Q 함수를 추론하는 방법 자체를 신경망으로 대체한 꼴이다. 