근데 우리는 Q 함수의 값이 가장 큰 행동만 하고 싶다. 즉, '활용'만 하고 싶다. 문제는 앞서 말했지만, '탐색'을 포기해야만 한다. 

'탐색'을 포기하면 수익 데이터를 얻을 수 없게 된다. 따라서 양쪽 다 버릴 수 없어서 ɛ-탐욕 정책은 타협점으로 사용했다. 

이러한 타협 없이 '탐색'도 하면서 '활용'도 하는 방안이 없을까? 

### 1. 온-정책, 오프-정책
---
다른 사람이 행하는 기술을 보고 스스로의 기술을 고칠 수 있는 힌트를 얻을 수 있는데, 이러한 방법은 자신과 다른 환경에서 얻은 경험을 토대로 자신의 정책을 개선한다. 이러한 접근 방식을 오프-정책 이라고 한다. 

반면 스스로 쌓은 경험을 토대로 자신의 정책을 개선하는 것은 온-정책이다. 

역할 측면에서 보면 에이전트의 정책은 두 가지이다. 
1. 대상 정책 : 평가와 개선의 대상으로서의 정책이다. 즉, 정책에 대해 평가한 다음 개선한다. 이러한 정책을 "대상 정책"이라고 한다. 

2. 행동 정책 : 다른 하나는 에이전트가 실제로 행동을 취할 때 활용하는 정책이다. 이 정책에 따라 '상태, 행동 보상'의 샘플 데이터가 생성된다. 이러한 정책이 "행동 정책"이다

이 두 정책을 구분하지 않고 둘 다 쓴다면 온-정책, 구분해서 생각하면 오프-정책 이라고 한다. (간단하게 온/오프(연결되어있다/떨어져있다)로 생각하자 대상 정책과 행동 정책이 온/오프 인것)

여태껏 온-정책을 사용했다. 이제는 오프-정책을 한번 봐보자

### 2. 중요도 샘플링
---
중요도 샘플링은 어떤 확률 분포의 기댓값을 다른 확률 분포에서 샘플링한 데이터를 사용하여 계산하는 기법이다. 

E<sub>π</sub>`[x]` 라는 기댓값 계산을 예로 들어보자

x는 확률 변수이고, x의 확률은 π(x)라고 해보자, 이러한 확률 분포표의 기댓값은 다음과 같다.

![[Pasted image 20240526030134.png]]

이 기댓값을 몬테카를로법으로 근사하려면 x를 확률 분포 π에서 샘플링하여 평균을 내면 된다. 

![[Pasted image 20240526030755.png]]

지금 우리는 x가 다른 확률 분포에서 샘플링된 경우의 문제를 풀어야 한다. 예를 들어 x가 (π가 아닌) b라는 확률 분포에서 샘플링 되었다고 가정해보자 이 경우에 기댓값 E<sub>π</sub>`[x]`는 다음과 같이 근사된다. 

![[Pasted image 20240526031228.png]]

E<sub>b</sub>`[x]` 확률 분포 π에 대한 기댓값을 확률 분포 b에 대한 기댓값으로 표현해 내었다. 또한 각각의 x에 π(x) / b(x) 가 곱해진다는 것도 중요하다. 여기서 p(x) = π(x) / b(x) 라고 한다면, 각각의 x에는 '가중치'로서 p(x)가 곱해진다고 볼 수 있다.

이에 근거하여

![[Pasted image 20240526032059.png]]

라고 쓸 수 있다. 이제 다른 확률 분포에서 샘플링한 데이터를 이용하여 기댓값 E<sub>π</sub>`[x]`를 계산할 수 있다.

### 2. 코드로 구현
---
``` python
x = np.array([1, 2, 3])         # 확률 변수
pi = np.array([0.1, 0.1, 0.8])  # 확률 분포

# =========== 기댓값의 참값 계산 ==================
e = np.sum(x * pi)
print('참값(E_pi[x]):', e)

# =========== 몬테카를로법으로 계산 ==================
n = 100  # 샘플 개수
samples = []
for _ in range(n):
    s = np.random.choice(x, p=pi)  # pi를 이용한 샘플링 pi를 써서 각 원소가 선택될 확률을 세팅한다. 
    samples.append(s)

mean = np.mean(samples)  # 샘플들의 평균
var = np.var(samples)    # 샘플들의 분산
print('몬테카를로법: {:.2f} (분산: {:.2f})'.format(np.mean(samples), np.var(samples)))
```

``` 
참값 : 2.7
몬테카를로법 : 2.78 (분산 : 0.27)
```

이어서 중요도 샘플링을 이용해서 기댓값을 구해보자

``` python
# =========== 중요도 샘플링으로 계산 ===========
b = np.array([1/3, 1/3, 1/3])
samples = []
for _ in range(n):
    idx = np.arange(len(b))         # b의 인덱스([0, 1, 2])
    i = np.random.choice(idx, p=b)  # b를 사용하여 샘플링
    s = x[i]
    rho = pi[i] / b[i]              # 가중치
    samples.append(rho * s)         # 샘플 데이터에 가중치를 곱해 저장
    
mean = np.mean(samples)
var = np.var(samples)
print('중요도 샘플링: {:.2f} (분산: {:.2f})'.format(np.mean(samples), np.var(samples)))
```

``` 
중요도 샘플링 : 2.95 (분산 : 10.63)
```

몬테카를로법때보다 분산이 훨씬 크다..


### 3. 분산을 작게 하기
---
분산이 작을수록 당연히 적은 샘플로도 정확하게 근사할 수 있다. 반대로 분산이 클수록 샘플을 더 많이 사용해야 한다. 

따라서 분산을 작게 해보자

그 전에 왜 샘플링을 하면 분산이 어마어마하게 커지는지에 대한 원인 파악이 필요하다.

![[Pasted image 20240526032739.png]]

샘플 데이터로 3을 선택한 예시를 보여준다. 이때 가중치는 2.4이다. 즉, 3 * 2.4 = 7.2를 얻는다.

뭔가 잘못된것 같은데 말이 되는 상황인게
1. 확률 분포 π를 기준으로 했을 때 3은 대표적인 값이기 때문에 (원래는) 3이 많이 샘플링 되어야 한다.
2. 하지만 확률 분포 b에서는 3이 특별히 많이 선택되지 않는다.
3. 이 간극을 메우기 위해 3이 샘플링된 경우, 그 값이 커지도록 '가중치'를 곱하여 조정한다.

이처럼 확률 분포 π와 b의 차이를 고려하여 샘플링된 값에 가중치를 곱하여 조정하는 일 자체는 의미가 있다. 하지만 샘플링된 값이 3인데 7.2로 취급한다면, 만에 하나 이게 첫 번째 샘플 데이터라면 현 시점 추정값이 7.2로 튀어버린다. 참값이 2.7인 반면에 엄청난 오차가 생긴다. 

그럼 이러한 분산이 커지는 것을 막기 위해서는 두 확률 분포 (b와 π)를 가깝게 만들면 된다. 이렇게 하면 가중치 p의 값을 1에 가깝게 만들 수 있다. 

``` python
# =========== 중요도 샘플링으로 계산 ===========
b = np.array([0.2, 0.2, 0.6])  # 앞에서는 b = np.array([1/3, 1/3, 1/3])
samples = []
for _ in range(n):
    idx = np.arange(len(b))         # b의 인덱스([0, 1, 2])
    i = np.random.choice(idx, p=b)  # b를 사용하여 샘플링
    s = x[i]
    rho = pi[i] / b[i]              # 가중치
    samples.append(rho * s)         # 샘플 데이터에 가중치를 곱해 저장
    
mean = np.mean(samples)
var = np.var(samples)
print('중요도 샘플링: {:.2f} (분산: {:.2f})'.format(np.mean(samples), np.var(samples)))

```

```
중요도 샘플링 : 2.72 (분산: 2.48)
```

확률 분포 pi에 가깝게 만들면 결과가 2.72로 참값에 더 가까워 졌다. 

따라서 중요도 샘플링 시 두 확률 분포를 비슷하게 설정하면 분산이 줄어든다. 단, 강화 학습에서 핵심은 한쪽 정책(확률 분포)은 '탐색'에 다른 쪽 정책은 '활용'에 이용하는 것이다. 이 조건을 염두에 둔 상태에서 두 확률 분포를 최대한 가깝게 조정하면 분산이 줄어든다.

이것이 바로 중요도 샘플링이다. 중요도 샘플링을 이용하면 오프-정책을 구현할 수 있다. 즉, 행동 정책이라는 확률 분포에서 샘플링된 데이터를 토대로 대상 정책에 대한 기댓값을 계산할 수 있다. 

