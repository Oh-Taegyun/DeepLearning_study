오프-정책은 에이전트가 행동 정책과 대상 정책을 따로 가진다고 했다. 이게 어떻게 가능할까?

### 1. 오프-정책과 중요도 샘플링
---
일전에 오프-정책을 이런 식으로 표현했다. 

>다른 사람이 행하는 기술을 보고 스스로의 기술을 고칠 수 있는 힌트를 얻을 수 있는데, 이러한 방법은 자신과 다른 환경에서 얻은 경험을 토대로 자신의 정책을 개선한다. 이러한 접근 방식을 오프-정책 이라고 한다. 

그래서 중요도 샘플링이란 것을 통해서, 다른 사람이 행하는 기술을 토대로 배울 수 있게 되었다.

오프-정책에서는 에이전트가 행동 정책과 대상 정책을 따로 가지고 있다. 행동 정책에서는 다양한 행동을 시도하며 샘플 데이터를 폭 넓게 수집한다. 그리고 이 샘플 데이터를 이용해 대상 정책을 탐욕스럽게 갱신한다. 이때 주의할 것이 두 가지 있다.

1. 행동 정책과 대상 정책의 확률 분포가 비슷할수록 결과가 안정적이다. 이 점을 고려해서 현재의 Q함수에 대해 행동 정책은 ɛ-탐욕 정책으로 갱신하고, 대상 정책은 탐욕 정책으로 갱신한다.
2. 두 정책이 서로 다르기 때문에 중요도 샘플링을 활용하여 가중치 ρ로 보정한다. 

더 구체적으로 살펴보자. 행동 가치 함수의 갱신식은 다음과 같다.

![[Pasted image 20240621135917.png]]

이 갱신식에 대응하는 다이어그램도 살펴보자

![[Pasted image 20240621140203.png|500]]

위 그림에서 보듯 (S<sub>t</sub>, A<sub>t</sub>)가 갱신 대상이다. 이 갱신 대상은 임의로 선택할 수 있다. 선택된 대상이 다음 시간 t+1에 어떻게 전이될지를 고려하는것이다. 

이때 다음 상태 S<sub>t+1</sub>는 환경 상태 전이 확률 p(s'|s, a)에 따라 샘플링된다. 그리고 상태 S<sub>t+1</sub>에서 선택되는 행동은 대상 정책 π(또는 행동 정책 b)에 따라 샘플링된다. 이렇게 얻은 샘플 데이터를 위의 갱신식에 대입해서 갱신한다.

이때 행동이 정책 π에 따라 선택됨을 명시하면 SARSA의 갱신식을 다음처럼 작성할 수 있다.

![[Pasted image 20240621184243.png]]

위의 식은 Q<sub>π</sub>(S<sub>t</sub>, A<sub>t</sub>)를 R<sub>t</sub>+γ Q<sub>π</sub>(S<sub>t+1</sub>, A<sub>t+1</sub>) 방향으로 갱신함을 나타낸다. 즉, R<sub>t</sub>+γ Q<sub>π</sub>(S<sub>t+1</sub>, A<sub>t+1</sub>)이 TD의 목표이다. 

이제 보정값을 구해보자, 행동 A<sub>t+1</sub>이 정책 b에 따라 샘플링된 경우를 생각해보자, 이 경우 가중치 ρ로 TD의 목표를 보정한다. (중요도 샘플링에서 다뤄봤다.) 

가중치 ρ는 '정책이 π일 때 TD목표를 얻을 확률'과 '정책이 b일 때 TD 목표를 얻을 확률'의 비율이다. 수식으로는 다음과 같다.

![[Pasted image 20240621185045.png|300]]

따라서 오프-정책 SARSA의 갱신식은 다음과 같다.


![[Pasted image 20240621185220.png]]

### 2. 코드
---
``` python
class SarsaOffPolicyAgent:
    def __init__(self):
        self.gamma = 0.9
        self.alpha = 0.8
        self.epsilon = 0.1
        self.action_size = 4

        random_actions = {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}
        self.pi = defaultdict(lambda: random_actions)
        self.b = defaultdict(lambda: random_actions)
        self.Q = defaultdict(lambda: 0)
        self.memory = deque(maxlen=2)

    def get_action(self, state):
        action_probs = self.b[state]  # 행동 정책에서 가져옴
        actions = list(action_probs.keys())
        probs = list(action_probs.values())
        return np.random.choice(actions, p=probs)

    def reset(self):
        self.memory.clear()

    def update(self, state, action, reward, done):
        self.memory.append((state, action, reward, done))
        if len(self.memory) < 2:
            return

        state, action, reward, done = self.memory[0]
        next_state, next_action, _, _ = self.memory[1]

        if done:
            next_q = 0
            rho = 1
        else:
            next_q = self.Q[next_state, next_action]
            rho = self.pi[next_state][next_action] / self.b[next_state][next_action]  # 가중치 rho 계산

        # rho로 TD 목표 보정
        target = rho * (reward + self.gamma * next_q)
        self.Q[state, action] += (target - self.Q[state, action]) * self.alpha

        # 각각의 정책 개선
        self.pi[state] = greedy_probs(self.Q, state, 0)
        self.b[state] = greedy_probs(self.Q, state, self.epsilon)


env = GridWorld()
agent = SarsaOffPolicyAgent()

episodes = 10000
for episode in range(episodes):
    state = env.reset()
    agent.reset()

    while True:
        action = agent.get_action(state)
        next_state, reward, done = env.step(action)

        agent.update(state, action, reward, done)

        if done:
            agent.update(next_state, None, None, None)
            break
        state = next_state

# [그림 6-9] 오프-정책 SARSA로 얻은 결과
env.render_q(agent.Q)

```
