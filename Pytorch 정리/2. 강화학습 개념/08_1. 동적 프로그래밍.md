가치 함수를 구해서 지속적 과제를 해결하는 아이디어는 매우 휼륭하다. 하지만 가치 함수의 문제는 조금만 상태와 행동 패턴이 늘어나면 연립 방정식을 풀기가 매우 어려워 진다는 점이다. 그래서 그 해결법으로 DP(동적 프로그래밍)이다.

### 1. 동적 프로그래밍과 정책 평가
---
1. 정책 평가 : 정책이 주어졌을때 그 정책의 가치 함수를 구하는 문제
2. 정책 제어 : 정책을 조정해서 최적의 정책을 만들어냄

정책 제어 부분이 강화학습의 궁극적인 목표이나, 만들기가 어렵다. 따라서 정책 평가를 먼저 해보자

### 2. 동적 프로그래밍 기초
---
![[Pasted image 20240519153535.png]]

위의 식은 매우 친숙한 벨만 방정식이다. 아래 식은 DP를 이용한 방법이다. 벨만 방정식을 갱신식으로 변형한 것이다.

이때 V<sub>k+1</sub>(s)는 k+1번째로 갱신된 가치 함수를 뜻한다. k번째로 갱신된 가치 함수는 V<sub>k</sub>(s)로 표기한다. 이때 V<sub>k+1</sub>(s)와 V<sub>k</sub>(s)는 '추정치'라서 실제 가치 함수인 v(s)와는 다르다.

위 식의 특징은, 다음 상태의 가치 함수 V<sub>k</sub>(s')를 이용해서 '지금 상태의 가치 함수' V<sub>k+1</sub>(s)를 갱신한다는 점이다. 그리고 이 식은 '추정치 V<sub>k</sub>(s)' 를 사용해서 '또 다른 추정치 V<sub>k+1</sub>(s)'를 개선한다.

당연히 DP를 이용한 반복적 정책 평가 알고리즘이며, 실제 문제에 적용하려면 언젠가는 갱신을 멈춰야 한다. 이때 갱신 횟수를 결정하는 기준으로 갱신된 양을 이용할 수 있다. 위 식의 갱신을 반복적으로 계산한다면, V<sub>π</sub>(s)에 수렴한다.

