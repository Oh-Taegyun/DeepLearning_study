Q러닝과 신경망을 이용한 기법이다. 이전에 배웠던 것과 다른 점은 새로운 기술은 '경험 재생'과 '목표 신경망'을 더한 기법이다. 

사용할 라이브러리는 `OpenAI Gym`이다. 

### 1. 상태와 관찰
---
>OpenAI Gym 문서에서는 '상태(state)' 대신 '관찰(observation)'이라는 용어를 사용합니다. API에서도 'observation'을 씁니다(env.observation_space 등).
>
>상태와 관찰은 다릅니다. 상태는 환경에 대한 '완전한 정보'입니다. 상태를 알면 마르코프 결정 과정을 통해 다음 상태와 보상의 확률 분포가 완벽하게 결정됩니다. 반면, 관찰은 환경에 대한 '부분적 정보'입니다. 예를 들어 포커나 고스톱에서는 상대의 패를 알 수 없습니다. 이처럼 에이전트가 환경의 일부만 볼 수 있는 문제를 상상하면 쉽게 이해될 것입니다.
>
>문제에 따라서는 상태와 관찰이 동일시되는 경우도 있지만, 다양한 강화 학습 문제를 고려하면 '상태'보다 '관찰'이 적절한 용어입니다. 그래서 OpenAI Gym에서는 '관찰'로 쓰고 있습니다. 다만 이 책에서는 상태와 관찰이 똑같은 문제만 다루기 때문에 지금까지와 마찬가지로 '상태'로 쓰겠습니다.

1. 상태 : 다음 상태와 보상의 확률 분포가 완벽하게 결정됨
2. 관찰 : 환경에 대해서 부분적인 정보만 알 수 있음

### 2. DQN의 핵심 기술
---
Q 러닝에서는 추정치를 사용해서 추정치를 계산한다 (부트스트래핑). 아직 정확하지 않은 추정치를 사용해서 현재의 추정치를 갱신하기에 Q러닝 (넓게는 TD법)은 불안정해지기 쉽다는 성질이 있다. 

![[Pasted image 20240620230427.png|350]]

![[Pasted image 20240626144700.png|500]]

간단히 말해서 저 Q의 샘플이 처음에는 정확한게 아닐 것이다. 그냥 랜덤하게 뽑았다. 여기에 신경망처럼 표현력이 높은 함수 근사화 기법이 더해지면 결과는 더욱 불안정해진다.  

DQN의 Q러닝과 신경망을 결합한 기법으로, 신경망의 학습을 안정화하기 위해 '경험 재생'과 '목표 신경망' 기술을 사용한다는 점이 특징이다. 


### 3. 경험 재생
---
Q 러닝은 에이전트가 환경 속에서 어떤 행동을 취할 때마다 데이터를 생성한다. 어떤 시간 t에서 얻은 E<sub>t</sub>=(S<sub>t</sub>, A<sub>t</sub>, R<sub>t</sub>, S<sub>t+1</sub>)을 이용해 Q 함수를 갱신한다. 여기서 E<sub>t</sub>를 경험 데이터라고 한다. 경험 데이터는 시간이 흐름에 따라 얻어지면서 경험 데이터 사이에는 강한 상관관계가 존재한다. (예컨데 E<sub>t</sub>와  E<sub>t+1</sub>사이에는 강한 상관관계가 있다.) 다르게 표현하면 Q 러닝에서는 상관관계가 높은 (편향된) 데이터를 사용하여 학습한다는 뜻이다. (딥러닝에서 미니배치로 학습할때, 데이터가 편향되는 것을 막아야 한다 (예를 들어 Mnist에서 '2'이미지만 계속 뽑아낸다거나))

경험 재생의 아이디어는 아주 간단하다. 우선 에이전트가 경험한 데이터 E<sub>t</sub>=(S<sub>t</sub>, A<sub>t</sub>, R<sub>t</sub>, S<sub>t+1</sub>)를 버퍼에 저장한다. 그리고 Q함수를 갱신할 때는 이 버퍼로부터 경험 데이터를 무작위로 꺼내 사용한다. 

![[Pasted image 20240628131310.png]]

이렇게 지도학습의 미니배치 학습법처럼 경험을 버퍼에 쌓고 꺼낸다.

경험 재생 기법으로 경험 데이터 사이의 상관관계를 약화시켜 결국 편향이 적은 데이터를 얻을 수 있다. 또한 경험 데이터를 반복해서 사용할 수 있기 때문에 효율도 높아진다. 

>경험 재생은 Q 러닝뿐 아니라 다른 강화 학습 알고리즘에서도 사용한다. 단, 오프-정책 알고리즘 한정이다. 온-정책 방식은 현재 정책에서 얻은 데이터만 사용할 수 있다. 과거게 수집한 경험 데이터는 사용할 수 없다.

경험 재생 버퍼는 큐로 만든다. 버퍼에 무한한 데이터를 쌓을 수 없기 때문에 상대적으로 과거의 경험 데이터를 삭제하는 형식으로 진행한다. 

![[Pasted image 20240628131857.png]]

``` python
class ReplayBuffer:
    def __init__(self, buffer_size, batch_size):
        self.buffer = deque(maxlen=buffer_size)
        self.batch_size = batch_size
		
    def add(self, state, action, reward, next_state, done):
        data = (state, action, reward, next_state, done)
        self.buffer.append(data)
		
    def __len__(self):
        return len(self.buffer)
		
    def get_batch(self):
        data = random.sample(self.buffer, self.batch_size)
		
        state = torch.tensor(np.stack([x[0] for x in data]))
        action = torch.tensor(np.array([x[1] for x in data]).astype(np.long))
        reward = torch.tensor(np.array([x[2] for x in data]).astype(np.float32))
        next_state = torch.tensor(np.stack([x[3] for x in data]))
        done = torch.tensor(np.array([x[4] for x in data]).astype(np.int32))
        return state, action, reward, next_state, done
```


### 4. 목표 신경망
---
일반적인 신경망의 지도 학습에는 정답 레이블이 변경될 일이 없다. 한번 데이터에 할당된 정답 레이블은 영원히 그게 정답이다. 반면 Q 러닝은 Q(S<sub>t</sub>, A<sub>t</sub>)의 값이 R<sub>t</sub>+γ max<sub>a</sub>Q(S<sub>t+1</sub>, a)가 된다. TD 목표는 지도 학습에서의 정답 레이블에 해당한다. 하지만, TD 목표의 값은 Q 함수가 갱신될 때마다 달라진다는게 문제이다. 이게 바로 지도 학습과 Q 러닝의 차이이다. 이러한 차이를 메우기 위해 "TD 목표를 고정 하는" 목표 신경망을 사용해야 한다.

어떻게 구현할까?
먼저 Q 함수를 나타내는 원본 신경망 (qnet)을 준비한다. 그리고 구조가 같은 신경망 (qnet_target)을 하나 더 준비한다 . qnet은 일반적인 Q러닝으로 갱신한다. 반면 qnet_target은 주기적으로 qnet의 가중치와 동기화시키고, 그 외에는 가중치 매개변수를 고정된 상태로 둔다. 이후 qnet_target을 사용하여 TD목표가 바뀌는 일을 억제할 수 있다. 즉, 정답 레이블인 TD 목표가 달라지지 않기 때문에 신경망 학습이 안정화 될 것이라고 기대할 수 있다.

> 목표 신경망은 TD 목표의 값을 고정하기 위한 기법이다. 단, TD 목표가 전혀 갱신되지 않으면 Q함수의 학습이 진행되지 않으므로 주기적으로(예컨대 100 에피소드마다) 목표 신경망을 갱신한다.

``` python
class QNet(nn.Module):
    def __init__(self, action_size):
        super().__init__()
        self.l1 = nn.Linear(4, 128)
        self.l2 = nn.Linear(128, 128)
        self.l3 = nn.Linear(128, action_size)
		
    def forward(self, x):
        x = F.relu(self.l1(x))
        x = F.relu(self.l2(x))
        x = self.l3(x)
        return x


class DQNAgent:
    def __init__(self):
        self.gamma = 0.98
        self.lr = 0.0005
        self.epsilon = 0.1
        self.buffer_size = 10000
        self.batch_size = 32
        self.action_size = 2

        self.replay_buffer = ReplayBuffer(self.buffer_size, self.batch_size)
        self.qnet = QNet(self.action_size)
        self.qnet_target = QNet(self.action_size)
        self.optimizer = optim.Adam(self.qnet.parameters(), lr=self.lr)

    def get_action(self, state):
        if np.random.rand() < self.epsilon:
            return np.random.choice(self.action_size)
        else:
            state = torch.tensor(state[np.newaxis, :])
            qs = self.qnet(state)
            return qs.argmax().item()
		
    def update(self, state, action, reward, next_state, done):
	    # 경험 재생 버퍼에 경험 데이터 추가
        self.replay_buffer.add(state, action, reward, next_state, done)
        if len(self.replay_buffer) < self.batch_size:
            return # 데이터가 미니배치 크기만큼 쌓이지 않았다면 여기서 끝
		
        state, action, reward, next_state, done = self.replay_buffer.get_batch()
        qs = self.qnet(state) # 출력값은 (32, 2)
        q = qs[np.arange(len(action)), action]
		
        next_qs = self.qnet_target(next_state)
        next_q = next_qs.max(1)[0]
		
        next_q.detach()
        target = reward + (1 - done) * self.gamma * next_q
		
        loss_fn = nn.MSELoss()
        loss = loss_fn(q, target)
		
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
		
    def sync_qnet(self):
        self.qnet_target.load_state_dict(self.qnet.state_dict())


episodes = 300
sync_interval = 20
env = gym.make('CartPole-v0')
agent = DQNAgent()
reward_history = []

for episode in range(episodes):
    state = env.reset()
    done = False
    total_reward = 0
	
    while not done:
        action = agent.get_action(state)
        next_state, reward, done, info = env.step(action)
		
        agent.update(state, action, reward, next_state, done)
        state = next_state
        total_reward += reward
		
    if episode % sync_interval == 0:
        agent.sync_qnet()
		
    reward_history.append(total_reward)
    if episode % 10 == 0:
        print("episode :{}, total reward : {}".format(episode, total_reward))
```

![[Pasted image 20240628135432.png]]

이런 식으로 그래프가 나오는데 강화 학습 알고리즘은 매번 시작할때마다, 결과가 달라진다. 따라서 같은 실험을 여러번 반복해서 평가해야한다.

같은 실험을 100번 반복해서 평균하면

![[Pasted image 20240628135636.png]]

가 된다. 보면 알겠지만 50번째부터 요령을 잡아서 점차 성장한다. 

참고로 학습 중인 에이전트는 ɛ-탐욕 정책에 따라 행동한다. 즉, ɛ의 확률로 무작위로 행동 한다. 이제 학습이 끝난 에이전트에게 탐욕 행동을 선택하도록 해보자

``` python
# 학습이 끝난 에이전트에 탐욕 행동을 선택하도록 하여 플레이
agent.epsilon = 0  # 탐욕 정책(무작위로 행동할 확률 ε을 0로 설정)
state = env.reset()[0]
done = False
total_reward = 0

while not done:
    action = agent.get_action(state)
    next_state, reward, terminated, truncated, info = env.step(action)
    done = terminated | truncated
    state = next_state
    total_reward += reward
    env.render()
print('Total Reward:', total_reward) # 116

```

대략 116단계 까지 균형을 잡을 수 있게 되었다. 아직 균형을 완벽하게 잡진 못하나, 하이퍼파라미터의 조정으로 결과가 더 나아질 것이다. 

