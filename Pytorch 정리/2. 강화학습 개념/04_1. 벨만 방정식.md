이전의 MDP를 직접 예제를 통해 구했는데, 궁금한것은 이것이다.

![[Pasted image 20240302024934.png]]

왼쪽이야 결정적이라 구하기 쉬웠는데 오른쪽 확률적인 경우들은 어떻게 구하나? 
상태 가치 함수는 어떻게 구하나? 

바로 이러한 궁금증을 해결해 주는 것이 이 벨만 방정식이다. 


### 들어가기 전에
---
![[Pasted image 20240615190820.png]]

상태 가치 함수는 왜 구할까?

### 1. 확률의 개념 되돌아보기
---
이산확률변수는 확률분포표를 작성해서 파악하는 편이 매우 쉽다. 하지만 책에서는 백업 다이어그램을 사용하고 있으므로 백업 다이어그램의 사용법과 백업 다이어그램을 확률분포표로 바꾸는 과정을 한 번 짚어보자

![[Pasted image 20240506224242.png]]

![[Pasted image 20240506224150.png]]

이렇게 바꿀 수 있다. 이것을 수식으로 한번 표현해보자

![[Pasted image 20240506230004.png]]

### 2. 벨만 방정식 도출
---
![[Pasted image 20240302020908.png|300]]

기존의 수익 식이다. 이것을 이제 다음과 같이 쓰자.

![[Pasted image 20240518143724.png|300]]

이제 다음 수익과 이전 수익간의 관계를 나타낼 수 있다. 

다음은 상태 가치 함수이다.

![[Pasted image 20240302023224.png|300]]

여기에 위 식을 넣으면 

![[Pasted image 20240518144338.png|450]]

마지막은 선형성의 원리때문에 가능하다. 

#### 2-1. 식 자체를 이해해보자
---
![[Pasted image 20240518153051.png]]

위 식의 중요한 점은 현재 벨류(상태 가치)와 다음 상태의 벨류(상태 가치) 사이의 관계를 나타내고 있다는 점이다. 

![[Pasted image 20240518231601.png]]

현재 상태의 상태 가치의 기댓값을 다음 스텝의 보상과 미래에 받을 보상을 더하는 방식으로 계산하겠다는 것이다. 

#### 2-2. 유도과정
---
![[Pasted image 20240518153051.png]]

그럼 이제 이러한 상태 가치 함수를 유도해보자, 먼저 다음과 같은 백업 다이어그램이 있다고 하자

![[Pasted image 20240518233120.png]]

#### 1) 첫번째 식 유도
---
![[Pasted image 20240518153131.png]]

"주어진 정책에서의 상태 s일때 보상 R의 기댓값" 이라고 말로 풀어쓴다면 다음처럼 구할 수 있다. 

만약 에이전트가 0.2의 확률로 행동 a1를 선택하고 0.6의 확률로 상태 s1로 전이한다고 가정해보자. 이 경우 얻게 되는 보상은 다음과 같다. 

![[Pasted image 20240518151226.png|]]

이때의 기댓값을 구하려면 다음과 같은 계산을 하면 된다. 

![[Pasted image 20240518151535.png]]

시그마가 나온 이유는 당연하게 이산확률분포의 기댓값(평균)이기 때문이다. 

이상확률분포표에 사용되는 확률P는 정책이 제공한다. 

#### 2) 두번째 식 유도
---
![[Pasted image 20240518153224.png]]

이제 오른쪽 식을 구해보자, 위 식을 말로 풀어써보자면, 상태 s를 일때의 다음 수익의 기댓값이다. 

오른쪽 식이 생긴게 꼭 상태 가치 함수처럼 생겼다. 그런데 약간 다르다. 상태 가치 함수는 다음과 같다.

![[Pasted image 20240518151957.png|550]]

무슨 차이일까? 현재 시간이 t일때, 한 단위 뒤 시간 t+1의 기대 수익을 의미하는 수식이다. 문제 해결의 조건은 s(t) = s 를 s(t+1) = s의 형태로 바꾸는 것이다. 즉, 시간을 한 단위만큼 흘려보내는 것이다. 

시간이 지난 뒤에는 당연히 다른 상태 s'가 되어 있을것이다. 따라서 위 식의 진정한 의미는 s에서 s'으로 전이했을때, 기대되는 기댓값이다. 

![[Pasted image 20240518233229.png]]

만약 에이전트의 현 상태가 s라고 해보자, 에이전트가 0.2의 확률로 a1 행동을 선택하고, 0.6의 확률로 s1상태로 전이한다고 해보자. 그러면 다음과 같이 나타낼 수 있다

![[Pasted image 20240518152316.png]]

즉 지금의 상태에서 다음 상태의 수익을 기대하는 상태로 전이되는 것을 말한다. 다음 단계의 시간을 '보는' 것으로 다음 상태의 가치 함수를 얻을 수 있다.

![[Pasted image 20240518152747.png]]

이제 두 식들을 합쳐보자

![[Pasted image 20240518153756.png]]

이게 바로 벨만 방정식이다. "상태 s의 상태 가치 함수" 와 다음에 취할 수 있는 "상태 s'의 상태 가치 함수" 의 관계를 나타낸 식이다. 모든 상태 s와 모든 정책 pi에 대해 성립한다. 

식의 의미가 꽤나 중요한데
1. ![[Pasted image 20240518153131.png]] 상태 s일때 보상 R의 기댓값 과거에서부터 현재까지의 결과의 평균치가 담겨 있다. 
2. ![[Pasted image 20240518153224.png]] 현 상태에서 다음 상태로 전이했을때의 다음 보상 기댓값 (수익은 현재 보상에 할인율을 적용한 것이다. 가까운 미래의 보상을 더 중시하기 위해)

"현재 상태의 여태껏 쌓아논 보상 + 미래 상태의 수익 기댓값"

현재 상태(s)에서 다음에 들어올 기대수익을 더한 값으로, 어찌보면 미래를 내다 보는 거라고 볼 수 있다. 

즉, 현재 상태에서 에이전트가 앞으로 얻길 기대하는 수익이다. 

### 3. 벨만 방정식의 의의
---
무한히 지속되는 지속적 과제에서 무한히 반복되는 계산을 유한한 연립방정식으로 나타내어 풀 수 있다는 점이다. 

상태 가치 함수는 기대 수익이며 '무한히 이어지는' 보상의 합으로 정의된다. 하지만 벨만 방정식은 무한이라는 개념이 없다. 무한한 경우를 유한한 경우로 축소해서 볼 수 있는 것이다. 

이 말이 뭐냐면,

생각해봐라 미래에 뭔 일이 어떻게 있을지 알고 앞으로 얼마만큼의 수익이 들어올까?를 예측할 수 있겠는가? 그런데 그걸 대단하게 수식으로 풀어냈다는 것이다.

"현재 상태의 여태껏 쌓아논 보상 + 미래 상태의 수익 기댓값" 으로 미래를 내다 볼 수 있는 것이다. 

우리가 여태껏 열심히 일해서 월급을 받아놨다면 그것은 현재 상태에 쌓인 보상이다.

그리고 만약 주식으로 자산을 더 불릴려면 우리는 이걸 수식으로 어떻게 나타내야 할지 모르겠다. 하지만 그것을 수식으로 나타내서 앞으로 들어올 기댓값을 계산할 수 있다.

그럼 주식을 할까 말까?에 대한 답을 찾을 수 있다. 