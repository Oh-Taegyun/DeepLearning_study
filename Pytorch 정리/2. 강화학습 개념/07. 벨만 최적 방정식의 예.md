![[Pasted image 20240518212328.png]]

이 상황에서 벨만 최적 방정식을 적용해보자. 그럼 모든게 무작위이고 지속적 과제에서 최적의 경우를 찾아갈 수 있을 것이다.

### 1. 벨만 최적 방정식 적용
---
![[Pasted image 20240518212614.png]]

상태 전이가 결정적이라면 다음과 같이 단순화 할 수 있다.

1. s '= f(s,a)일때
2. 
	![[Pasted image 20240518212737.png]]

무려 환경 상태 전이 함수를 날린건데 막 날려도 되나요? 라고 할 수 있는데, 사실 여기엔 다음과 같은 트릭이 있다. 

![[Pasted image 20240518155055.png]]
결정적 상태 전이라 어차피 일어날 확률이 100%이다. 그래서 날릴 수 있는 것이다.

이제 각각의 상태 전이를 구해보자.

![[Pasted image 20240518213457.png]]

![[Pasted image 20240518213853.png|400]]

앞에 max가 붙어버려서 위 식은 비선형 연산이다. 비선형 연산을 풀면 
1. v*(L1) = 5.26
2. v*(L2) = 4.73
이다

즉, 현재 상태에서 최상의 시나리오로 구성되었을때, L1은 5.26을 얻을 수 있다는 이야기이다. 그럼 누구나 L1에서부터 움직이겠다라고 생각하지 않겠는가?  

### 2. 최적 정책 구하기
---
최적 행동 가치 함수를 이미 알고 있다고 가정하자, 그럼 상태 s에서의 최적 행동은 다음과 같이 구할 수 있다.

![[Pasted image 20240518215512.png]]

argmax는 최댓값을 만들어내는 인수를 반환한다. 최적 행동 가치 함수를 알고 있는 경우, 함수의 값이 최대가 되는 행동을 선택하면 된다. 그 행동을 선택하는 것이 바로 최적 정책이다.

다음 식은 행동 가치 함수를 이용한 벨만 방정식이다.

![[Pasted image 20240518220230.png]]

만약 이게 최적 정책이라면 

![[Pasted image 20240518221035.png]]

이고 이것을 다시 위 식에 적용한다면,

![[Pasted image 20240518221205.png]]

최적 상태 가치 함수를 사용해서 최적 정책을 얻을 수 있다. 이 식은 탐욕 정책이라고 할 수 있다. 코딩 테스트의 그 그리드 맞다. 국소적인 후보 중에서 최선의 행동을 찾는다. 이번 처럼 벨만 최적 방정식에서는 현재 상태(s)와 다음 상태(s')만이 관련 있으며, 단순히 다음 상태만을 고려하여 가치가 가장 큰 행동을 선택한다. 

이제 최적 상태 가치 함수인 v*(L1) 과 v*(L2)를 구했으니 최적 행동을 구해보자

![[Pasted image 20240518222334.png|500]]

![[Pasted image 20240518222413.png]]

할인율이 0.9라면 위 식은 다음과 같다. 

먼저 L1을 선택했을때(Left) 상태 L1로 전이하여 보상 -1을 얻는다. (반드시 선택한다는 전제이기에... 확률적 상태 전이는 100%라 1이다.)

![[Pasted image 20240518222527.png]]

만약 L2을 선택한다면(Right) 상태 L2로 전이하여 보상 1을 얻는다. 이 경우 값은 다음과 같다.

![[Pasted image 20240518222817.png]]

따라서 값이 더 큰 행동은 Right이다. 상태 L1에서 최적 행동은 오른쪽이란 것이다. 같은 방식으로 상태 2에서도 계산하면 왼쪽으로 가라고 나온다.

![[Pasted image 20240518223239.png|300]]

드디어 최적 정책을 찾았다. L1에서는 오른쪽, L2에서는 왼쪽으로 이동하는 행동이 최적 정책이다. 






