상태 가치 함수는 정말이지 좋은 가치 함수이다. 그렇지만 자꾸 상태 가치 함수대신 Q함수를 쓰고 우회하려고 한다 그 이유는 무엇일까??

이는 상태 가치 함수의 의미를 되짚어보면 알 수 있다.

### 1. 상태 가치 함수
---
![[Pasted image 20240518153756.png]]

이게 바로 벨만 방정식이다. "상태 s의 상태 가치 함수" 와 다음에 취할 수 있는 "상태 s'의 상태 가치 함수" 의 관계를 나타낸 식이다. 모든 상태 s와 모든 정책 pi에 대해 성립한다. 

식의 의미가 꽤나 중요한데
1. ![[Pasted image 20240518153131.png]] (주어진 정책을 기반으로) 상태 s일때 보상 R의 기댓값 과거에서부터 현재까지의 결과의 평균치가 담겨 있다. 

2. ![[Pasted image 20240518153224.png]] 현 상태에서 다음 상태로 전이했을때의 다음 보상 기댓값 (수익은 현재 보상에 할인율을 적용한 것이다. 가까운 미래의 보상을 더 중시하기 위해)

현재 상태(s)에서 다음에 들어올 기대수익을 더한 값으로, 어찌보면 미래를 내다 보는 거라고 볼 수 있다. 

즉, 현재 상태에서 에이전트가 앞으로 얻길 기대하는 수익이다. 

>1. 상태 가치 함수의 벨만 방정식
	- 현재 상태에서, 모든 정책을 고려했을 때, 앞으로 들어올 기대되는 수익은 얼마일까요? 

뭐가 문제인지 알았나?

그렇다.

![[Pasted image 20240620230240.png]]

"모든 정책을 고려 했을 때"이게 제일 문제이다. 모든 정책을 고려한다는 의미는 다시 말해 계산 복잡도가 어마무시하게 증가한다는 것을 알 수 있다. 


### 2. 최적 정책을 구하는 공식에서...
---
그래서 이전에 이런 설명을 했을 것이다. 

![[Pasted image 20240626161857.png]]

우리는 아래식을 사용하려면 환경 모델이 필요하다고 했다. 즉, p(s' | s, a)와 r(s, a, s')을 알 수 없다고 하였다.

그런데 생각해보자, 일반적으로 우리가 구한 Q함수에도 분명

![[Pasted image 20240518224308.png]]

p(s' | s, a)가 들어가있지 않나요?? 라고 할 수 있다.

결론만 말하면 맞다. 위 식들은 환경 모델이 있을때나 사용이 가능한 식들이다.

즉, 최적 정책을 구하는 식 이외에도 저렇게 유도된 식까지 싹 다 환경 모델이 없으면 사용이 불가능하다.

그래서 우리는 굳이 어렵게 어렵게 원시적인 식

![[Pasted image 20240625152638.png|400]]

을 사용해 나간것이다. 그럼 반대로 상태 가치 함수도?

![[Pasted image 20240302023224.png|300]]

이거를 쓰면 언제든 사용이 가능하다 문제가 있으면 진짜 너무나도 비효율적이라는게 문제.

그래서 우리는 매우 효율적인 TD법을 사용하는 것이다.

![[Pasted image 20240620230427.png|350]]

TD법을 잘 활용하기 위해 사용된 방법이 SARSA알고리즘 인것이다

