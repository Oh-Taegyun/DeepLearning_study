![[Pasted image 20240626151501.png]]

이런 맵이 있다고 하자, 만약 내가 임의로 선택한 경로가 다음과 같다. 각 경로에는 간단하게 미래에 얻을 수 있는 수익(G)값만 적어두자

![[Pasted image 20240626151702.png]]

만약 우리가 행동 정책을 100%로 잡으면 위 경로만 표기가 될 것이다.  따라서 이러한 문제점을 피하기 위해 ϵ-Greedy 알고리즘을 사용했다. 

![[Pasted image 20240626151800.png]]

![[Pasted image 20240626151819.png]]

조금 전으로 돌아가서 별점, 즉 reward가 1인 집을 가는데에는 여러가지 방법이 있다.  대부분이 보상이 없기에 굳이 돌아갈 이유가 없다. 그럼 어떻게 빠르게 도달하는 방법을 추론할 수 있을까?

![[Pasted image 20240626152404.png]]

방법은 바로 할인계수(γ)를 곱해서 이동하면 된다. 그러면 다음과 같이 γ가 1이 아닌 이상 최종적으로 출발지에서 더 먼길을 통하는 방향은 더 작은 값을 갖게 될 것이다.

![[Pasted image 20240626152417.png]]


### 우리 교재에서...
---

우리 교재에서 상태 가치 함수로 정책을 구하면 다음과 같이 된다.

![[Pasted image 20240626165901.png]]

![[Pasted image 20240626165917.png]]

![[Pasted image 20240626165929.png]]

하지만 만약 행동 가치 함수로 표현하면

![[Pasted image 20240626165257.png]]

가 된다. 왜 위는 1칸으로만 나타나는게 밑은 4개의 칸으로 나타날까??

이는 뭘 목표로 하느냐에 따라 달라지기 때문이다

![[Pasted image 20240518203252.png]]
![[Pasted image 20240625152638.png|400]]
![[Pasted image 20240302023224.png|300]]

일반적으로 상태 가치 함수는 "모든 정책"에 따른 미래에 얻을 수 있는 기대 보상을 말한다. 따라서 화살표의 방향 또한 기대 보상이 늘어나는 쪽으로 되어있다.

반대로 Q함수는 "1가지 정책"에 따른 미래에 얻을 수 있는 기대 보상을 말한다. 따라서 우리의 그리드 월드는 4가지의 행동 정책이 있으므로 4가지에 대한 기대 보상을 모두 나타낼 수 있다. 


![[Pasted image 20240526015945.png]]






