여태껏 Q러닝, SARSA, 몬테카를로법 등은 가치함수를 사용한 가치 기반 기법이다. 가치 기반 기법은 가치 함수를 모델링하고 가치 함수를 학습한다. 그리고 가치 함수를 '경유'하여 정책을 얻는다. 

또 다른 전략으로는 정책 경사법에 기반한 알고리즘이다. 


### 1. 가장 간단한 정책 경사법
---
정책 경사법은 경사, 즉 기울기를 이용하여 정책을 갱신하는 기법들의 총칭이다.

확률적 정책은 수식으로 π(a | s)로 표현한다. 그리고 신경망의 가중치 매개변수 전체를 θ 기호로 집약하여 표현하자. θ는 모든 매개변수의 원소들을 한 줄로 나열한 벡터이다. 그리고 신경망으로 구현한 정책을 π<sub>θ</sub>(a | s)로 표현하자

다음으로 정책 π<sub>θ</sub>를 이용하여 목적 함수를 설정한다. 목적 함수를 설정하면 이후 목적 함수의 값을 가장 크게 만드는 매개변수 θ를 찾아야 한다. 이 일이 '최적화'라고 불리는 작업이며 일반적인 신경망 학습 과정을 말한다. 

>최적화 문제를 풀 때 일반적으로 쓰이는 손실 함수 대신에 목적 함수를 설정한다. 손실 함수는 경사 하강법을 통해 최솟값을 찾는다. 반면 목적 함수는 경사 상승법으로 최댓값을 찾는다. 경사 하강법은 기울기에 마이너스를 곱한 방향으로, 경사 상승법은 플러스를 곱한 방향으로 매개변수를 갱신한다.  단, 목적 함수에 마니어스를 붙이면 손실 함수로 취급할 수 있으므로(반대도 마찬가지) 손실 함수와 목적 함수는 본질적으로 같은 역할을 한다.

그럼 정책 π<sub>θ</sub>를 사용하여 목적 함수를 설정해보자. 먼저 문제 설정을 명확히 해보자 일회성 과제이고 행동은 정책 π<sub>θ</sub>에 따라 선택한다고 해보자. 그리고 매 행동의 결과로 다음과 같이 "상태, 행동, 보상"으로 구성된 시계열 데이터를 얻었다고 가정하자

![[Pasted image 20240628144044.png]]

τ는 궤적을 뜻한다. 이떄 수익은 할인율을 이용해서 다음 식으로 표현할 수 있다.

![[Pasted image 20240628144631.png|300]]

수익을 τ로부터 계산할 수 있음을 명시하기 위해 G(τ)로 표기했다. 이때 목적 함수 J(θ)는 다음 식으로 표현된다.

![[Pasted image 20240628144847.png|200]]

>정책 목적 함수는 수익이 최대가 되는 정책을 구하는 함수이다. 

수익 G(τ)는 확률적으로 변하기 때문에 그 기댓값이 목적 함수가 된다. 지금 식에서 E의 첨자로 ''τ~π<sub>θ</sub>"가 붙어 있다. 시계열 데이터 τ가 정책 π<sub>θ</sub>로부터 생성됨을 뜻하는 표기 방식이다. 

>τ의 생성 과정에는 에이전트의 정책 외에도 환경의 상태 전이 확률 p(s' | s, a)와 보상 함수 r(s, a, s')도 관여한다. 그러나 우리가 통제할 수 있는 요인은 에이전트의 정책뿐이다. 그래서 E의 식처럼 ''τ~π<sub>θ</sub>"로만 표기하기로 하자

목적 함수가 정해지면 다음으로 그 기울기를 구한다. 매개변수 θ에 대한 기울기를 ᐁ<sub>θ</sub>로 표현하자. 우리의 목표는 ᐁ<sub>θ</sub> J(θ)를 구하는 것이고 결과는 다음과 같다.

![[Pasted image 20240628145749.png]]

이 식에서 주목할 점은 ᐁ<sub>θ</sub>가 E안에 들어있다는 점인데(기울기 계산은  ᐁ<sub>θ</sub> log π<sub>θ</sub>(A<sub>t</sub> | S<sub>t</sub>)) 이는 뒤에서 설명하겠다.

ᐁ<sub>θ</sub> J(θ)가 구해지면 이어서 신경망의 매개변수를 갱신한다. 적용할 수 있는 최적화 방법은 다양하며, 경사 상승법에 따른 간단한 방법을 다음 식으로 표현할 수 있다.

![[Pasted image 20240628150151.png|300]]

이 식과 같이 매개변수 θ를 기울기 방향으로 α만큼 갱신한다. 여기서는 α는 학습률이다. 

>손실 함수는 "이 모델이 얼마만큼 문제를 잘 못푸냐"에 초점을 맞춰서 손실함수를 줄이는 방향으로 신경망을 업데이트한다.
>목적 함수는 "우리가 얻을 수 있는 최대치의 목표가 뭐냐"에 초점을 맞춰서 수익을 높이는 방향으로 신경망을 업데이트한다. 

![[Pasted image 20240630005219.png]]
### 2. 정책 경사법 알고리즘
---
![[Pasted image 20240628145749.png]]

∇₀ 𝑱(𝜃)는 위 식과 같이 기댓값으로 표현된다. 이 기댓값은 몬테카를로법으로 구할 수 있다. 

에이전트를 정책 𝜋₀에 따라 실제로 행동하게 하여 𝑛개의 궤적 𝜏를 얻었다고 가정하자. 이때 각 𝜏에서 기댓값

![[Pasted image 20240628151053.png|200]]

를 계산하고 평균을 구하면 ∇₀ 𝑱(𝜃)를 근사할 수 있다. 수식으로는 다음과 같이 표현된다. 

![[Pasted image 20240628151246.png|400]]

이 식에서 𝑖번째 에피소드에서 얻은 궤적을 𝜏⁽ⁱ⁾, 𝑖번째 에피소드의 시간 𝑡에서의 행동을 𝐴ₜ⁽ⁱ⁾, 상태를 𝑆ₜ⁽ⁱ⁾로 표현했다.

참고로 몬테카를로법의 샘플 수가 1개일 때, 즉 앞의 식에서 𝑛 = 1일 때를 생각해보자. 이런 경우는 다음과 같이 단순화할 수 있다.

![[Pasted image 20240628151324.png|300]]

이번 장에서는 원리를 이해하기 쉽도록 위 식을 대상으로 한 정책 경사법을 다룰 것이다. 

위 식은 ![[Pasted image 20240628151415.png|100]]를 모든 시간(𝑡 = 0 ~ 𝑇)에서 구하고, 각 기울기에 수의 𝐺(𝜏)를 ‘가중치’로 곱하여 모두 더한다.

![[Pasted image 20240628151444.png]]

이제 그림에서 수행하는 계산의 ‘의미’를 생각해보자. 우선 log의 미분으로 다음의 식이 성립한다.

![[Pasted image 20240628151817.png|300]]
이 식과 같이 ![[Pasted image 20240628151415.png|100]]는 ![[Pasted image 20240628151933.png|100]]라는 기울기(벡터)에 ![[Pasted image 20240628151951.png|80]]를 곱한 것이다. 이로부터 ![[Pasted image 20240628151415.png|100]]와 ![[Pasted image 20240628151933.png|100]]는 같은 방향을 가리킨다는 사실을 알 수 있다. 

![[Pasted image 20240628151933.png|100]]는 상태 𝑆ₜ에서 행동 𝐴ₜ를 취할 확률이 가장 높아지는 방향을 가리킨다. 마찬가지로 ![[Pasted image 20240628151415.png|100]]도 상태 𝑆ₜ에서 행동 𝐴ₜ를 취할 확률이 가장 높아지는 방향을 가리킨다. 그 방향에 대해 식 ![[Pasted image 20240628152134.png|140]]와 같이 𝐺(𝜏)라는 ‘가중치’가 곱해진다.

>미적분에서 그레디언트는 함수가 가장 빨리 증가하는 방향 또는 방향 벡터이다. 
> ᐁ f(x<sub>0</sub>) : x0에서 f가 가장 빨리 증가하는 방향 또는 방향 벡터
> -ᐁ f(x<sub>0</sub>) : x0에서 f가 가장 빨리 감소하는 방향 또는 방향 벡터

예를 들어 에이전트가 수익 𝐺(𝜏)로 100을 얻었다고 해보자. 그렇다면 수익을 얻도록 해준 직전 행동이 더 잘 선택되도록 기울기를 조정해야 하니, 가중치를 100만큼 주어 강화한다는 뜻이다. 즉, 선택의 결과가 좋았다면 그만큼 직접 행동을 강화한다는 뜻이다. 반대로 좋지 않은 선택에 대해서는 직접 행동을 그만큼 약화시킨다.

### 3. 정책 경사법 구현
---
``` python
class Policy(nn.Module):
    def __init__(self, action_size):
	    # action_size : 행동의 수 
        super().__init__()
        self.l1 = nn.Linear(4, 128)
        self.l2 = nn.Linear(128, action_size)
		
    def forward(self, x):
        x = F.relu(self.l1(x))
        x = F.softmax(self.l2(x), dim=1) # 출력을 소프트 맥스로 해서 각 행동의 확률을 얻을 수 있다. 
        return x
```

``` python
class Agent:
    def __init__(self):
        self.gamma = 0.98
        self.lr = 0.0002
        self.action_size = 2
		
        self.memory = []
        self.pi = Policy(self.action_size)
        self.optimizer = optim.Adam(self.pi.parameters(), lr=self.lr)
		
    def get_action(self, state):
        state = torch.tensor(state[np.newaxis, :]) # 배치 처리용 축 추가
        probs = self.pi(state) # 신경망의 순전파 수행해서 확률 분포 probs를 얻는다. 
        probs = probs[0] 
        m = Categorical(probs) # 확률 분포에 따라 하나의 행동 샘플링
        action = m.sample().item()
        return action, probs[action] # 선택된 행동과 확률 반환
		
    def add(self, reward, prob):
        data = (reward, prob)
        self.memory.append(data)
		
    def update(self): # 목표 도달하면 호출
        G, loss = 0, 0
        for reward, prob in reversed(self.memory):
            G = reward + self.gamma * G
		
        for reward, prob in self.memory:
            loss += - torch.log(prob) * G
		
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        self.memory = []
```

이 것을 카트 폴 문제에 대입하면 점차 보상의 총합이 커지나, 아직 개선할 점이 많다.