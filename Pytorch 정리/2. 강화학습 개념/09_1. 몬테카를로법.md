지금까지는 환경 모델이 알려진 문제를 다뤘다. 예를 들어 '그리드 월드' 문제에서는 에이전트의 행동에 따른 다음 상태 (위치)와 보상이 명확했다. 수식으로 표현하면 상태 전이 확률 p(s' | s, a)와 보상 함수  r(s, a, s')를 이용할 수 있었다. (상태 전이가 결정적이라면 s' = f(s, a)함수로 나타낼 수도 있다.) 이처럼 환경 모델이 알려진 문제에서는 에이전트 측에서 '상태, 행동, 보상'의 전이를 시뮬레이션 할 수 있다. 

하지만 우리의 최종 목표는 환경 모델을 모른다!. 이를 어떻게 해결해야할까? 당연하게 에이전트가 여러 시행착오를 거쳐 직접 환경 모델에 대한 정보를 얻어야 한다. 

### 1. 주사위 예제
---
주사위 두 개를 굴리는 문제를 생각해보자. 주사위 눈의 합을 확률 분포로 나타낸다면, 

![[Pasted image 20240526004627.png]]

이산 확률 분포표를 얻었다면, 기댓값을 구하기 너무나도 쉽다. 

이렇게 확률 분포로 표현된 모델을 분포 모델이라고 한다. 모델을 표현하는 방법에는 샘플 모델도 있다. 샘플 모델이란 "표본을 추출(샘플링)할 수만 있으면 충분하다." 라는 모델이다. (수학적으론 표본 집단)

1. 분포 모델 : 확률 분포를 정확하게 알고 있다.
2. 샘플 모델 : 샘플링만 할 수 있다면 뭔들 못하겠는가?

이다.

위의 주사위로 예시를 들자면, 직접 굴려서(샘플링해서) 나온 눈의 합을 구하는 방법이다.

>이처럼 샘플링을 많이 해서 평균을 구하는 방법이 몬테카를로법이다. 

몬테카를로법은 당연하게도 샘플 수가 늘어나면 늘어날수록 정확해진다. (표본 집단이 모집단에 근사하게 된다.) 즉, 분산이 작아진다. 


##### 다시 돌아와서
만약 내가 위의 주사위 문제를 샘플 모델로 구현해보겠다.

한 주사위를 1000번 던져서 나온 값이(기댓값) 6.98이라고 하자, 대략 분포 모델에서 얻는 평균이 7인걸 감안하면 얼추 맞다. 

당연히 증분 방식으로 구하는 것이 더 효율적인 계산법일 것이다.  


### 2. 몬테카를로법으로 정책 평가하기
---
가치 함수의 기본적인 정의는 다음과 같다.

![[Pasted image 20240302023224.png|300]]

>정책 π에 따라 행동했을 때 얻을 수 있는 기대 수익 

이 가치 함수를 몬테카를로법으로 계산해보자. 이를 위해 정책에 따라 실제로 행동을 취하도록 해보자.

![[Pasted image 20240526005537.png]]

상태  s에서 시작하여 얻은 수익을 G로 표기하고, i번째 에피소드에서 얻을 수익을 G<sup>(i)</sup>
로 표기하자.

예시를 들자면,

![[Pasted image 20240526010141.png]]

똑같은 상태에서 시작해도, 에이전트의 정책이 확률적이거나 환경의 상태 전이가 확률적이기 때문에, 보상 또한 확률적으로 달라진다. 

따라서 이들의 평균은 (3 + 2) / 2 = 2.5으로, 이 시점의 가치 함수는 V<sub>π</sub>(s)는 2.5가 된다. 

>즉 몬테카를로법은 이렇게 각 시나리오에서 얻은 수익을 바탕으로 기댓값을 구하는 아주 단순 무식한 방법이다.
>
>수익을 바탕으로 미래의 가능성을 예측하는게 상태 가치 함수인데, 에초에 과거 데이터를 바탕으로 하지 않았던가? 이놈도 그냥 모든 시나리오에 대해서 전부 파악한 뒤에 미래를 예측하겠단 의미이다. 

### 3. 모든 상태의 가치 함수 구하기
---
위의 과정을 하나의 상태에서만 주목하는 것이 아니라, 모든 상태에서의 가치 함수를 구해보자. 

![[Pasted image 20240526010601.png]]

각 상태에서부터 출발하여 실제로 행동을 수행하고 샘플 데이터를 수집한 다음에 각 상태에서의 수익을 평균을 내면 가치 함수를 구할 수 있다.

문제는 이 방법은 계산 효율이 안좋다... 그리고 시작 상태를 원하는 대로 설정할 수 있는 문제에서만 적용이 가능하다. 시뮬레이터의 위치를 임의로 설정한다거나, 게임에서 에이전트의 시작 위치를 임의로 설정한다거나...

더 효율적인 방법은 아래와 같다.

![[Pasted image 20240526011240.png|200]]

위 그림은 상태 A에서 출발해 정책(π)에 따라 행동한 결과다. A, B, C 순서로 상태를 거쳐 목표에 도달했을 때, 도중에 얻은 보상은 위 그림과 같다. 할인율이 γ일때, 상태 A에서 출발해서 얻은 수익은 다음 식으로 표현된다.

G<sub>A</sub> = R<sub>0</sub> + γR<sub>1</sub> + γ<sup>2</sup>R<sub>2</sub> 

이것이 상태 A를 시작 위치로 했을 때의 수익이다.

![[Pasted image 20240526011536.png|400]]

이런식으로 때어낸다면, 상태 B를 시작 위치로 했을 때의 수익이라고 볼 수 있다. 

G<sub>B</sub> = R<sub>1</sub> + γR<sub>2</sub> 

마찬가지로 상태 C에서 시작했을 때의 수익은 다음과 같다.

G<sub>C</sub> = R<sub>2</sub> 

이와 같이 '한 번의 시도'만에 '세 가지 상태에 대한 수익(샘플 데이터)'를 얻었다.

무척이나 단순 무식한 방법이라고 생각할 수 있는데, 에이전트의 시작 위치가 고정되어 있더라도 에피소드를 반복하는 동안 모든 상태를 경유할 수 있다면 모든 상태에 대한 수익 샘플 데이터를 수집할 수 있다. 예를 들어 에이전트가 무작위로 행동한다면 에피소드를 반복하면서 다양한 상태로 전이할 것이고, 결국 모든 상태를 경유할 수 있다. 그렇다면 에이전트의 시작 상태를 임의 위치에 설정할 필요가 없다.


### 4. 계산 효율 개선
---
1. G<sub>A</sub> = R<sub>0</sub> + γR<sub>1</sub> + γ<sup>2</sup>R<sub>2</sub> 
2. G<sub>B</sub> = R<sub>1</sub> + γR<sub>2</sub> 
3. G<sub>C</sub> = R<sub>2</sub> 

이 식을 다음처럼 바꿀 수 있다.

1. G<sub>A</sub> = R<sub>0</sub> + G<sub>B</sub>
2. G<sub>B</sub> = R<sub>1</sub> + G<sub>C</sub>
3. G<sub>C</sub> = R<sub>2</sub> 

즉, G<sub>C</sub>를 구한다면 위의 식들은 금방 구할 수 있을 것이다. 

