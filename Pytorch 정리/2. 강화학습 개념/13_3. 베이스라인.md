REINFORCE를 개선하는 베이스 라인이라는 기술을 알아보자

### 1. 베이스라인 아이디어
---
A, B, C라는 세 사람이 시험을 치렀고 결과는 다음과 같다.

| 인원  | 점수  |
| --- | --- |
| A   | 90  |
| B   | 40  |
| C   | 50  |

시험 성적의 분산을 구해보자. 시험 성적의 분산은 466.6667으로 큰 값이다. 분산은 '데이터의 흩어진 정도'를 나타내므로 점수의 편차가 심하다는 뜻이다. 이 분산을 줄일 방법을 생각해보자

이전 시험 성적들을 이용해 볼 수 있다. 예를 들어 지금까지의 시험 성적이 다음과 같았다고 해보자

![[Pasted image 20240630015638.png]]

이와 같이 이전 시험들의 성적이 주어지면 다음 시험의 점수를 예측할 수 있다. 간단한 방법으로는 이전 시험들의 평균을 내는 방법이 있다. 다음 시험 성적은 지금까지의 평균과 같을 거라고 예측하는 것이다. 

위의 결과를 각각 평균하면 A는 82, B는 46, C는 49라고 가정해보자. 이를 "예측값"으로 사용해서 다음 시험의 실제 결과와 얼마나 차이가 나는지 보자.

![[Pasted image 20240630015659.png]]

이렇게 되면 분산이 32.66이 된다. 어떤 결과에서 예측값을 빼면 분산을 줄일 수 있다. 예측값의 정확도가 높을수록 분산은 작아진다. 이것이 바로 베이스라인 기법의 아이디어이다. 지금 예에서는 평균을 베이스라인으로 이용한다. 

### 2. 베이스라인을 적용한 정책 경사법
---
![[Pasted image 20240630021231.png]]

위 식은 REINFORCE의 식이고 밑은 REINFORCE에 베이스라인을 적용한 식이다. 여기서 b(S<sub>t</sub>)는 임의의 함수이다. 즉, 입력이 S<sub>t</sub>이기만 하면 어떤 함수라도 상관이 없다는 뜻이다. 이 b(S<sub>t</sub>)이 베이스라인이다. 

예를 들어 상태 S<sub>t</sub>에서 지금까지 얻은 보상의 평균을 b(S<sub>t</sub>)로 사용할 수 있다. 위의 시험 성적 예가 여기에 해당한다. 그리고 실무에서는 가치 함수를 많이 사용한다. 수식으로 쓰면 b(S<sub>t</sub>) = V<sub>πθ</sub>(S<sub>t</sub>)가 된다. 베이스라인을 적용하여 분산을 줄일 수 있다면 학습 시 샘플 효율이 좋아진다. 

참고로 베이스라인으로 가치 함수를 사용하면 실제 가치 함수 v<sub>πθ</sub>(S<sub>t</sub>)를 알 수 없다. 이 경우 가치 함수에 대해서도 학습해야 한다.

### 3. 베이스 라인을 사용하는 이유
---
직관적으로 보자

![[Pasted image 20240630021919.png]]

막대가 균형을 잃어 게임이 끝나기 직전이다. 이 상태에서는 어떤 행동을 선택하던 몇 단계 후에 게임이 종료될 것이다. 

그림의 상태를 s, 이 상태에서의 행동을 a라고 하자, 그리고 상태 s에서 몇 단계 후, 예를 들어 3단계 후에는 반드시 게임이 끝난다고 가정하자. 그러면 상태 s에서의 수익은 3이 된다. (수익률은 1로 가정) 

이 조건에서 기본적은 REINFORCE라면 상태 s에서 행동 a는 가중치 3만큼 강화된다. (상태 s에서 행동 a가 선택될 확률이 커짐). 하지만 어떤 행동을 하든 3단게 후에는 반드시 게임이 끝나기 때문에 행동 a가 선택될 확률을 높이는 건 아무런 의미가 없다.

이때 베이스 라인이 등장한다. 베이스라인으로 가치 함수를 사용하고 위 그림의 예시에서 V<sub>πθ</sub>(S<sub>t</sub>) = 3임을 알고 있다고 가정하자(실제로는 몬테카를로법이나, TD법 등으로 학습하여 추정해야 한다) 그렇다면 가중치는 G-V<sub>πθ</sub>이므로 결국 0이다. 가중치가 0이므로 어떤 행동을 선택하든 그 행동이 선택될 확률은 커지지도 작아지지도 않는다. 이처럼 베이스라인을 적용하면 학습 과정에서의 낭비를 줄일 수 있다.



