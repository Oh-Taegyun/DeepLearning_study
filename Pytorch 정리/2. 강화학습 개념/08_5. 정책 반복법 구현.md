![[Pasted image 20240519164115.png|400]]

이 문제에서

![[Pasted image 20240525212050.png]]

일 때, 

``` python
def argmax(d):
    """d (dict)"""
    max_value = max(d.values())
    max_key = -1
    for key, value in d.items():
        if value == max_value:
            max_key = key
    return max_key

def greedy_policy(V, env, gamma): # 가치 함수에 대해 탐욕 정책
    pi = {} 

    for state in env.states(): # 모든 상태에 대해서
        action_values = {}

        for action in env.actions(): # 4가지의 행동 불러오기 
            next_state = env.next_state(state, action)
            r = env.reward(state, action, next_state)
            value = r + gamma * V[next_state] # r(s, a, s') + r * v_pi(s')계산
            action_values[action] = value # 엑션에 따른 보상 담기

        max_action = argmax(action_values) # argmax 호출 가장 큰 보상의 액션 인덱스 추출
        action_probs = {0: 0, 1: 0, 2: 0, 3: 0} # 4가지의 행동 중 가장 큰 행동 중에
        action_probs[max_action] = 1.0 # Max_action이 1.0(결정적)이 되도록 확률 분포 생성
        pi[state] = action_probs # 현 상태의 정책을 저장
    return pi
```

### 1. 평가와 개선 반복
---
정책 반복법을 만들어보자

파라미터
1. env(Environment) : 환경
2. gamma(float) : 할인율
3. threshold(float) : 정책을 평가할 때 갱신을 중지하기 위한 임계값
4. is_render(bool) : 정책 평가 및 개선 과정을 렌더링할지 여부

``` python
def policy_iter(env, gamma, threshold=0.001, is_render=True):
    pi = defaultdict(lambda: {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25})
    V = defaultdict(lambda: 0)
    
    while True:
        V = policy_eval(pi, V, env, gamma, threshold)  # 현재 상태 가치 함수 반환
        new_pi = greedy_policy(V, env, gamma)          # 상태 가치 함수를 토대로 최적 정책 추출

        if is_render:
            env.render_v(V, pi)
            
        if new_pi == pi:  # 갱신 여부 확인 갱신이 안되면, 벨만 최적 방정식 만족
            break # 그때의 pi가 최적 정책이
        pi = new_pi # 개선된 

    return pi
```

![[Pasted image 20240525213127.png]] 

무작위 정책으로 시작했을때, 위와 같다.

![[Pasted image 20240525213951.png]]

최적 정책은 위와 같다. 


