![[Pasted image 20240301223828 1.png]]

밴디트 문제에서는 슬롯머신 각각의 특성이 서로 다르다고 가정한다. 즉, 좋은 그림 조합이 나올 확률이 각각 다른 벤디트 머신이 여러대가 있는 것이다. 


### 1. 어떤 머신을 고르는 것이 제일 좋을까?
---
처음 플레이어는 각각의 머신의 확률을 모른다. 머신 A에서는 0.8의 확률로 코인 1개를 줄 수 있고, 머신 B는 0.1의 확률로 코인 1개를 준다고 가정하자

당연하게 플레이어는 돈을 많이 벌려면 머신 A를 여러번 돌리는 것이 타당하다. 

하지만 만약 다음과 같은 확률을 가진 슬롯머신들이 있다면 어떤가?

1. 슬롯머신 A. (이산확률분포표)
	![[Pasted image 20240301224719 1.png|400]]

2. 슬롯머신 B. (이산확률분포표)
	![[Pasted image 20240301224744 1.png|400]]

이러면 문제가 좀 어려워진다. 얻을 수 있는 코인 개수(보상)이 플레이할때마다 달라지기 떄문이다. 슬롯머신에서 가장 중요한 특성은 무작위성이다. 얻을 수 있는 코인 개수(보상)이 플레이할때마다 달라지기 때문이다. 이 무작위성에 맞게 확률을 이용해서 문제를 풀어야 한다. 이제부터는 확률문제로 넘어가게 된다. 


### 2. 기댓값
---
위 슬롯머신을 무한대로 돌리면 값이 일정한 값으로 수렴하게 될 것이다. 이 값이 바로 기댓값이라고 한다.

1. 슬롯머신 A 기댓값
	(0 * 0.7) + (1 * 0.15) + (5 * 0.12) + (10 * 0.03) = 1.05
2. 슬롯머신 B 기댓값
	(0 * 0.5) + (1 * 0.4) + (5 * 0.09) + (10 * 0.01) = 0.95

이렇게 기댓값을 기준으로 어떤 슬롯머신이 더 좋은지 비교할 수 있다. 따라서 매번 A를 선택하는 것이 더 많은 돈을 얻게 될 것이다.


### 3. 가치 추정 방법
---
당연하게 플레이어는 처음에는 어떤 머신이 더 좋은지 어떤 머신이 어느정도의 확률을 가지고 있는지 전혀 모른다. 그럼 어떻게 해야할까? 

당연하게 아무 정보가 없을때는 아무거나 머신을 돌려봐야한다. 머신 A와 B를 각각 3번씩 돌려봐서 표를 정리해보자

![[Pasted image 20240301230949 1.png|400]]

그리고 기댓값을 각각 나타내보면 
1. 슬롯머신 a의 행동 가치 - Q(a) = 2
2. 슬롯머신 b의 행동 가치 - Q(b) = 0.333

이렇게 3번만 확인해봤을때 현재 상태로는 a가 좋은 슬롯머신이니까 a를 계속 돌리는 것이 타당하다.

이렇게 슬롯머신을 실제로 플레이하여 얻은 보상은 어떤 확률 분포에서 생성된 샘플(표본)이다. 따라서 실제 휙득한 보상의 평균을 표본 평균이라고 할 수 있다. 표본 평균은 샘플링 횟수가 늘어날 수록 실제값(보상의 기댓값)에 가까워진다.

그럼 이러한 과정을 수식으로 써보자

![[Pasted image 20240301231408 1.png|300]]

근데 이 수식은 컴퓨터 계산상 그렇게 좋진 않다. 왜냐면 플레이 횟수(n)이 늘어날 수록 보상(R)의 원소 수가 늘어난다. 따라서 `sum(rewards)`코드의 실행 비용도 함께 증가한다. 즉, 플레이 횟수가 늘어날수록 메모리와 계산량이 너무 늘어나는게 문제

따라서 다음처럼 수식을 변경하겠다. 

![[Pasted image 20240301232209 1.png]]

여기서 중요한 것은 ![[Pasted image 20240301232310 1.png|170]]이다.

`Q_n`은 `Q_(n-1)`에 어떤 값을 더해 구할 수 있다. 이를 그림으로 그리면 다음과 같다. 

![[Pasted image 20240301232824 1.png]]

이전의 행동 가치`(Q_(n-1))`는 `R_n`방향으로 얼마만큼의 `1/n`을 곱했냐에 따라서 갱신되는 양이 다르다. 즉, 이러한 `1/n`은 갱신되는 양을 조절하기 때문에 `학습률`과 같은 역할을 한다.

``` python
Q += (reward - Q) / n
```

이렇게 Q1, Q2, Q3, ... 식으로 하나씩 순서대로 증가시키며 구할 수 있다는 뜻에서 이런 구현 방식을 증분 구현이라고 한다.


### 4. 플레이어의 정책
---
그럼 플레이어는 어떤 정책을 따라야 할까? 가장 먼저 떠오르는 방식은 그냥 아무거나 실제로 플레이해보고 결과가 가장 좋은 슬롯 머신을 선택하는 것이다. 

1. 탐욕 정책
	각각 다 플레이 해보고 가치 추정치(실제 휙득한 보상의 평균)이 가장 큰 슬롯머신을 선택하는 정책

하지만 당장 좋은 슬롯머신을 무한대로 선택했더니, 사실 운이 좋아서 초반에만 좋은 결과를 가져다 줬을 뿐 실제 기댓값은 낮을 수 있다. 불확실성이 스며 있기 때문에 생기는 문제이다. 확실하지 않고 그저 운이 좋았을 뿐인 결과를 전적으로 믿어가면서 플레이하면 이러한 불확실성이 매우 커지게 될 수 밖에 없다. 

따라서 다음 정책들을 선택할 수 있다.

1. 활용(Exploitaion) : 지금까지 실제로 플레이한 결과를 바탕으로 가장 좋다고 생각되는 슬롯머신을 플레이한다. (탐욕 정책)
2. 탐색(Exploration) : 슬롯머신을 가치를 정확하게 추정하기 위해 다양한 슬롯머신을 시도

즉, 탐색을 통해서 조금이나마 다양한 슬롯머신을 돌려봐야 한단 것이다.

밴디트 문제에서 단기적인 실행으로 좋은 결과를 얻어야 한다면 당연하게 활용을 선택해야 한다. 하지만 장기적인 관점에서 더 나은 결과를 얻고 싶다면, '탐색'이 필요하다.

강화학습은 이렇게 `활용과 탐색의 균형`을 어떻게 잡느냐에 따라 달라진다. 균형을 맞추기 위해 여러 알고리즘이 제공되었지만 가장 기본적인 `ɛ-그리디정책` 을 보도록 하자


### 5. ɛ-그리디 정책
---
ɛ의 확률로 탐색을 하고, 계속 활용을 하는 단순한 알고리즘이다.

ɛ의 확률로 탐색을 하여 다음 행동을 무작위로 선택해서 다양한 경험을 쌓고, 당장의 상황에서 활용을 하여 가능한 모든 행동 가치 추정치의 신뢰도가 조금이나마 높이는 기술이다. 가장 잘 알려진 ɛ의 확률은 0.1이다.

``` python
class Bandit: # 밴디트 머신
    def __init__(self, arms=10):
        self.rates = np.random.rand(arms) # 여러대의 머신의 고유 확률을 설정
        
    def play(self, arm):
        rate = self.rates[arm] # 임의의 머신을 선택했을 때 코인 반환
        if rate > np.random.rand():
            return 1
        else:
            return 0

class Agent:
    def __init__(self, epsilon, action_size=10):
        self.epsilon = epsilon # ɛ 설정
        self.Qs = np.zeros(action_size) # 각 슬롯머신의 가치 추정치
        self.ns = np.zeros(action_size) # 각 슬롯머신의 플레이 횟수 

    def update(self, action, reward):
        self.ns[action] += 1 # action 번째 슬롯머신을 플레이한 횟수 증가 
        self.Qs[action] += (reward - self.Qs[action]) / self.ns[action] # 가치 추정치 업데이트
        
    def get_action(self):
        if np.random.rand() < self.epsilon: # 정해둔 ɛ보다 낮다면 
            return np.random.randint(0, len(self.Qs)) # 랜덤으로 선택후 경험치를 쌓는다
        return np.argmax(self.Qs) # 아니면 가장 좋은 머신을 선택해서 플레이
```

사용된 함수 정보
1. `np.random.rand(a)` : 0.0 ~ 1.0 사이의 무작위 수 생성 a가 입력되면 a만큼 생성해준다. 반환값은 ndarray 객체를 반환. 

![[Pasted image 20240302001330 1.png]]
![[Pasted image 20240302001345 1.png]]

구현한 ɛ-탐욕 정책이 잘 작동되는 것을 확인할 수 있다. 

하지만 당연하게 운이 좋은 경우도 있을 수 있고, 운이 나쁜 경우도 있을 수 있다. 즉, 무작위성 때문에 플레이할때마다 매번 결과가 다르다는 것이다. 

따라서 강화 학습 알고리즘을 비교할때 한 번의 실험만으로 판단 하는 것은 큰 의미가 없다. 알고리즘의 평균적인 우수성을 평가해야 한다. 같은 실험을 여러 번 반복하여 결과를 평균하는 식으로 알고리즘의 평균적인 우수성을 알 수 있다.



