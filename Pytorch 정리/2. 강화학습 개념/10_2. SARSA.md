TD법으로 정책을 평가했으니, 이젠 정책을 제어해보자

### 1. 온-정책 SARSA
---
TD법에서는 가치 함수를 평가했다. 근데 뭔가 TD법을 보면 생각나지 않는가?  

![[Pasted image 20240620230427.png|350]]

아 저기서 이후의 정책이 모두 선택된다면?? 그것은 이제 행동 가치 함수로 표현할 수 있지 않을까? 라고 생각해봤다.

그 때문인지 정책을 제어할 때는 상태 가치 함수가 아니라 행동 가치 함수가 대상이다. Q<sub>π</sub>(s, a)가 대상이다. 개선 단계에서는 정책을 탐욕화해야 하며, 상태 가치 함수는 V<sub>π</sub>(s)의 경우에는 환경 모델이 필요하다. (우리가 이 고생을 하는 이유도 상태 가치 함수는 환경 모델이 있다는 가정 하에 수행하는데... 안타깝게도 우리의 최종 목적은 환경 모델이 결정되어 있지 않다.)

>하지만 행동 가치 함수의 장점은 환경 모델이 필요가 없다는 것이다! 

![[Pasted image 20240621003306.png]]

단순히 Q(s,a)가 최대가 되는 행동 a만 찾으면 그만인거라 환경 모델이 필요가 없다.  

이제 TD법에서 구한 상태 가치 함수를 보자

![[Pasted image 20240621004520.png]]

이렇게 바꿀 수 있을 것이다. 이렇게 바꿀 수 있는 이유는 에초에 상태 가치 함수로부터 행동 가치 함수가 나왔기 때문이다. 

>TD법을 행동 가치 함수로 구하는 알고리즘이 SARSA이다.