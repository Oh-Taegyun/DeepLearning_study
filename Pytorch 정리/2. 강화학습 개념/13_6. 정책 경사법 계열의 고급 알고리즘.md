좀 더 고도화된 알고리즘을 소개해보겠다. 

### 1. A3C
---
Asynchronous Advantage Actor-Critic의 약자이다. 이름에 A가 세 개 , C가 1개여서 A3C라고 한다. 

특징은 이름에 나와 있듯, 비동기(Asynchronous)라는 점이다. 여기서 말하는 비동기란 여러 에이전트가 병렬(Parallel)로 행동하며 비동기적으로 매개변수를 갱신한다는 뜻이다. 

A3C는 신경망으로 모델링한 행위자-비평자를 이용해서 정책을 학습한다. 그리고 다음 그림과 같이 하나의 전역 신경망과 여러 개의 지역 신경망을 사용한다.

![[Pasted image 20240630211143.png]]

지역 신경망은 각자의 환경에서 독립적으로 플레이하며 학습한다. 그리고 학습 결과인 기울기를 전역 신경망에 보낸다. 전역 신경망은 여러 지역 신경망에서 보내온 기울기를 이용해 비동기적으로 가중치 매개변수를 갱신한다. 이렇게 전역 신경망의 가중치 매개변수를 갱신 하는 동안 주기적으로 전역 신경망과 지역 신경망의 가중치 매개변수를 동기화한다.

A3C의 장점은 여러 에이전트를 병렬로 행동하게 하여 학습 속도를 높인다는 것이다. 게다가 에이전트들이 독립적으로 행동(탐색)하기 때문에 한층 다양한 데이터를 얻을 수 있다. 그 덕분에 학습 데이터의 전체적인 상관관계를 약화시킬 수 있어 학습이 더욱 안정적으로 이루어진다. 

>DQN에서는 학습 데이터의 상관관계를 약화시키기 위해 경험 재생 기법을 사용했다. 경험 재생은 경험 데이터를 버퍼에 저장해두고, 거기서 무작위로 여러 개를 골라내는 기법이다. 하지만, 경험 재생은 온-정책 방식으로는 사용할 수 없다. 메모리 사용량과 계산량이 커진다는 것도 단점이다.

병렬 처리는 온-정책 방식에서도 사용할 수 있는 범용 아이디어이다. 여러 에이전트를 병렬로 움직여 (경험 재생에 의존하지 않고도) 데이터의 상관관계를 약화시킬 수 있다. 실제로 A3C 논문 발표 이후 많은 연구에서 병렬 처리가 유행처럼 번지고 있다.

또한 A3C의 행위자-비평자는 신경망의 가중치를 공유한다. 

![[Pasted image 20240630211628.png]]

그림과 같이 하나의 신경망에서 가중치를 공유하여 정책과 가치 함수를 출력한다. 정책과 가치 함수 모두 입력에 가까운 층은 가중치가 비슷할 것이라서 이처럼 신경망 사이에 매개변수를 공유하는 구조가 효과가 있다.


### 2. A2C
---
매개변수를 (비동기가 아닌) 동기적으로 갱신한다. 

![[Pasted image 20240630212243.png]]

그림과 같이 각각의 환경에서 에이전트가 독립적으로 동작한다. 따라서 시간 t에서의 상태는 환경마다 다르다. 그리고 시간 t에서 각 환경의 상태를 (동기화하여) 배치로 묶어 신경망을 학습시킨다. 이때 신경망의 출력 (정책의 확률 분포)에서 다음 행동을 샘플링하며, 샘플링된 행동을 각 환경에 전달한다.

실험 결과 A2C의 방식의 동기식 갱신 성능도 A3C보다 떨어지지 않았다. 그런데 구현하기는 A2C가 더 쉽고 GPU 같은 컴퓨터 하드웨어 자원을 더 효율적으로 사용할 수 있었다. 이런 장점 때문에 실무에서는 A2C를 더 많이 이용한다. 


### 3. DDPG
---
정책을 직접 모델링하는 정책 경사법 같은 기법은 행동 공간이 연속적인 문제에도 활용할 수 있다. 예를 들어

![[Pasted image 20240630212614.png]]

위 그림과 같이 정책을 모델링한 신경망이 '정규분포의 평균'을 출력하도록 설계할 수 있다. 실제 행동은 이 정규분포에서 샘플링하여 얻는다.

DDPG는 Deep Deterministic Policy Gradient method(심층 결정적 정책 경사법)의 약자이다. 이름이 말해주듯 연속적인 행동 공간에서의 문제에 맞춰 설계된 알고리즘이다. 

![[Pasted image 20240630212808.png]]

위 그림과 같이 이 알고리즘의 신경망은 행동을 연속적인 값으로 직접 출력한다.

DDPG의 정책은 특정 상태 s를 입력하면 행동 a가 고유하게 결정되기 때문에 결정적 정책이다. DDPG에서는 이 결정적 정책을 DQN에 통합한다. 정책을 나타내는 신경망을 μ<sub>θ</sub>(s)라 하고, DQN의 Q 함수를 나타내는 신경망을 Q<sub>Φ</sub>(s, a)라 하며, θ와 Φ(파이)는 각 신경망의 매개변수이다. 이때 DDPG는 다음 두 가지 학습 과정을 거쳐 매개변수를 갱신한다.

1. Q 함수 출력이 커지도록 정책 μ<sub>θ</sub>(s)의 매개변수 θ를 갱신
2. DQN에서 수행하는 Q러닝을 통해 Q함수 Q<sub>Φ</sub>(s, a)의 매개변수 Φ를 갱신


##### 1. Q 함수 출력이 커지도록 정책 μ<sub>θ</sub>(s)의 매개변수 θ를 갱신

첫 번째 학습 과정부터 알아보자, 첫 번재 학습은 다음 그림과 같이

![[Pasted image 20240630213512.png]]

두 개의 신경망을 조합하여 Q함수의 출력이 최대가 되도록 결정적 정책 μ<sub>θ</sub>(s)의 매개변수 θ를 갱신하는 것이다. 위 그림에서 중요한 점은 μ<sub>θ</sub>(s)가 출력하는 행동 a가 연속적인 값이고, 이 출력 a가 그대로 Q<sub>Φ</sub>(s, a)의 입력이 된다는 것이다. 이렇게 연결된 두 신경망에서 역전파를 수행며, 그 결과로 기울기 ᐁ<sub>θ</sub>q가 구해지고 (q는 Q함수의 출력) 기울기 ᐁ<sub>θ</sub>q를 이용해 매개변수 θ를 갱신할 수 있다. 

>만약 위 그림에서 확률적 정책으로 샘플링 했다면, 역전파가 샘플링을 하는 지점에서 멈춘다. 샘플링 이후로는 기울기가 0으로만 전달된다는 이야기이다. 이렇게 되면 정책의 매개변수를 갱신할 수 없다.

##### 2. DQN에서 수행하는 Q러닝을 통해 Q함수 Q<sub>Φ</sub>(s, a)의 매개변수 Φ를 갱신

두 번째 학습은 DQN으로 하는 Q러닝이다. (이전에 쓰던 방법과 동일) 다만 이번에는 결정적 정책 μ<sub>θ</sub>(s)를 사용하여 계산 효율을 높일 수 있다.

DQN에서 Q 함수의 갱신은 Q<sub>Φ</sub>(Sₜ, Aₜ)의 값이 Rₜ + γmax Q<sub>Φ</sub>(Sₜ₊₁, a)가 되도록(또는 근접하도록) 하는 것이었다. 그런데 앞서 첫 번째 학습을 통해 정책 μ₀(s)는 Q 함수가 커지는 행동을 출력한다. 따라서 다음 근사를 적용할 수 있다.

max<sub>a</sub> Q<sub>Φ</sub>(s, a) ≃ Q<sub>Φ</sub>(s, μ₀(s))

최댓값을 구하는 max<sub>a</sub>는 일반적으로 계산량이 많다. 그런데 DDPG에서는 max<sub>a</sub> Q<sub>Φ</sub>(s, a) 계산을 Q<sub>Φ</sub>(s, μ₀(s))라는 두 개의 신경망 순전파로 대체한다. 이렇게 계산을 단순화하여 학습 효율을 높이는 것이다.

>DDPG는 '소프트 목표'와 '탐색 노이즈'라는 아이디어도 활용한다. 소프트 목표는 DQN의 '목표 신경망'을 '부드럽게' 만든 기법이다. 목표 신경망의 매개변수를 학습 중인 매개변수와 주기적으로 동기화하는 대신, '매번' 학습 중인 매개변수의 방향에 가까워지도록 갱신하는 것이다. '탐색 노이즈'는 결정적 정책에 노이즈를 넣어 행동에 무작위성을 주입하는 기법이다. 

### 4. TRPO, PPO
---
정책 경사법에서는 정책을 신경망으로 모델링하고 기울기 기반으로 매개변수를 갱신한다. 정책 경사법은 기울기로 매개변수의 갱신 '방향'은 알 수 있지만, 얼마만큼 갱신해야 좋은가를 뜻하는 '갱신 폭'은 알 수 없다는 단점이 있다. 폭이 너무 넓으면 정책이 나빠지고, 반대로 너무 좁으면 학습이 거의 진행되지 않는다. 이 문제를 해결한 것이 TRPO(Trust Region Policy Optimization)이다. '신뢰 영역 정책 최적화'라고 번역할 수 있다. 이름에서 알 수 있듯이 신뢰할 수 있는 영역 안에서, 즉 적절한 갱신 폭으로 정책을 최적화할 수 있다.

두 확률 분포가 얼마나 유사한지를 측정하는 지표로 쿨백-라이블러 발산(Kullback-Leibler Divergence, KLD)이 있다. TRPO에서는 정책 갱신 전후의 쿨백-라이블러 발산을 지표로 삼아, 값이 임계값을 넘지 않아야 한다는 제약을 부과한다. 즉, 쿨백-라이블러 발산 제약이 걸린 상태에서 목적 함수를 최적화하는 문제로 보는 것이다. 이 제약 덕분에 적절한 갱신 폭을 구할 수 있다.

>TRPO의 중요한 점은 기울기 갱신 폭을 적절하게 관리하기 위해, 한 번의 갱신으로 정책이 너무 크게 변하지 않도록 제약을 부과한다는 것이다. 이 제약 안에서 학습을 진행하는 것이다. 이후는 제약이 걸린 최적화 문제이며, 수학적으로 해결할 수 있다. 

TRPO에서 제약이 걸린 최적화 문제를 풀려면 헤세 행렬이라는 이차 미분 계산이 필요하다. 헤세 행렬은 계산량이 많이 병목을 일으킨다. 이 문제를 개선한 방법이 PPO이다. PPO는 TRPO를 단순화한 기법으로, 계산량을 줄이면서도 성능은 TRPO와 비슷해 실무에서 많이 활용한다.




