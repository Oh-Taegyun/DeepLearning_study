앞의 정책 경사법을 개선한 기법이다. 가장 간단한 정책 경사법은 다음과 같다. 

![[Pasted image 20240628145749.png]]
 
G(τ)는 지금까지 얻은 모든 보상의 총합이다. (정확히는 '할인율을 적용한' 보상의 총합) 여기서 생각해볼 문제가 있다. "G(τ) ᐁ log π<sub>θ</sub>(A<sub>t</sub> | S<sub>t</sub>)" 부분을 보면, 특징 시간 t 에서 행동 A<sub>t</sub>를 선택할 확률에 "항상 일정한" 가중치 G(τ)를 적용하고 있다. 

그런데 좋은 행동인지 나쁜 행동인지는 그 행동 '이후에' 얻는 보상의 총합으로 평가된다. (가치 함수의 정의를 생각해보자) 행동 '전에' 얻은 보상은 그 행동의 좋고 나쁨과 무관하다. 

예를 들어 특정 시각 t에 취한 행동 A<sub>t</sub>를 평가할 때는 그 이전에 무엇을 했고 몇 보상을 얼마나 얻었는지는 중요하지 않는다. 행동 A<sub>t</sub>를 하고 난 후 어떤 결과가 나오느냐에 따라, 즉 시각 t 이후에 얻는 보상의 총합에 따라 행동 A<sub>t</sub>의 좋고 나쁨이 결정된다.

![[Pasted image 20240628144631.png]]

위 식에서 행동 A<sub>t</sub>에 대한 가중치는 G(τ)이다. 이 가중치에는 시각 t 이전의 보상도 포함된다. 본질적으로 관련이 없는 보상이 노이즈로 섞여 있다는 뜻이다. 이 노이즈를 제거하기 위해 가중치 G(τ)를 다음과 같이 변경할 수 있다.

![[Pasted image 20240630011500.png]]

이와 같이 가중치를 G<sub>t</sub>로 변경했다. 가중치 G<sub>t</sub>는 시각 t ~ T 동안에 얻는 보상의 총합이다. 이제 시각 t 앞의 보상은 포함하지 않는 가중치 G<sub>t</sub>를 써서 행동 A<sub>t</sub>가 선택될 확률을 강화할 수 있다. 이것이 앞의 절의 정책 경사법을 개헌하는 아이디어이다.

위 식에 기반한 알고리즘을 REINFORCE라고 한다.

### 2. REINFORCE 구현
---
REINFORCE는 분산이 작기 때문에 데이터 샘플이 적더라도 더 정확하게 근사할 수 있다. 

``` python
class Agent:
    def __init__(self):
        self.gamma = 0.98
        self.lr = 0.0002
        self.action_size = 2
		
        self.memory = []
        self.pi = Policy(self.action_size)
        self.optimizer = optim.Adam(self.pi.parameters(), lr=self.lr)
		
    def get_action(self, state):
        state = torch.tensor(state[np.newaxis, :])
        probs = self.pi(state)
        probs = probs[0]
        m = Categorical(probs)
        action = m.sample().item()
        return action, probs[action]
		
    def add(self, reward, prob):
        data = (reward, prob)
        self.memory.append(data)
		
    def update(self):
        G, loss = 0, 0
        for reward, prob in reversed(self.memory):
            G = reward + self.gamma * G
            loss += - torch.log(prob) * G
			
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        self.memory = []
```














