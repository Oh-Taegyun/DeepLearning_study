행동 가치 함수는 상태 가치 함수와 같이 강화 학습 이론에 등장하는 중요한 함수이다. 지금까지는 환경 상태 가치 함수를 이용해서 벨만 방정식을 도출했지만, 행동 또한 결정적과 확률적인 방법이 있다. 행동 가치 함수를 구해보자

상태 가치 함수는 다음과 같다.

![[Pasted image 20240302023224.png|300]]

상태 가치 함수의 조건은 2가지이다.
1. 상태가 s 일것
2. 행동 정책이 π일것

이때 조건을 하나 더 추가할 수 있다. 바로 행동에 관한 조건이다. 이를 포함하면

![[Pasted image 20240518202242.png]]
이게 바로 행동 가치 함수이다. 

> Q함수는 시간이 t일때 상태 s에서 행동 a를 취하고, t+1부터는 정책 π에 따라 행동을 결정한다. 이때 얻을 수 있는 기대 수익이 ![[Pasted image 20240518202339.png|70]]이다

주의할 점은 ![[Pasted image 20240518202339.png|70]]의 행동 a는 정책 π와 무관하다는 점이다. ![[Pasted image 20240518202339.png|70]]의 행동 a는 자유롭게 결정할 수 있으며, 다음 행동부터 정책 π를 따른다. 뭔소리냐면, 원래 행동 a는 정책에 따른다. 결정적이든 확률적이든 말이다. 하지만, 행동 가치 함수에서의 a는 이러한 정책과 무관한 a란 것이다. 

![[Pasted image 20240518203252.png]]

위 그림이 꽤나 중요하다. 상태 가치 함수는 현재 상태(s)에서 다음 상태(s')을 결정할때 어떤 보상을 얻게 되는지에 대한 기댓값으로, 모든 행동에 대해 고려한다면, Q함수는 1가지의 행동에 대해서만 고려하기 때문이다. 

상태 가치 함수에서 행동(a)는 반드시 정책 π에 따라 선택된다. Q함수에서 행동 a는 자유롭게 선택할 수 있다. 이러한 점이 크게 차이가 난다.

>느낌이 오는가? 상태 가치 함수가 지금 상태에서 모든 행동을 하면 얻을 수익은 어떻게 될까요? 라면, Q함수는 지금 상태에서 딱 1가지의 행동을 하게 된다면 어떤 수익을 얻을 수 있을까요? 이다.

만약에 Q함수의 행동 a를 정책 π에 따라 선택할 수 있게 설계한다면 Q함수와 상태 가치 함수는 완젼히 동일할 것이다. (에초에 Q함수는 함수가 실행되면 자유롭게 행동 a를 선택 후에 다음 정책 π에 따라 원래방식대로 행동을 선택하는 형식인데.... 이 첫 행동 a조차 π를 따라간다면 그냥 처음과 다를게 없다.)

수식으로도 증명할 수 있다. 

행동 후보가 {a1, a2, a3} 세 가지 있다고 가정하고 정책 π에 따라 행동한다고 하면 다음과 같이 나타낼 수 있다. 

![[Pasted image 20240518203905.png]]

이 경우 기대 수익은 Q함수의 가중 합으로 볼 수 있다. 

![[Pasted image 20240518203929.png]]

이 식은 다시 상태 가치 함수와 같아진다. 그렇다면 상태 가치 함수를 복잡한 식 말고

![[Pasted image 20240518220930.png]]

이렇게 바꿀 수 있다.

### 1. 행동 가치 함수를 이용한 벨만 방정식
---
![[Pasted image 20240518204147.png]]

이전에 수익을 다음식으로 나타냈었다. 

![[Pasted image 20240518143724.png|300]]

상태(s)와 행동(a)는 이미 결정되어 있다. 이제 위 식을 전개해본다면 

![[Pasted image 20240518204720.png]]
으로도 나타낼 수 있다. 

![[Pasted image 20240518204858.png]]

a'는 시간 t+1에서의 행동이다. 바로 위 식이 행동 가치 함수를 이용한 벨만 방정식이다.


### 2. 행동 가치와 행동 가치 함수
---
이전에 이런 것을 본 적이 있을 것이다. 

![[Pasted image 20240626103033.png]]

그럼 행동 가치와 행동 가치 함수는 뭐가 다른걸까? 

행동 가치와 행동 가치 함수의 차이는 다음과 같다

1. **행동 가치 (Action Value)**:
   - 행동 가치는 특정 상태에서 특정 행동을 취했을 때 예상되는 미래 보상의 총합을 나타내는 값입.
   - 이 값은 그 상태에서 그 행동을 취했을 때 얻을 수 있는 보상과 그 후의 상태들에서 얻을 수 있는 보상들의 기대값을 포함한다.
   - 간단히 말해, 행동 가치는 숫자 하나로, 상태 \(s\)에서 행동 \(a\)를 취했을 때의 예상 보상을 수치화한 것이다.

2. **행동 가치 함수 (Action Value Function)**:
   - 행동 가치 함수는 모든 가능한 상태와 행동 쌍에 대해 행동 가치를 계산하는 함수이다.
   - 이 함수는 보통 ( Q(s, a) )로 표현되며, 상태 ( s )와 행동 ( a )를 입력으로 받아, 해당 상태-행동 쌍의 행동 가치를 출력한다.
   - 행동 가치 함수는 특정 정책 또는 최적의 행동을 선택하기 위한 기준으로 사용된다. 이 함수는 학습 과정에서 계속해서 업데이트되며, 환경에 대한 이해를 높여 가는 데 중요한 역할을 한다.

즉, 행동 가치는 단일 상태-행동 쌍에 대한 예상 보상 값이고, 행동 가치 함수는 모든 상태-행동 쌍에 대해 이 값을 계산하는 함수이다.
