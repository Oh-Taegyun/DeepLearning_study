![[Pasted image 20240519164115.png|400]]

이 문제에서 환경 모델을 사용하지 않고 정책을 평가해보자

``` python
class RandomAgent: # 에이전트!
    def __init__(self):
        self.gamma = 0.9 # 할인율
        self.action_size = 4 # 행동할 수 있는 횟수

        random_actions = {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25} # 초기 랜덤 정책
        self.pi = defaultdict(lambda: random_actions)
        self.V = defaultdict(lambda: 0) # 가치 함수를 담는다
        self.cnts = defaultdict(lambda: 0) # 증분 방식으로 수익의 평균 구할때 씀
        self.memory = [] # 에이전트가 실재로 행동해서 얻은 경험('상태, 행동, 보상')을 담는 역할이다. 

    def get_action(self, state):
        action_probs = self.pi[state]
        actions = list(action_probs.keys())
        probs = list(action_probs.values())
        return np.random.choice(actions, p=probs) # 확률 분포에 따라 행동을 한 개씩 샘플링

    def add(self, state, action, reward): # 수행한 행동과 보상을 기록해준다. 
        data = (state, action, reward)
        # 튜플로 묶는 이유는 [(S0, A0, R0), (S1, A1, R1), ... (S8, A8, R8)] 으로 묶기 위함이다. 이때 맨 마지막 상태는 저장이 안되는데 마지막 상태는 목표 지점에 도달한 상태로 어차피 상태 가치 함수가 0이기 때문이다. 
        self.memory.append(data)

    def reset(self):
        self.memory.clear()

    def eval(self): # 몬테카를로법을 수행한다 
        G = 0 # 수익 0으로 초기화 
        for data in reversed(self.memory):  # 역방향으로(reserved) 따라가기
            state, action, reward = data
            G = self.gamma * G + reward
            self.cnts[state] += 1
            self.V[state] += (G - self.V[state]) / self.cnts[state]
```

`eval()`은 이 공식을 따른 것이다.

![[Pasted image 20240617221144.png]]

``` python
env = GridWorld()
agent = RandomAgent()

episodes = 1000
for episode in range(episodes):  # 에피소드 1000번 수행
    state = env.reset()
    agent.reset()

    while True:
        action = agent.get_action(state)             # 행동 선택
        next_state, reward, done = env.step(action)  # 행동 수행

        agent.add(state, action, reward)  # (상태, 행동, 보상) 저장
        if done:   # 목표에 도달 시
            agent.eval()  # 몬테카를로법으로 가치 함수 갱신
            break         # 다음 에피소드 시작

        state = next_state

# [그림 5-12] 몬테카를로법으로 얻은 가치 함수
env.render_v(agent.V)
```

이렇게 몬테카를로법으로 얻은 가치 함수는 경험을 미리 제공하는 동적 프로그래밍으로 얻은 가치 함수와 별 차이가 없다. 

물론 지금까지 한 것은 초기 행동 정책 "{0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}"으로만 가치 함수를 평가했기 때문에, 다음과 같은 결과가 나올 것이다.

![[Pasted image 20240617221822.png]]

그럼 이제 정책을 갱신하면서 달려가보자

