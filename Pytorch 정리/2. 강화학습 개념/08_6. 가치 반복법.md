정책 반복법은 '평가와 '개선'이라는 두 과정을 번갈아 반복한다.

평가 단계에서는 '정책'을 평가하여 가치 함수를 얻는다. '개선'단계에서는 가치 함수를 탐욕화하여 개선된 정책을 얻는다. 이를 번갈아서 반복하면 최적 정책과 최적 가치 함수에 점점 가까워진다. 

![[Pasted image 20240525221022.png]]

간단하게 2차원으로 나타내면 위 그림과 같다. 

위쪽 직선은 V = v<sub>μ</sub> 을 나타내면서, 임의의 가치 함수 V와 정책 μ의 실제 가치 함수 v<sub>μ</sub>가 일치하는 곳이다. 아래쪽 직선은 가치 함수 V를 탐욕화하여 얻은 정책 μ가 일치하는 곳이다. 

1. 평가 단계 - 정책 μ을 평가하여 V<sub>μ</sub> 를 얻는다.
2. 개선 단계 - V를 탐욕화 

하지만 목표에 도달하기까지 꼭 위 그림을 따라야 하는것은 아니다, 막말로

![[Pasted image 20240525221348.png|500]]

이렇게라도 도달할 수 있으면 상관이 없지 않겠는가?

평가를 끝내기 전에 개선을 하고 개선이 끝나기 전에 평가를 끝내는 방법으로 도달하는 그림이다. 이러한 반복을 "일반화한 정책 반복"이라고 한다. 

정책 평가와 정책 개선의 두 과정을 교대로 반복하는 알고리즘에서 평가와 개선의 '정밀도', 즉 얼마나 정확하게 평가(혹은 개선)할 것인가는 자유롭게 조절할 수 있다. 예를 들어 '평가' 단계에서는 실제 가치와 일치하게 갱신되지 않더라도 V가 v<sub>μ</sub>의 방향으로 가까워지기만 하면 된다. 마찬가지로 '개선'단계에서는 탐욕화되는 방향으로 향하기만 하면(일부만 탐욕화) 되면 된다.

이전에 만든 정책 반복법은 평가와 개선을 최대한 하고 번갈아면서 수행하고 있다. 그럼 평가와 개선을 최소한으로 수행하면 어떻게 될까?

이 아이디어가 바로 가치 반복법이다.

### 1. 가치 반복법 아이디어어
---
![[Pasted image 20240525222533.png]]

위와 같이 모든 상태의 가치 함수를 여러 번 갱신했었다. 이 갱신 작업이 수렴된다면 개선 단계로 넘어가고 다시 평가하고 그랬다. 

근데, 이렇게 모든 상태를 동시에 개선하는 것보다. 상태를 한번씩만 개선하고 평가하고 그런다면 어떨까?

하나의 상태만 딱 한 번 '갱신'하고 곧바로 '개선'단계로 넘어갈 수 있다면 가치 반복법의 핵심 아이디어 일 것이다.  

![[Pasted image 20240525222735.png|500]]

이런식으로 말이다. 

### 2. 가치 반복법 수식
---
![[Pasted image 20240525223509.png]]

그런데 자세히 보니까 식

![[Pasted image 20240526001024.png]]

이 중복된다. 개선 단계에서 탐욕 행동을 찾기 위해 계산을 한 후, 평가 단계에서 그 탐욕 행동을 이용하여 똑같은 계산을 다시 수행한다.

따라서 이 계산들을 묶는다면,

![[Pasted image 20240526001227.png]]
일 것이다. 

한번 더 생각해 봐야 할 것은 정책이 없다. 다시 말해 행동 정책 μ이 없다. 정책 없이 가치 함수를 갱신하고 있어서 이러한 알고리즘을 '가치 반복법' 이라고 부른다. 가치 반복법은 위 식 하나만 이용해서 '평가'와 '개선' 둘 다 수행한다.

![[Pasted image 20240526001541.png]]

근데 위 식이 벨만 최적 방정식과 아예 똑같은 형태를 취하고 있다. 즉, 벨만 최적 방정식을 갱신식으로 바꾸면 바로 가치 반복법이 된다는 것이다.

즉 얘만 가지고 갱신해주면 알아서 평가하고 개선하고를 지 혼자 다한단 것이다.  

가치 반복법 또한 DP테이블을 이용한 DP알고리즘이다.

![[Pasted image 20240526001938.png]]

자기가 알아서 
1. 현 정책에 대한 상태 가치 함수 추출
2. 현 상태 가치 함수중 최적의 정책 추출
3. 반복

을 하겠다는 거다. 최종적으로는 최적 상태 가치 함수가 나온다. 그리고 그 최적 상태 가치 함수에서 최적의 정책만 추출하면 그게 바로 우리가 목표하던 정책이 아니겠는가??
