![[Pasted image 20240519164115.png|400]]

• 에이전트는 상하좌우 네 방향으로 이동할 수 있다.
• `[그림 4-8]`에서 회색 칸은 벽을 뜻하며 벽 안으로는 들어갈 수 없다.
• 그리드 바깥도 벽으로 둘러싸여 더 이상 나아갈 수 없다.
• 벽에 부딪히면 보상은 0이다.
• 사과는 보상 +1, 폭탄은 보상 -1, 그 외의 보상은 0이다.
• 환경의 상태 전이는 고유하다(결정적). 즉, 에이전트가 오른쪽으로 이동하는 행동을 선택하면 (벽만 없다면) 반드시 오른쪽으로 이동한다.
• 이번 문제는 일회성 과제로서, 사과를 얻으면 종료한다.

### 1. GridWorld 클래스 구현
---
``` python
import numpy as np
import common.gridworld_render as render_helper


class GridWorld:
    def __init__(self):
        self.action_space = [0, 1, 2, 3]  # 행동 공간(가능한 행동들)
        self.action_meaning = {  # 행동의 의미
            0: "UP",
            1: "DOWN",
            2: "LEFT",
            3: "RIGHT",
        }

        self.reward_map = np.array(  # 보상 맵(각 좌표의 보상 값)
            [[0, 0, 0, 1.0],
             [0, None, 0, -1.0],
             [0, 0, 0, 0]]
        )
        self.goal_state = (0, 3)    # 목표 상태(좌표)
        self.wall_state = (1, 1)    # 벽 상태(좌표)
        self.start_state = (2, 0)   # 시작 상태(좌표)
        self.agent_state = self.start_state   # 에이전트 초기 상태(좌표)
        # 상태는 좌표값으로 한다. 

    @property
    def height(self): # 맵 행 반환
        return len(self.reward_map)

    @property
    def width(self): # 맵 열 반환
        return len(self.reward_map[0])

    @property
    def shape(self): # 맵 형상 반환
        return self.reward_map.shape

    def actions(self): # 가능한 행동 
        return self.action_space

    def states(self): # 상태 반환 
        for h in range(self.height):
            for w in range(self.width):
                yield (h, w) 
                # yield는 제너레이터를 만든다. 

    def next_state(self, state, action):
        # 이동 위치 계산
        action_move_map = [(-1, 0), (1, 0), (0, -1), (0, 1)] # 행동 맵
        move = action_move_map[action]
        next_state = (state[0] + move[0], state[1] + move[1])
        ny, nx = next_state

        # 이동한 위치가 그리드 월드의 테두리 밖이나 벽인가?
        if nx < 0 or nx >= self.width or ny < 0 or ny >= self.height:
            next_state = state
        elif next_state == self.wall_state:
            next_state = state

        return next_state  # 다음 상태 반환

    def reward(self, state, action, next_state):
        return self.reward_map[next_state]

    def reset(self):
        self.agent_state = self.start_state
        return self.agent_state

    def step(self, action): # 에이전트에게 행동을 시켜본다. 
        state = self.agent_state 
        next_state = self.next_state(state, action) 
        reward = self.reward(state, action, next_state) 
        done = (next_state == self.goal_state) # 만약 목표에 도달했다면 True

        self.agent_state = next_state
        return next_state, reward, done
        
	# 시각화용 메서드
    def render_v(self, v=None, policy=None, print_value=True): 
        renderer = render_helper.Renderer(self.reward_map, self.goal_state,
                                          self.wall_state)
        renderer.render_v(v, policy, print_value)

    def render_q(self, q=None, print_value=True):
        renderer = render_helper.Renderer(self.reward_map, self.goal_state,
                                          self.wall_state)
        renderer.render_q(q, print_value)

```

##### 1-1. defaultdict 사용법
---
``` python
from common.gridworld import GridWorld

env = GridWorld()
V = {}

for state in env.states():
    V[state] = 0 # 모든 좌표의 상태 가치는 0으로 초기화한다. 

state = (1, 2)
print(V[state]) 
```

딕셔너리는 key값이 있어야 정상 동작한다. 위 경우에는 계속 딕셔너리에 키와 값을 추가해주고 있다. 이 초기화의 번거로움을 줄여주는 기능이 파이썬 표준 라이브러리에 있는데 그게 바로 defaultdict이다. 

``` python
from collections import defaultdict
from common.gridworld import Gridword

env = GridWorld()
V = defaultdict(lambda: 0) # 이렇게 생성된 V는 일반 딕셔너리처럼 사용할 수 있다. 만약 딕셔너리에 존재하지 않는 키를 건네면 (주어진 키, 기본값) 형태의 원소를 새로 만들어 넣는다. 

state = (1,2) 
print(V[state]) # 일단 (1,2)의 키에 해당하는 값이 없으므로 람다함수로 정의한 0이 출력된다. 그 후 (1,2)에 대한 새로운 원소를 만들어 넣는다. 
```

##### 1-2. 행동 정책 함수
---
무작위 정책 π을 구현해보자

그리드 월드 문제에서는 에이전트가 취할 수 있는 행동은 네개이다. 각 행동이 균일하게 무작위로 선택된다면 각 행동이 수행될 확률은 모두 0.25이다.  따라서

``` python
pi = defaultdict(lamda: {0: 0.25, 1: 0.25, 2: 0.25, 3:0.25})
# 존재하지 않은 키에 접근하면 {0: 0.25, 1: 0.25, 2: 0.25, 3:0.25}라는 딕셔너리를 반환한다. 

state = (0, 1)
print(pi[state]) # [출력 결과] {0: 0.25, 1: 0.25, 2: 0.25, 3:0.25}
```

상태 (0, 1) 에서의 행동 확률 분포를 출력했다. 모든 행동이 0.25 확률로 선택되는 분포이다. 


### 2. 반복적 정책 평가 구현
---
갱신을 한 단계만 수행하는 함수를 구현해보자. eval_onestep( )함수는 다음과 같이 네 개의 매개변수를 받는다. 

1. pi(defaultdict) : 정책
2. V(defaultdict) : 가치 함수
3. env(GridWorld) : 환경
4. gamma(float) : 할인율

![[Pasted image 20240617214823.png]]

``` python
def eval_onestep(pi, V, env, gamma=0.9):
    for state in env.states():  # 각 상태에 접근
        if state == env.goal_state:  # 목표 상태에서의 가치 함수는 항상 0 당연한거다. 에피소드가 끝나고 그 다음 전개 즉, 미래에 대한 기대값이 아예 없기 때문이다. 
            V[state] = 0
            continue

        action_probs = pi[state] #probs는 probabilities(확률)의 약자 
        new_V = 0
        
        # 각 행동에 접근        
        for action, action_prob in action_probs.items():
            next_state = env.next_state(state, action)
            r = env.reward(state, action, next_state)
            # 새로운 가치 함수
            new_V += action_prob * (r + gamma * V[next_state])

        V[state] = new_V
    return V
```

![[Pasted image 20240519173212.png|300]]

이와 같이 모든 상태에 순차적으로 접근한다. 만약 목표 상태라면 가치 함수를 0으로 설정한다. 에이전트가 목표 지점에 도달하면, 에피소드가 시작하자마자 끝나기 때문이다. 목표 상태에서의 가치 함수 값은 항상 0이다. 

>당연하다 상태 가치 함수의 대 전제는 미래 수익의 기댓값이다. 

이제 이건 1번만 갱신 된 것이다. 이 갱신을 계속 반복해야 모든 상태에 대해서 가치 함수를 판단할 수 있다. 

``` python
def policy_eval(pi, V, env, gamma, threshold=0.001):
    while True:
        old_V = V.copy()  # 갱신 전 가치 함수
        V = eval_onestep(pi, V, env, gamma)

        # 갱신된 양의 최댓값 계산
        delta = 0
        for state in V.keys():
            t = abs(V[state] - old_V[state])
            if delta < t:
                delta = t

        # 임계값과 비교
        if delta < threshold:
            break
    return V
```

이제 구현한 GridWorld 클래스와 polict_eval() 함수를 사용하여 정책 평가를 해보자

``` python
env = GridWorld()
gamma = 0.9  # 할인율

pi = defaultdict(lambda: {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25})  # 정책
V = defaultdict(lambda: 0)  # 가치 함수
V = policy_eval(pi, V, env, gamma)  # 정책 평가

# [그림 4-13] 무작위 정책의 가치 함수
env.render_v(V, pi)
```

![[Pasted image 20240519164115.png|400]]

![[Pasted image 20240519184947.png]]

이제 무작위 정책의 가치 함수를 보여준다. 

시작점인 왼쪽 맨 아래 칸의 가치 함수는 -0.10이다. 즉, 지금 정책으로는 기대할 수 있는 수익이 -0.10이란 소리이다. 사과(+1)보단 폭탄(-1)을 밟을 확률이 더 크단 것이다. 
