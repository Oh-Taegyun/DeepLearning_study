![[Pasted image 20240630215921.png]]

### 1. 범주형 DQN
---
Q함수의 수식은 다음과 같다.

![[Pasted image 20240630220007.png|300]]

그림과 같이 확률적 사건인 수익 G<sub>t</sub>를 기댓값이라는 하나의 값으로 표현하는 것이 Q함수의 특징이다.

![[Pasted image 20240630220026.png]]

DQN에서는 Q함수 즉 기댓값이라고 표현되는 값을 학습한다. 이를 발전시켜 Q함수라는 기댓값이 아니라 '분포'를 학습시키자는 아이디어가 있다. 이 아이디어를 분포 강화 학습이라고 한다. 확률 분포인 Z<sub>π</sub>(s, a)를 학습한다.

범주형 DQN은 바로 이 분포 강화 학습을 기반으로 한다. 여기서 범주형이란 아래 그림과 같이 범주형 분포로 모델링한다는 뜻이다.

![[Pasted image 20240630220529.png|400]]

범주형 분포는 여러 범주(이산 값) 중 어느 범주에 속할 것인지에 대한 확률 분포이다. 위 그림과 같이 수익이 취하는 값이 몇 개의 영역(빈<sup>bin</sup>)으로 나뉘고, 각 빈에 들어갈 확률이 범주형 분포로 모델링된다.

범주형 DQN에서는 수익을 범주형 분포로 모델링하고 그 '분포의 형태'를 학습한다. 이를 위해 범주형 분포 버전의 벨만 방정식을 도출하고 그 방정식을 이용해 범주형 분포를 갱신한다. 


### 2. Noisy Network
---
DQN에서는 ε-탐욕 정책으로 행동을 결정한다. ε의 확률로 무작위 행동을 선택하고, 나머지 1 - ε의 확률로 탐욕 행동(Q 함수가 가장 큰 행동)을 선택한다. 실전에서는 대체로 에피소드가 진행될수록 ε값을 조금씩 낮추도록 스케줄링한다. 여기서 문제는 ε값인데, ε은 하이퍼파라미터라서 어떻게 설정하느냐에 따라 최종 정확도가 크게 달라질 수 있다. 하지만 ε값의 후보는 매우 다양하다.

이러한 ε 설정 문제를 해결하기 위해 Noisy Network가 제안되었다. Noisy Network는 신경망에 무작위성을 도입한다. 그 덕분에 행동을 (ε-탐욕 정책이 아닌) 탐욕 정책에 따라 선택할 수 있다. 정확하게는 출력 쪽의 완전 연결 계층에서 '노이즈가 들어간 완전 연결 계층'을 사용한다. '노이즈가 들어간 완전 연결 계층'에서 가중치는 정규분포의 평균과 분산으로 모델링되며 (순전파할 때마다) 가중치가 정규분포에서 샘플링된다. 이렇게 하면 순전파할 때마다 무작위성이 스며들어 최종 출력이 달라진다.


### 3. 레인보우
---
지금까지 다양한 DQN 확장 알고리즘이 있었다. 그걸 다 결합한게 레인보우이다. 

1. Double DQN
2. 우선 순위 경험 재생
3. Dueling DQN
4. 범주형 DQN
5. Noisy Network

이 모든 기법을 다 결합한 것이다.


### 4. 레인보우 이후의 발전된 알고리즘
---
레인보우 이후 CPU/GPU를 이용한 여러 분산 병렬 학습이 큰 성과를 올렸다. 이를 분산 강화 학습이라고도 하며, 실행 환경을 여러 개 준비하여 학습을 병렬로 진행한다. 분산 강화 학습으로 유명한 기법이 Ape-X이다. Ape-X는 레인보우를 기반으로 여러 개의 에이전트를 각각의 CPU에서 독립적으로 행동시킨다. 이때 에이전트들의 탐색 비중인 ε을 모두 다르게 설정하여 다양한 경험 데이터를 수집한다. 이처럼 분산 병렬화를 통해 학습을 빠르게 진행하는 동시에 경험 데이터를 다양하게 얻어 성능을 높였다.

R2D2는 Ape-X를 더욱 개선한 기법이다. R2D2는 Ape-X에 더해 시계열 데이터를 처리하는 순환 신경망(RNN)을 사용했습니다(정확하게는 LSTM 사용). 간단한 아이디어지만 RNN으로 학습하기 위해 많은 노력을 기울여 Ape-X의 성능을 한층 높이는 데 성공했다. 참고로 R2D2의 이름은 Recurrent(순환)와 Replay(경험 재생)에서 'R' 두 개를 가져오고, Distributed(분산)와 Deep Q-Network(DQN)에서 'D' 두 개를 가져와서 만든 이름이다

다음은 R2D2를 더욱 발전시킨 NGU이다. NGU는 'Never Give Up(절대 포기하지 마)'의 약자입니다. NGU는 R2D2의 토대에 내적 보상(intrinsic reward) 메커니즘을 추가하여 어려운 과제, 특히 보상이 적은 과제에서도 탐색을 포기하지 않도록 했다. 알고리즘 이름은 이러한 특성에서 유래했다. 내적 보상은 상태 전이가 예상과 다를수록, 즉 얼마나 '놀랐는가'에 따라 스스로 보상을 더해주는 기법이다. 보상이 0에 가까운 희박한 작업에서 내적 보상은 (보상의 크기가 아니라) '호기심'에 따라 행동하도록 유도한다. 이 과정에서 (잘만 하면) 보상을 극대화하는 방법을 찾을 수 있다.

>아이들 특히 어린아이는 새롭고 놀라운 경험을 찾아 놀기를 좋아한다. 확실한 목적을 가지고 학습하기보단 호기심이 이끄는 대로 행동한다. 내적 보상의 목표는 바로 이처럼 호기심을 쫒는 행동을 에이전트에 주입하는 것이다.

마지막으로 만나볼 기법은 Agent57이다. NGU를 발전시킨 기법이다. 중요한 특성은 내적 보상 메커니즘을 개선하고 '메타 컨트롤러'라는 구조를 사용하여 에이전트들에 할당되는 정책을 유연하게 배분했다는 점이다. 아타리, 정확하게는 '아타리 2600'에는 게임이 모두 '57'개가 있다. Agent57은 이 모든 게임에서 사람보다 우수한 성적을 거두는 데 성공했다. 강화 학습 알고리즘으로서는 처음 이룬 쾌거였다.




