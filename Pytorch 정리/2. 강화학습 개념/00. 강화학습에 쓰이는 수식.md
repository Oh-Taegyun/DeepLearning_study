강화학습에 사용되는 언어나 수식을 정리해둔 공간

### 1. 행동 (Action)
---
에이전트가 수행하는 행동. `A`로 표기한다.

어떤 에이전트가 행동을 a, b만 취할 수 있다면 A는 {a, b} 중 하나의 값을 취하게 된다.

### 2. 보상 (Reward)
---
에이전트가 행동을 함으로써 얻게 되는 보상값. `R`로 표기한다.
t 번째에 얻는 보삼임을 나타낼때는 R_t로 쓴다. 

### 3. 기댓값 (Expectation)
---
확률 변수로 정의되는 기댓값, 기호로는 `E`로 표기한다.
1. 보상 R의 기댓값 : `E[R]`
2. 행동 A를 선택했을 때의 보상 기댓값 : `E[R|A]`
	이때 `|`는 조건부 확률(조건부 기댓값)을 뜻한다. 

### 4. 행동 가치 (Quality)
---
보상에 대한 기댓값을 행동 가치라고 한다.
1. `q(A) = E[R|A]` : 행동 A의 행동 가치
2. 대문자(`Q(A)`)의 경우 추정치를 의미한다.
3. 소문자(`q(A)`)의 경우 실제 행동 가치(실제 휙득한 보상의 평균)를 의미한다.

### 5. 가치 추정치
---
가치 추정치 : 실제 획득한 보상의 평균값

![[Pasted image 20240301230949 1.png|400]]

기댓값을 각각 나타내보면 
1. 슬롯머신 a의 행동 가치 - Q(a) = 2
2. 슬롯머신 b의 행동 가치 - Q(b) = 0.333

이렇게 3번만 확인해봤을때 현재 상태로는 a가 좋은 슬롯머신이니까 a를 계속 돌리는 것이 타당하다.

이렇게 슬롯머신을 실제로 플레이하여 얻은 보상은 어떤 확률 분포에서 생성된 샘플(표본)이다. 따라서 실제 휙득한 보상의 평균을 **`표본 평균`** 이라고 할 수 있다. **`표본 평균`** 은 샘플링 횟수가 늘어날 수록 실제값(보상의 기댓값)에 가까워진다.

그럼 이러한 과정을 수식으로 써보자

![[Pasted image 20240301231408 1.png|300]]

근데 이 수식은 컴퓨터 계산상 그렇게 좋진 않다. 왜냐면 플레이 횟수(n)이 늘어날 수록 보상(R)의 원소 수가 늘어난다. 따라서 `sum(rewards)`코드의 실행 비용도 함께 증가한다. 즉, 플레이 횟수가 늘어날수록 메모리와 계산량이 너무 늘어나는게 문제

따라서 다음처럼 수식을 변경하겠다. 

![[Pasted image 20240301232209 1.png]]

여기서 중요한 것은 ![[Pasted image 20240301232310 1.png|170]]이다.

`Q_n`은 `Q_(n-1)`에 어떤 값을 더해 구할 수 있다. 이를 그림으로 그리면 다음과 같다. 

![[Pasted image 20240301232824 1.png]]

이전의 행동 가치`(Q_(n-1))`는 `R_n`방향으로 얼마만큼의 `1/n`을 곱했냐에 따라서 갱신되는 양이 다르다. 즉, 이러한 `1/n`은 갱신되는 양을 조절하기 때문에 `학습률`과 같은 역할을 한다.

``` python
Q += (reward - Q) / n
```

이렇게 Q1, Q2, Q3, ... 식으로 하나씩 순서대로 증가시키며 구할 수 있다는 뜻에서 이런 구현 방식을 증분 구현이라고 한다.


### 6. 비정상 문제를 풀기 위해서...
---
앞에서는 행동 가치를 다음과 같이 설정했다.

![[Pasted image 20240302002325 1.png|550]]

이는 다음과 같은 그래프로 그릴 수 있다. 이떄 `1/n`은 각 보상에 대한 가중치로 볼 수 있다. 

![[Pasted image 20240302002830 1.png]]

이는 모든 보상에 대해서 똑같은 가중치가 부여된다는 것이다. 새로 얻은 보상이든 오래전에 얻은 보상이든 모두 동등하게 취급된다는 뜻이다.

하지만 비정상 문제에서는 시간이 흐르면 환경(슬롯머신)이 변하기 때문에 과거 데이터(보상)의 중요도는 점점 낮아져야 한다. 반대로 새로 얻은 보상의 가중치는 점점 커져야 한다. 

따라서 위 식을 다음처럼 바꿔보자

![[Pasted image 20240302005456 1.png|300]]

가중치 `1/n`을 고정값 α(0<α<1)로 바꾼다면 다음처럼 그림이 그려진다.

![[Pasted image 20240302005633 1.png]]

이렇게 고정값 α(0<α<1)로 바꾼다면 오래전에 받은 보상일수록 기하급수적으로 낮아지는 것을 확인할 수 있다.

이처럼 바꾼 식은 지수적으로 감소하기 때문에 
![[Pasted image 20240302005919 1.png|550]]
이 식을 지수 이동 평균, 지수 가중 이동 평균이라고 한다. 

### 7. MDP 이론
---
![[Pasted image 20240615190841.png]]

### 7-1. 에이전트와 환경의 상호작용
---
![[Pasted image 20240302012047.png|300]]

상태 S_t에서 시작해서 에이전트가 행동 A_t를 수행하여 보상 R_t를 얻고 다음 상태인 S_(t+1)로 전환된다. 이러한 에이전트와 환경의 상호작용은 실제로 다음과 같은 정리를 만들어낸다.

![[Pasted image 20240302012250.png|350]]

어찌보면 이것도 시간에 따라서 진행되는 것이므로 엄연히 `시계열 데이터`이다. 


### 7-2. 환경 상태 전이
---
환경 상태는 어떻게 전이되는가? 에 대해서 나타내는 수식이다.

2가지가 있다.
1. 절대적 상태 전이 함수
2. 확률적 상태 전이 함수

![[Pasted image 20240302012924.png]]

왼쪽 그림은 에이전트가 왼쪽으로 이동하려는 행동을 선택했다. 그 결과 에이전트는 반드시 왼쪽으로만 이동했다. 이러한 성질을 결정적이라고 한다.

다음 상태 `s'`는 현재 상태 `s`와 행동 `a`에 의해 '단 하나로' 결정된다. 따라서 함수로는 다음처럼 표현할 수 있다. 

![[Pasted image 20240302013134.png|200]]
`Next_State = f(State, Action)`

이러한 함수를 `상태 전이 함수`라고 한다. 

반면 오른쪽 그림은 확률적으로 이동하고 있다. 에이전트가 왼쪽으로 이동하려는 행동을 선택하더라도 0.9 확률로만 왼쪽으로 이동하고, 0.1의 확률로는 그 자리에 머물러 있는 것을 의미한다. 상태 전이가 결정적이여도 확률적으로 표현할 수 있다. 1.0의 확률로 왼쪽으로 이동한다고 기술하면 된다.

확률적 상태 전이를 표기하는 방법은 다음과 같이 서술할 수 있다.

![[Pasted image 20240302013417.png|150]]

상태 s에서 행동 a를 선택했을 때 s'로 전이될 확률(P)을 나타내는 식이다.

![[Pasted image 20240302013527.png]]

다음 상태를 정하는데 '현재' 상태 s와 행동 a만이 영향을 준다. 다시 말해서 상태 전이에서는 과거의 정보, 즉 지금까지 어떤 상태를 거쳐 왔고 어떤 행동들을 취해 왔는지는 신경쓰지 않는다.

이처럼 현재의 정보만 고려하는 성질을 **`마르코프 성질`** 이라고 한다.

MDP는 마르코프 성질을 만족한다고 가정하고 상태 전이(와 보상)를 모델링한다. 마르코프 성질을 도입하는 가장 큰 이유는 문제를 더 쉽게 풀기 위해서이다. 만약 마르코프 성질을 따른다고 가정하지 않는다면 과거의 모든 상태와 행동까지 고려해야 해서, 그 조합이 기하급수적으로 많아지기 때문이다.  


### 7-3. 보상 함수
---
보상이 `결정적`으로 주어진다고 할 때, 에이전트가 상태 s에서 행동 a를 수행하여 다음 상태 s'가 되었을 때 얻는 보상을 `r(s, a, s')`라는 함수로 정의한다. 

![[Pasted image 20240302014507.png]]

이렇게 정의할 수 있다.
`Reward = r(State, Action, Next_State)`


### 7-4. 정책 함수
---
마르코프 성질에 의할때, 정책에서 중요한 점은 에이전트는 '현재 상태'만으로 행동을 결정할 수 있다는 것이다. 

MDP의 마르코프 성질이라는 특성은 에이전트에 대한 제약이 아니라 '환경에 대한 제약'으로 볼 수 있다. 즉, 마르코프 성질을 만족하도록 '상태'를 관리하는 책임이 환경 쪽쪽에 있다는 것이다. 에이전트 관점에서 보면 최선의 선택에 필요한 정보가 모두 현재 상태에 담겨 있기 때문에 현재만을 바라보고 행동할 수 있다는 것이다.

이때 행동을 결정하는 방식은 2가지가 있다.
1. 결정적 행동 결정 정책
2. 확률적 행동 결정 정책

![[Pasted image 20240302015115.png]]

##### 1. 결정적 행동 결정 정책

![[Pasted image 20240302015248.png|150]]

현재 상태 's'를 전해주면 행동 'a'를 반환하는 함수이다.

##### 2. 확률적 행동 결정 정책

![[Pasted image 20240302015540.png|150]]

현재 상태 's'에서 행동 'a'를 취할 확률이다 다음과 같이 표기할 수 있다.

![[Pasted image 20240302015620.png|600]]

`Pi(Action | State)`

### 7-5. MDP의 목표
---
![[Pasted image 20240302015930.png]]

이러한 틀 안에서 최적 정책을 찾는 것이 MDP의 목표이다. 

MDP는 문제에 따라서 일회성 과제와 지속적 과제로 나뉜다. 

1. 일회성 과제
	끝이 '있는' 과제, 바둑을 예시로 들 수 있다. 언젠가 끝이 있고 결국 승리/패배/무승부 중 하나로 귀결된다

2. 지속적 과제
	끝이 '없는' 과제, 예를 들어 재고 관리와 같은 것이다. 판매량과 재고량을 보고 최적의 구매량을 결정해야 한다. 

### 7-6. 수익
---
![[Pasted image 20240302020755.png]]

이럴때 수익은 다음과 같이 정의할 수 있다. 

![[Pasted image 20240302020908.png|350]]

이떄 ϒ(0<ϒ<1)은 할인율이다. 왜 할인율을 붙여놨을까? 일단, 지속적 과제에서 수익이 무한대가 되지 않도록 조절하는 것이다. 수렴하지 않고 수익이 무한대로 늘어난다면, 우리는 최적의 정책을 찾을 수 없을것이다.

또한 할인율은 가까운 미래의 보상을 더 중요하게 보이도록 한다. 이는 생물의 모든 행동 원칙인데, 예를 들어서 지금 1만원 받기와 1년 뒤 2만원 받기는 누가 뭐래도 지금 1만원 받는게 더 큰 매력을 느낀다.


### 7-7. 상태 가치 함수 (수익의 기댓값)
---
에이전트의 목표는 '수익'을 극대화 하는 것이다. 

주의할 점은 에이전트와 환경이 '확률적'으로 동작할 수 있다는 것이다. 에이전트는 다음 행동을 확률적으로 결정할 수 있고, 상태 역시 확률적으로 전이될 수 있다. 그렇다면 얻는 수익도 '확률적'으로 달라질 것이다. 

이렇게 확률적 동작에 대응하기 위해서 기댓값, 즉 '수익의 기댓값'을 지표로 삼아야 한다.

![[Pasted image 20240302021826.png]]

이 수식을 상태 가치 함수라고 한다. 

에이전트의 정책 π는 조건으로 주어진다. 정책 π가 바뀌면 에이전트가 얻는 보상도 바뀌고 총합인 수익도 바뀌기 때문이다. 이런 특징을 반영해서 식을 다음처럼 쓸 수 있다.

![[Pasted image 20240302023224.png|300]]

>정책 π에 따라 행동했을 때 얻을 수 있는 기대 수익

일반적인 이산분포표 기댓값 구하는 수식이랑 다르게 뭔가 안쪽에 많은데 (기존의 기댓값 평균은 m = E(X) 이었다.) 

이는 모든 경우의 수의 평균이 아니라 "어떤 상태에서" 나오는 수익의 기댓값이기 때문이다. 따라서 조건부 확률이 붙은 것이다. 

### 7-8. 수익 vs 상태 가치 함수
---
1. 수익 : 일반적인 수익은 특정 경로를 따라 움직이면서 얻는 누적 보상을 계산하는 방식이다. 에피소드가 종료될 때까지 모든 단계에서 얻은 보상을 합산한다.

2. 상태 가치 함수 : 상태 가치 함수는 특정 상태에서 출발하여 장기적으로 기대되는 보상의 합을 나타낸다. 이는 주어진 정책을 따를 때, 해당 상태에서 시작하여 미래의 보상을 고려한 기대값을 의미한다.

- **일반적인 수익**: 에피소드 동안 상태 s1​에서 시작하여 s2, s3​, s4​로 이동하면서 얻은 보상이 각각 2, 3, 5라면, 총 수익은 2+3+5=10이다. (할인율이 1) 즉, 실제로 일어난 일에 대해 얻은 결과를 말한다. 굳이 말하자면, 정답 레이블에 해당한다.

- **상태 가치 함수**: 상태 s1​에서 시작하여 정책 π를 따를 때, 미래의 보상들이 각각 2, 3, 5로 기대된다면, 할인 인자 γ가 0.9일 때, 기대 가치 함수는 2 + 0.9⋅3 + 0.9^2 ⋅ 5로 계산된다. 즉, 예측에 관한 것으로 학습 데이터에 해당한다.

즉, 과거를 바탕으로 미래를 예측하려고 하는게 상태 가치 함수이다. 에초에 여태껏 있던 데이터의 기댓값(평균)이다. 과거에서 이게 젤 좋았으니 앞으로도 좋을 것이라는 가정이다.

수익은 따지자면 현 상태(s)에 대해 특정 행동을 하여 얻은 에피소드 1회의 보상의 합이고,

상태 가치 함수는 따지자면 과거부터 현재까지의 데이터의 평균이다. n회의 보상의 합이 평균을 낸 것이다. 


### 8. 최적 방정식
---
![[Pasted image 20240626141709.png]]

### 8-1. 벨만 방정식
---
한 줄 요약하면,

"현재 상태의 여태껏 쌓아논 보상 + 미래 상태의 수익 기댓값" 이다.

![[Pasted image 20240518224308.png]]


![[Pasted image 20240518153756.png]]

이게 바로 벨만 방정식이다. "상태 s의 상태 가치 함수" 와 다음에 취할 수 있는 "상태 s'의 상태 가치 함수" 의 관계를 나타낸 식이다. 모든 상태 s와 모든 정책 pi에 대해 성립한다. 

식의 의미가 꽤나 중요한데
1. ![[Pasted image 20240518153131.png]] (주어진 정책을 기반으로) 상태 s일때 보상 R의 기댓값 과거에서부터 현재까지의 결과의 평균치가 담겨 있다. 
2. ![[Pasted image 20240518153224.png]] 현 상태에서 다음 상태로 전이했을때의 다음 보상 기댓값 (수익은 현재 보상에 할인율을 적용한 것이다. 가까운 미래의 보상을 더 중시하기 위해)

현재 상태(s)에서 다음에 들어올 기대수익을 더한 값으로, 어찌보면 미래를 내다 보는 거라고 볼 수 있다. 

즉, 현재 상태에서 에이전트가 앞으로 얻길 기대하는 수익이다. 


>1. 상태 가치 함수의 벨만 방정식
	- 현재 상태에서, 모든 정책을 고려했을 때, 앞으로 들어올 기대되는 수익은 얼마일까요? 
>2. 행동 가치 함수의 벨만 방정식
	- 현재 상태에서, 딱 1가지의 행동만 고려했을 때, 기대되는 수익을 어떻게 될까요? (현재 상태에서 각 행동에 대한 기댓값을 얻을 수 있다. ) 

### 8-2. 벨만 최적 방정식
---
![[Pasted image 20240519173437.png]]

>1. 상태 가치 함수의 벨만 최적 방정식
	- 현재 상태에서, 모든 정책을 고려했을 때, 가장 최고의 시나리오로 구성된 최고의 수익
	- "현재 상태 쌓아논 보상 + 수 많은 미래 상태에서 가장 최고의 보상을 얻는 최댓값"

상태 가치 함수의 벨만 방정식 : "현재 상태 쌓아논 보상 + 현재 행동 정책에 의한 수익 기댓값"(밀를 모를때)
상태 가치 함수의 벨만 최적 방정식 : "현재 상태 쌓아논 보상 + 현재 행동 정책에서 가장 최고의 정책을 따랐을때의 보상 최댓값" (미래를 알고 행동할때)



>2. 행동 가치 함수의 벨만 최적 방정식
	- 현재 상태에서, 모든 행동 중 1개의 행동만 고려했을 때, 가장 최고의 시나리오로 구성된 가장 최고의 수익

행동 가치 함수의 벨만 방정식 : "현재 상태 쌓아논 보상 + 현재 정책에서 임의의 1가지 행동 + 현재 정책에 따른 미래 상태의 수익 기댓값"
행동 가치 함수의 벨만 최적 방정식 : "현재 상태 쌓아논 보상 + 현재 정책에서 임의의 1가지 행동 + 최고의 행동 정책만을 따라갔을때 얻는 보상 최댓값"



>행동 가치 함수의 벨만 최적 방정식 : "현재 상태 쌓아논 보상 + 임의의 1가지 행동 + 가장 최고의 행동 정책 시나리오 보상을 얻는 최댓값"
>
>그럼 그 임의의 1가지 행동중 가장 최고인 행동을 하나 뽑으면 보상이 세계 최고 수준이 되겠네?에 대한 답변이다.

### 8-3. 최적 정책
---
![[Pasted image 20240519173506.png]]

행동 가치 함수의 벨만 방정식이 현재 상태 중 1개의 행동만 고려했을 때, 가장 최고의 수익이라면, 그 최고의 수익을 고르게 한 행동은 무엇일까? 에 대한 답을 구할 수 있는 식이다.

> 중요한 점은 확률적인 행동 정책을 결정적인 행동 정책으로 뽑을 수 있다는 점이다!!

임의의 1가지 행동 중 최상의 1가지 행동을 추출하기 위한 식, 추출해내면 "현재 상태 쌓아논 보상 + 최상의 1가지 행동 + 가장 최고의 시나리오 보상을 얻는 최댓값“ 이 된다.



### 8-4. 정책 반복법
---
![[Pasted image 20240519192605.png]]

이 식 자체가 바로 최적 정책이 만족하는 식이다. 탐욕화를 수행해도 정책이 그대로라면, 이미 최적 정책이란 것이다.

다음 기호를 사용하여 정책 개선 방법을 설명해보자
1. 최적 정책 : μ<sub>*</sub>(s)
2. 최적 정책의 상태 가치 함수 : v<sub>*</sub>(s)
3. 최적 정책의 행동 가치 함수(Q 함수) : q<sub>*</sub>(s, a)

![[Pasted image 20240519192044.png]]

최적 정책은 다음과 같다고 할 때, 임의의 결정적 정책에 대해 다음과 같이 적용해보자 (일반화)

![[Pasted image 20240519192242.png]]

이때 각 개념을 다음 기호로 표기해보자
1. 현 상태의 정책: μ(s)
2. 정책 μ(s)의 상태 가치 함수 : v<sub>μ</sub>(s)
3. 새로운 정책: μ'(s)

만약 정책이 바뀐다면, 정책 μ' 가 정책 μ와 달라진다면 새로운 정책은 반드시 기존 정책보다 좋아야 한다 따라서 모든 상태에서 "v<sub>μ'</sub>(s) >= v<sub>μ</sub>(s)" 이란 것이다

그럼 이제, 다음 순서를 따른다. 

>1. 먼저 𝜋<sub>0</sub>​이라는 정책에서 시작한다. 정책 𝜋<sub>0</sub>​은 확률적일 수도 있으므로 𝜇<sub>0</sub>(𝑠)가 아닌 𝜋<sub>0</sub>(𝑠|𝑎)로 표기한다.
>2. 다음으로 정책 𝜋<sub>0</sub>​의 가치 함수를 평가하여 V<sub>0</sub>​를 얻는다. 반복적 정책 평가 알고리즘을 이용하면 된다.
>3. 그리고 가치 함수 V<sub>0</sub>​를 이용하여 탐욕화를 수행한다. 탐욕 정책은 언제나 하나의 행동을 선택하므로 결정적 정책인 𝜇<sub>1</sub>​을 얻을 수 있다.
>4. 1 ~ 3 과정을 반복한다.

왜 가치 함수를 구하나요? 라고 할 수 있는데, 저기 위 식을 보면 상태 가치 함수를 구하고 정책을 구할 수 있기 때문이다. 이걸 무한 반복하면 정답에 근접하지 않겠냐 라는 것(이것은 상태 가치 함수를 동적 프로그래밍을 해서 그렇다.)

이때, 상태가 절대적으로 변한다는 가정 하라면, 

![[Pasted image 20240525212050.png]]

이런식으로 식을 다시 쓸 수 있다. 

### 9. 동적 프로그래밍
---
![[Pasted image 20240617192709.png]]

### 10. 가치 반복법
---
![[Pasted image 20240615211748.png]]


