강화학습에 사용되는 언어나 수식을 정리해둔 공간

### 1. 행동 (Action)
---
에이전트가 수행하는 행동. `A`로 표기한다.

어떤 에이전트가 행동을 a, b만 취할 수 있다면 A는 {a, b} 중 하나의 값을 취하게 된다.

### 2. 보상 (Reward)
---
에이전트가 행동을 함으로써 얻게 되는 보상값. `R`로 표기한다.
t 번째에 얻는 보삼임을 나타낼때는 R_t로 쓴다. 

### 3. 기댓값 (Expectation)
---
확률 변수로 정의되는 기댓값, 기호로는 `E`로 표기한다.
1. 보상 R의 기댓값 : `E[R]`
2. 행동 A를 선택했을 때의 보상 기댓값 : `E[R|A]`
	이때 `|`는 조건부 확률(조건부 기댓값)을 뜻한다. 

### 4. 행동 가치 (Quality)
---
보상에 대한 기댓값을 행동 가치라고 한다.
1. `q(A) = E[R|A]` : 행동 A의 행동 가치
2. 대문자(`Q(A)`)의 경우 추정치를 의미한다.
3. 소문자(`q(A)`)의 경우 실제 행동 가치(실제 휙득한 보상의 평균)를 의미한다.

### 5. 가치 추정치
---
가치 추정치 : 실제 획득한 보상의 평균값

![[Pasted image 20240301230949 1.png|400]]

기댓값을 각각 나타내보면 
1. 슬롯머신 a의 행동 가치 - Q(a) = 2
2. 슬롯머신 b의 행동 가치 - Q(b) = 0.333

이렇게 3번만 확인해봤을때 현재 상태로는 a가 좋은 슬롯머신이니까 a를 계속 돌리는 것이 타당하다.

이렇게 슬롯머신을 실제로 플레이하여 얻은 보상은 어떤 확률 분포에서 생성된 샘플(표본)이다. 따라서 실제 휙득한 보상의 평균을 **`표본 평균`** 이라고 할 수 있다. **`표본 평균`** 은 샘플링 횟수가 늘어날 수록 실제값(보상의 기댓값)에 가까워진다.

그럼 이러한 과정을 수식으로 써보자

![[Pasted image 20240301231408 1.png|300]]

근데 이 수식은 컴퓨터 계산상 그렇게 좋진 않다. 왜냐면 플레이 횟수(n)이 늘어날 수록 보상(R)의 원소 수가 늘어난다. 따라서 `sum(rewards)`코드의 실행 비용도 함께 증가한다. 즉, 플레이 횟수가 늘어날수록 메모리와 계산량이 너무 늘어나는게 문제

따라서 다음처럼 수식을 변경하겠다. 

![[Pasted image 20240301232209 1.png]]

여기서 중요한 것은 ![[Pasted image 20240301232310 1.png|170]]이다.

`Q_n`은 `Q_(n-1)`에 어떤 값을 더해 구할 수 있다. 이를 그림으로 그리면 다음과 같다. 

![[Pasted image 20240301232824 1.png]]

이전의 행동 가치`(Q_(n-1))`는 `R_n`방향으로 얼마만큼의 `1/n`을 곱했냐에 따라서 갱신되는 양이 다르다. 즉, 이러한 `1/n`은 갱신되는 양을 조절하기 때문에 `학습률`과 같은 역할을 한다.

``` python
Q += (reward - Q) / n
```

이렇게 Q1, Q2, Q3, ... 식으로 하나씩 순서대로 증가시키며 구할 수 있다는 뜻에서 이런 구현 방식을 증분 구현이라고 한다.

