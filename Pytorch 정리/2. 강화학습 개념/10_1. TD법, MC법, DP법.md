몬테카를로법은 에피소드의 끝에 도달한 후에야 가치 함수를 갱신할 수 있다. 에피소드가 끝나야만 수익이 확정되기 때문이다. 

몬테카를로법은 정말 좋은데 가장 큰 문제가 2가지가 있다.
1. 만약 지속적 과제에서는 에피소드가 안끝나기 때문에 사용이 불가능하다.
2. 일회성 과제라도 시간이 오래 걸리면 마찬가지로 사용이 불가능하다.

따라서 이에 대한 해결책이 필요하다. 그게 바로 TD법이다. TD법은 '몬테카를로법'과 '동적 프로그래밍'을 합친 기법이다. 

### 1. TD법 도출하기
---
우리는 여태껏 수익을 다음과 같이 정의했다. 

![[Pasted image 20240620224606.png]]

이 수익을 적용하면 '가치 함수'는 다음과 같이 정의된다.

![[Pasted image 20240620224644.png]]

이제 이 식을 가지고 MC법과 DP법을 도출해보자

### 2. MC법
---
기댓값을 계산하는 대신 실제 수익의 샘플 데이터를 평균하여 

![[Pasted image 20240620225135.png]]

위 식의 기댓값을 근사한다. 

평균에는 2가지가 있다.
1. 표본 평균
2. 지수 이동 평균

>왜 지수 이동 평균일까??
>그것은 비정상 문제를 풀기 위해서이다. 비정상 문제는 확률 분포가 변하는 문제이다. 우리는 아예 정보조차 없으니까 매번 확률 분포가 변할것이다...  

이때 지수 이동 평균을 이용하려면 새로운 수익이 발생할 때마다 고정된 값 α로 갱신한다. 수식으로는 다음과 같다. 

![[Pasted image 20240620225550.png]]

여기서 V<sub>π</sub>는 현재의 가치 함수이고 V<sup>  '</sup><sub>π</sub> 는 갱신 후의 가치 함수이다. 그래서 위 식은 현재의 가치 함수 V<sub>π</sub>를 G<sub>t</sub>쪽으로 갱신하고 있다. G<sub>t</sub>쪽으로 얼마나 갱신할지는 α로 조정한다. 

### 3. DP법
---
DP법은 MC법과 달리 계산으로 기댓값을 구한다. 

![[Pasted image 20240620225904.png]]

위 식은 아주 친숙한 벨만 방정식이다. 위 식에서 중요한 것은 '현재 상태에서의 가치함수' 를 '다음 상태에서의 가치 함수'로 갱신한다는 것이다. 이때 모든 전이를 고려하는것이 특징이다.

>우리가 상태 가치 함수를 다이나믹 프로그래밍을 이용해 만든게 DP법이다. 근데 그냥 상태 가치 함수와 행동 가치 함수가 있었던것 기억하는가? 상태 가치 함수는 모든 가능성을 엿보는게 특징이었다. 

![[Pasted image 20240620230240.png]]

>DP법은 다음 가치 함수의 추정치를 이용해 현재 가치 함수의 추정치를 갱신한다. 이 원리를 '부트스트랩'이라고 한다.

반면 MC법은 실제로 얻은 일부 경험만을 토대로 현재의 가치 함수를 갱신한다. 이 두 방법을 적절하게 융합한 것이 TD법이다.

>DP법은 정말 최고의 가치 함수 추정치이다. 문제가 있다면, 모든 행동을 해보고 나서 데이터를 모두 구해야 하는게 문제다. 에초에 환경 모델이 제시가 되는게 아니면, 솔직히 현실적으로 모든 상황에 맞는 답안을 구할 수 있을리가 없다...

### 4. TD법
---
![[Pasted image 20240620230427.png|350]]

위 그림과 같이 다음 행동과 가치 함수만을 이용해 현재 가치 함수를 갱신한다. 중요한 점은 다음 두 가지이다.

1. DP법처럼 부트스트랩을 통해 가치 함수를 순차적으로 갱신
2. MC법처럼 환경에 대한 정보 없이 샘플링된 데이터만으로 가치 함수 갱신

이제 수식에서 도출해보자 먼저 벨만 방정식을 다음과 같이 전개하자

![[Pasted image 20240620231253.png]]

식 (1)은 모든 후보에 대해 

![[Pasted image 20240620231338.png]]

를, 즉 보상과 다음 가치 함수를 계산한다. 이를 기댓값 형태로 다시 쓰면 식 (2)가 된다. 

TD법에서는 식 (2)를 이용하여 가치 함수를 갱신하는데, R<sub>t</sub>+ϒ v<sub>π</sub>(S<sub>t+1</sub>) 부분을 샘플 데이터에서 근사한다. 그래서 TD법을 갱신식을 수식으로 표현하면 다음과 같다.

![[Pasted image 20240620231940.png]]

위 식에서 V<sub>π</sub>는 가치 함수의 추정치이고, R<sub>t</sub>+ϒ v<sub>π</sub>(S<sub>t+1</sub>)은 목적지(목표)이다. 이 목적지를 RD목표라고 하며 TD법은 V<sub>π</sub>(S<sub>π</sub>)를 TD 목표 방향으로 갱신한다.

### 5. MC법과 TD법 비교
---
환경 모델을 모를 때 사용할 수 있는 도구는 이제 TD법과 MC법이다. 그렇다면 어떤것을 사용해야 할까? 지속적인 과제는 MC법을 사용할 수 없으니 TD법만이 유일한 방법이다. 

일회성 과제도 안타깝게도 이론적으로 뭐가 더 나은지 밝혀진것이 없다. 하지만 현실의 많은 문제는 TD법이 더 빠르게 학습한다(가치 함수의 갱신이 더 빠르다). MC법과 TD법이 무엇을 목표로 하는지 보면 그 이유를 알 수 있다.

![[Pasted image 20240621000037.png]]

>이때 프라임의 의미는 다음 가치 함수, 새로운 가치 함수라는 의미이다.

1. MC법 : G<sub>t</sub>를 목표로 하여 그 방향으로 V<sub>π</sub>를 갱신한다. 여기서 G<sub>t</sub>는 목표에 도달했을 때 얻을 수 있는 수익의 샘플 데이터이다.
2. TD법 : 한 단계 앞의 정보를 이용해 계산한다. 이 경우 시간이 한 단계씩 진행될 때마마 가치 함수를 갱신할 수 있기 때문에 효율적인 학습을 기대할 수 있다. 

MC법의 목표는 많은 시간을 쌓아서 만든 것이기에 변동이 심한 편이다. 즉, 분산이 크다. 반면 TD법은 한 단계 앞의 데이터를 기반으로 하여 변동이 적다. 따라서 다음과 같은 상황이 된다.

![[KakaoTalk_20240621_000329502.jpg]]

MC법의 목표는 멀리 있어서 분산이 크다. 반면 TD법은 목표가 겨우 한 단계 앞의 시간이기 떄문에 분산이 작다.

![[Pasted image 20240621000037.png]]

TD 목표는  R<sub>t</sub>+ϒ V<sub>π</sub>(S<sub>t+1</sub>)인데, 잘 보면 추정치인 V<sub>π</sub>가 사용되고 있다. 즉, TD법은 '추정치로 추정치를 갱신'하는 부트스트래핑이다. 이처럼 TD목표는 추정치를 포함하기 때문에 정확한 값이 아니다. 학문적으로는 '편향이 있다.'라고 한다. 하지만 편향은 갱신이 반복될때마다 점점 작아져 결국에는 0으로 수렴한다.

반면 MC법의 목표에는 추정치가 포함되지 않기 때문에 '편향이 없다'라고할 수 있다.  

### 6. TD법 구현
---
``` python
class TdAgent:
    def __init__(self):
        self.gamma = 0.9
        self.alpha = 0.01
        self.action_size = 4
		
        random_actions = {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}
        self.pi = defaultdict(lambda: random_actions)
        self.V = defaultdict(lambda: 0)
		
    def get_action(self, state):
        action_probs = self.pi[state]
        actions = list(action_probs.keys())
        probs = list(action_probs.values())
        return np.random.choice(actions, p=probs)
		
    def eval(self, state, reward, next_state, done):
        next_V = 0 if done else self.V[next_state]  # 목표 지점의 가치 함수는 0
        target = reward + self.gamma * next_V
        self.V[state] += (target - self.V[state]) * self.alpha
```

`eval()`메서드를 보면 TD법을 이용해서 정책을 평가한다.

이 메서드는 상태 state에서 행동 action을 수행하고, 보상 reward를 받고, 다음 상태 next_state로 넘어갔을 때 호출된다. 또한 에피소드가 끝났는지 여부(next_state가 목표인지 여부)를 나타내는 플래그 done도 매개변수로 받는다. 

>가치 함수는 미래에 얻을 수 있는 보상의 총합이다. 따라서 목표 지점에서의 가치 함수는 항상 0이다. 이후로 더 얻을 보상이 없기 때문이다.



