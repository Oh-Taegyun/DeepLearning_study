최적 정책은 '평가'와 '개선'을 번갈아 반복하면서 얻는다. 

몬테카를로법또한 정책을 평가했다. 따라서 이제 개선을 해야 한다. 

![[Pasted image 20240526013356.png]]

여태까지는 가치 함수 V에 대한 평가를 진행했다. 만약 가치 함수 V를 사용하여 정책을 개선한다면 위 식을 사용하면 된다. 

하지만 위 식은 제약이 존재한다. 바로 일반적인 강화 학습 문제에서는 환경 모델, 즉  상태 전이 함수 p(s'|s, a)와 보상 함수 r(s, a, s')를 알 수 없다. 

위 식은 이걸 모르면 계산이 불가능하다. 따라서 일반적인 강화 학습 문제에서는 Q함수를 이용하는 근본 식

![[Pasted image 20240526014617.png]]

을 써야한다. 이 식은 단순히 최대가 되는 행동 a만 찾아내면 되므로, 환경 모델이 필요없다. 

>완전 근본 식인 Q함수를 써야 하므로
>![[Pasted image 20240625152638.png|400]]
>이걸 써야 한다.


Q 함수를 대상으로 개선할 경우 Q 함수를 '평가'해야한다. 앞 절까지는 몬테카를로법으로 상태 가치 함수를 평가했는데 평가 대상을 Q함수로 바꿔줘야 한다.  그럴려면 몬테카를로법의 갱신식에서 V(s)에서 Q(s, a)로 전환하면 된다. 

따라서 몬테카를로법을 조금 손봐주자

Q 함수는 근본적으로 상태 가치 함수에서 나온 녀석이기에 그냥 그대로 바꿔줘도 문제가 없다. 

![[Pasted image 20240526015945.png]]

``` python
class McAgent: # RandomAgent 클래스와 거의 같다. 
    def __init__(self):
        self.gamma = 0.9
        self.epsilon = 0.1  # (첫 번째 개선) ε-탐욕 정책의 ε
        self.alpha = 0.1    # (두 번째 개선) Q 함수 갱신 시의 고정값 α
        self.action_size = 4

        random_actions = {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}
        self.pi = defaultdict(lambda: random_actions)
        self.Q = defaultdict(lambda: 0)
        # self.cnts = defaultdict(lambda: 0)
        self.memory = []

    def get_action(self, state):
        action_probs = self.pi[state]
        actions = list(action_probs.keys())
        probs = list(action_probs.values())
        return np.random.choice(actions, p=probs)

    def add(self, state, action, reward):
        data = (state, action, reward)
        self.memory.append(data)

    def reset(self):
        self.memory.clear()

    def update(self):
        G = 0
        for data in reversed(self.memory):
            state, action, reward = data
            G = self.gamma * G + reward
            key = (state, action)
            self.cnts[key] += 1
            self.Q[key] += (G - self.Q[key]) / self.cnts[key] # 에러 발생
            self.pi[state] = greedy_probs(self.Q, state, self.epsilon)
```

``` python
def greedy_probs(Q, state, epsilon=0, action_size=4):
    qs = [Q[(state, action)] for action in range(action_size)]
    max_action = np.argmax(qs)

    action_probs = {action: 0.0 for action in range(action_size)}  
    # 이 시점에서 action_probs는 {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0}이 됨
    action_probs[max_action] += 1 # 에러 발생
    return action_probs
```

매개변수로 받은 state 상태에서 Q 함수의 값이 가장 큰 행동만을 취하게끔 확률 분포를 만들어 준다. 예를 들어 주어진 상태에서 0번째 행동의 Q 함수 값이 가장 크다면, `[0: 1.0, 1:0.0, 2:0.0, 3:0.0]`을 반환한다. 

위 식에서 greedy_probs는 탐욕 행동을 취하도록 하는 확률 분포를 반환한다. 즉, 매개변수로 받은 state 상태에서 Q함수의 값이 가장 큰 행동만을 취하게끔 확률 분포를 만들어 준다.

문제는 저 코드에서

``` python
1. self.Q[key] += (G - self.Q[key]) / self.cnts[key]
2. action_probs[max_action] += 1
```
이 두 부분이 문제가 있다. 

### 1. ɛ-탐욕 정책으로 변경
---
``` python
self.Q[key] += (G - self.Q[key]) / self.cnts[key]
```

이 부분을 바꿔야 한다. 

에이전트는 개선 단계에서 정책을 탐욕화한다. 탐욕화의 결과로 해당 상태에서 취할 수 있는 행동이 단 하나로 고정된다. 

![[Pasted image 20240526020953.png]]

위 그림과 같이 탐욕 행동만을 수행다면, 에이전트의 경로가 딱 한가지로만 고정된다. 

문제는 딱 한가지로만 고정 된다면, 모든 상태와 행동 조합에 대한 수익 샘플 데이터를 수집할 수 없다. 다시 말해 모든 상태와 행동 조합에 따른 수익 데이터를 얻을 수 없단 말이다. 이 문제를 해결하기 위해서는 에이전트가 '탐색'도 시도하도록 해야 한다. 

이 문제는 앞서 벤티드 문제와 같은 고질적 문제인데, 해결 방법은 의외로 간단하다.

ɛ-탐욕 정책을 사용하면 된다. 기본적으로 Q함수의 값이 가장 큰 행동을 선택하되, 무작위성을 첨가해서 낮은 확률로 아무 행동이나 선택하도록 하는 것이다.

``` python
def greedy_probs(Q, state, epsilon=0, action_size=4):
    qs = [Q[(state, action)] for action in range(action_size)]
    max_action = np.argmax(qs)

    base_prob = epsilon / action_size
    action_probs = {action: base_prob for action in range(action_size)}  
    # {0: ε/4, 1: ε/4, 2: ε/4, 3: ε/4}
    action_probs[max_action] += (1 - epsilon) 
    return action_probs
```

### 2. 고정값 α 방식으로 수행
---
``` python 
def update(self):
	G = 0
	for data in reversed(self.memory):
		state, action, reward = data
		G = self.gamma * G + reward
		key = (state, action)
		# self.cnts[key] += 1
		# self.Q[key] += (G - self.Q[key]) / self.cnts[key]
		self.Q[key] += (G - self.Q[key]) * self.alpha
		self.pi[state] = greedy_probs(self.Q, state, self.epsilon)
```

![[Pasted image 20240526023614.png]]

수정 전과 후의 차이이다. 간단하게 모든 샘플 데이터에 가중치를 균일하게 주는 방식과, 고정값 α로 갱신하는 방식이다. 

이렇게 고정값 α로 개선하는 방식을 지수 이동 평균이라고 한다.

근데 왜 이렇게 지수 이동 평균을 쓸까?

'수익'은 샘플 데이터가 생성되는 확률 분포가 시간에 따라 달라지기 때문이다. 간단히 말해 에피소드가 진행될수록 정책이 갱신되기 때문에 수익이 생성되는 확률 분포도 달라진다. 즉, 비정상 문제라는 것이다. 

수익은 '환경의 상태 전이'와 '에이전트의 정책'이라는 두 가지 확률적 처리를 반복하며 만들어진다. 이 두 가지 처리에서의 확률 분포가 아무것도 변하지 않는다면 샘플링되는 수익의 분포 역시 정상(staedy)이다. 하지만 둘 중 하나라도 분포가 바뀌면, 확률 분포는 비정상이 된다. 

정책을 반복적으로 계산하는 다이나믹 프로그래밍 기법(상태 가치 함수를...)을 채용했으므로, 지속적으로 정책이 바뀌게 될 것이다. (정책 반복법) 따라서 수익의 확률 분포가 달라진다.

비정상 문제에서 이럴 경우 최신의 정보를 중요하게 여기는게 중요하므로, 지수 이동 평균을 쓰는게 옳다.


