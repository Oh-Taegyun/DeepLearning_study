### 1. torch.autograd 를 사용한 자동 미분
---
이 부분은 우리가 Dezero를 만들면서 사용했던 이론 그대로 따라간다. Dezero에선 variable이라는 클래스를 만들고 Function이라는 클래스를 만들었다. 이때 Function이라는 클래스가 입력 (variable)과 출력값(variable)을 기억하고 저장함으로써 계산 그래프가 실시간으로 연결되어 갔다. 이 부분이 Define-and-run의 중요 부분이었다. 

마지막 출력값(variable)에서 역전파를 구하는 메서드인 backward()를 실행시켜주면 연결된 계산 그래프를 따라가서 미분값을 도출 할 수 있었다. 참고로 고계 미분의 경우 미분값을 도출할때 그려논 계산그래프를 다시 역전파 시켜버리는 엄청난 아이디어로 쉽게 구할 수 있었다!!! 


### 2. Autograd 사용 방법
---
![[../image/03.png]]
``` python
import torch

x = torch.ones(5)  # input tensor
y = torch.zeros(3)  # expected output
w = torch.randn(5, 3, requires_grad=True)
b = torch.randn(3, requires_grad=True)
z = torch.matmul(x, w)+b
loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)
```

이 신경망에서, `w`와 `b`는 최적화를 해야 하는 **매개변수**입니다. 따라서 이러한 변수들에 대한 손실 함수의 변화도를 계산할 수 있어야 합니다. 이를 위해서 해당 텐서에 `requires_grad` 속성을 설정합니다.

연산 그래프를 구성하기 위해 텐서에 적용하는 함수는 사실 `Function` 클래스의 객체입니다. 이 객체는 _순전파_ 방향으로 함수를 계산하는 방법과, _역방향 전파_ 단계에서 도함수(derivative)를 계산하는 방법을 알고 있습니다. 역방향 전파 함수에 대한 참조(reference)는 텐서의 `grad_fn` 속성에 저장됩니다. 

-> Dezero에서 설명한 그대로 똑같다는 뜻


### 3. 변화도 추적 멈추기
---
``` python
with torch.no_grad():
	# 연산 그래프 추적 멈추는 기능
	# 순전파의 경우를 떠올려보자
    z = torch.matmul(x, w)+b
```






