![[Pasted image 20240213161251.png|400]]

신경망 스스로 덧셈 예제들을 학습시켜서 정답을 내놓게 할것이다. 참고로 이러한 머신러닝을 평가하고자 만든 간단한 문제를 ‘장난감 문제’ 라고 한다.


### 1. 가변 길이 시계열 데이터
---
이번 ‘덧셈’ 문제에서는 샘플마다 데이터의 시간 방향 크기가 다르다. (예 : “57+5”나 “628+521”등) ‘가변 길이 시계열 데이터’를 다룬다는 뜻이다. 당연하게 데이터의 정규화는 필수가 된다. 

또한 신경망 학습 시 ‘미니배치 처리’를 하려면 무언가 추가 노력이 필요하게 된다. 미니배치로 학습할 떄는 다수의 샘플을 한꺼번에 처리한다. 이떄 적어도 우리 구현에서는 한 미니배치에 속한 샘플들의 데이터 형상이 모두 똑같아야 한다.

가변 길이 시계열 데이터를 미니배치로 학습하기 위한 가장 단순한 방법은 패딩을 사용하는 것이다. 패딩이란 원래의 데이터에 의미 없는 데이터를 채워 모든 데이터의 길이를 균일하게 맞추는 기법이다.

![[Pasted image 20240213163341.png]]

1. 0~999 사이의 숫자 2개만 더하기로 함
2. 질문과 정답을 구분하기 위해서 출력 앞에 구분자로 밑줄(`_`)을 붙이기로 함. 

이처럼 패딩을 적용해 데이터 크기를 통일시키면 가변 길이 시계열 데이터도 처리할 수 있다. 그러나 원래는 존재하지 않던 패딩용 문자까지 seq2seq가 처리하게 된다. 따라서 패딩을 적용해야 하지만 정확성이 중요하다면 seq2seq에 패딩 전용 처리를 추가해야 한다. 예컨대 decoder에 입력된 데이터가 패딩이라면 손실의 결과에 반영하지 않도록 해야한다. 

한편 encoder에 입력된 데이터가 패딩이라면 LSTM 계층이 이전 시각의 입력을 그대로 출력하게 한다. 즉, LSTM 계층은 마치 처음부터 패딩이 존재하지 않았던 것처럼 인코딩할 수 있다.


### 2. 덧셈 데이터셋
---
load_data(file_name, seed)는 file_name 으로 지정한 텍스트 파일을 읽어 텍스트를 문자 ID로 변환하고, 이를 훈련 데이터와 테스트 데이터로 나눠 반환한다. Seed는 이 메서드 내부에서 사용하는 무작위수의 초깃값이다.

![[fig 7-12.png|400]]


### 3.  Encoder 클래스
---
![[Pasted image 20240220183143.png]]

Encoder 클래스는 Embedding 계층과 LSTM 계층으로 구성된다. 이 구성에서 더 위에는 다른 계층이 없으니 LSTM 계층의 위쪽 출력은 폐기된다. 위 그림에서 보듯 Encoder에서는 마지막 문자를 처리한 후 LSTM 계층의 은닉 상태 h를 출력한다.. 그리고 이 은닉 상태 h가 Decoder로 전달된다.

``` python
import torch
import torch.nn as nn

class Encoder(nn.Module):
    def __init__(self, vocab_size, wordvec_size, hidden_size):
        super(Encoder, self).__init__()
        # Embedding layer
        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=wordvec_size)
        # LSTM layer
        self.lstm = nn.LSTM(input_size=wordvec_size, hidden_size=hidden_size, batch_first=True)

    def forward(self, xs):
        # xs: [batch_size, seq_len]
        embedded = self.embedding(xs)  # embedded: [batch_size, seq_len, wordvec_size]
        # LSTM 출력: outputs (batch, seq_len, num_directions * hidden_size), (h_n, c_n)
        outputs, (hidden, cell) = self.lstm(embedded)
        # outputs[:, -1, :]: [batch_size, hidden_size], 마지막 시간 단계의 hidden state 반환
        return outputs[:, -1, :]
```


### 4. Decoder 클래스
---
![[fig 7-17.png]]

RNN으로 문장을 생성할 때, 학습 시와 생성 시의 데이터 부여 방법이 다르다. 학습 시는 정답을 알고 있기 때문에 시계열 방향의 데이터를 한꺼번에 줄 수 있다. 

한편, 추론 시 (새로운 문자열을 생성할 때)에는 최초 시작을 알리는 구분 문자(이번 예에서는 `‘_’`) 하나만 준다. 그리고 그 출력으로부터 문자를 하나 샘플링하여, 그 샘플링한 문자를 다음 입력으로 사용하는 과정을 반복하는 것이다.

이번엔 ‘확률적’이 아닌 ‘결정적’으로 선택한다. (이번 문제는 덧셈이므로 이러한 확률적인 비결정성을 배제하고 결정적인 답을 생성하고자 함을 위함) 즉, 마지막 계층을 softmax 계층보다는 그냥 Affine 계층을 이용한단 뜻

``` python
import torch
import torch.nn as nn

class Decoder(nn.Module):
    def __init__(self, vocab_size, wordvec_size, hidden_size):
        super(Decoder, self).__init__()
        self.hidden_size = hidden_size
        self.embed = nn.Embedding(vocab_size, wordvec_size)
        self.lstm = nn.LSTM(wordvec_size, hidden_size, batch_first=True)
        self.affine = nn.Linear(hidden_size, vocab_size)
        
        # LSTM의 초기 상태 설정을 위한 매개변수
        self.h0 = nn.Parameter(torch.zeros(1, 1, hidden_size))
        self.c0 = nn.Parameter(torch.zeros(1, 1, hidden_size))

    def forward(self, xs, h=None):
        if h is None:
            h = self.init_hidden(xs.size(0))
        out = self.embed(xs)
        out, (hn, cn) = self.lstm(out, h)
        out = self.affine(out)
        return out

    def init_hidden(self, batch_size):
        # 초기 h와 c 상태를 생성합니다.
        h0 = self.h0.repeat(1, batch_size, 1)
        c0 = self.c0.repeat(1, batch_size, 1)
        return (h0, c0)

    def generate(self, start_id, sample_size):
        sampled = []
        sample_id = start_id
        h = self.init_hidden(1)
        
        for _ in range(sample_size):
            x = torch.tensor([[sample_id]], dtype=torch.long)
            out = self.embed(x)
            out, h = self.lstm(out, h)
            score = self.affine(out)
            sample_id = torch.argmax(score.flatten()).item()
            sampled.append(sample_id)
        
        return sampled

```


### 5. seq2seq 클래스
---

