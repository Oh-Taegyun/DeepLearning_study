GRU(Gated Recurrent Unit)는 순환 신경망(RNN)의 한 종류로, 시계열 데이터를 처리하기 위해 설계된 신경망 구조 중 하나이다. 

GRU는 LSTM(Long Short-Term Memory)의 간소화된 버전으로 볼 수 있으며, 시간적으로 떨어져 있는 정보 사이의 의존성을 학습하는 데 효과적이다. GRU는 LSTM과 비교했을 때 구조가 간단하면서도 비슷한 성능을 낼 수 있다는 장점이 있다.

간소화 했다고 LSTM 이전 모델로 생각할 수 있는데 에초에 GRU는 LSTM을 개선하기 위한 모델이다. 

### 1. 인터페이스
---
![[Pasted image 20240213130419.png]]

LSTM이 은닉 상태와 기억 셀이라는 2개의 선을 사용하는 데 반해, GRU는 은닉 상태만 사용한다.

![[Pasted image 20240213130440.png]]

![[Pasted image 20240213131556.png|300]]

GRU에는 기억 셀은 없고, 시간 방향으로 전파하는 은닉 상태 h뿐이다. 그리고 r과 z라는 2개의 게이트를 사용한다. (참고로 LSTM은 3개의 게이트를 사용했다.) 여기서 r은 reset 게이트 z는 update 게이트이다.

Reset 게이트는 은닉 상태를 얼마나 ‘무시’할지를 정한다. 만약 r이 0이면 3번째 식으로부터 새로운 은닉 상태 ![[Pasted image 20240213131638.png|15]]는 입력 x_t만으로 결정된다. 즉, 과거의 은닉 상태는 완전히 무시된다.

한편 update 게이트는 은닉 상태를 갱신하는 게이트이다. LSTM의 forget 게이트와 input 게이트라는 두 가지 역할을 혼자 담당하는 것이다. 

Forget 게이트로써의 기능은 ![[Pasted image 20240213131912.png|100]] 이다. 이 계산에 의해 과거의 은닉 상태에서 잊어야 할 정보를 삭제한다. 

input 게이트로써의 기능은 ![[Pasted image 20240213132125.png|50]] 부분이다. 이에 따라 새로 추가된 정보에 input게이트의 가중치를 부여한다. 
