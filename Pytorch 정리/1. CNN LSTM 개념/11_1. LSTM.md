![[1. CNN LSTM 개념/image2/19.png]]

한층 더 진화된 RNN인 LSTM을 알아보자

위에서 보듯 LSTM계층의 인터페이스에는 c라는 경로가 있다는 차이가 있다. 이 c를 기억 셀(혹은 단순히 셀)이라 하며 LSTM 전용의 기억 메커니즘이다.

기억 셀의 특징은 데이터를 자기 자신으로만(LSTM 계층 내에서만) 주고받는다는 것이다. 즉, LSTM 계층 내에서만 완결되고, 다른 계층으로는 출력하지 않는다. 

![[1. CNN LSTM 개념/image2/20.png|500]]

이 기억셀은 은닉상태의 계산에 영향을 준다.


### 1. 게이트
---
![[1. CNN LSTM 개념/image2/21.png|600]]
LSTM의 가장 중요한 점은 0.0 ~ 1.0까지 게이트를 얼마나 열 것인지를 자동으로 학습한다는 것이다. 참고로, 게이트의 열림 상태를 구할 때는 시그모이드 함수를 사용하는데, 시그모이드 함수의 출력이 마침 0.0 ~ 1.0사이의 실수이기 때문이다.

### 2. output 게이트
---
![[1. CNN LSTM 개념/image2/22.png|500]]

tanh(c_t)의 각 원소에 대해 ‘그것이 다음 시각의 은닉 상태에 얼마나 중요한가’를 조정한다. 한편, 이 게이트는 다음 은닉 상태 h_t의 출력을 담당하는 게이트이므로 output 게이트라고 한다.

Output 게이트의 열림 상태(다음 몇 %만 흘려보낼까)는 입력 x_t와 이전 상태 h_(t-1)로부터 구한다. 

![[Pasted image 20240221003610.png]]

이 O는 0.0 ~ 1.0 사이의 값을 가지게 될 것이다. 게이트가 형성된 것이다.

![[1. CNN LSTM 개념/image2/23.png|500]]

즉 게이트를 얼마나 열 것인가? 에 대한 학습또한 `계층`으로 만들어서 학습시킨단 것이다. 

이게 가능한 이유는 계산 그래프의 형성은 추적을 허용한 텐서(requires_grad_)을 어떻게 계산하고 바꾸는지에 대해서 형성이 되기 떄문이다. 즉, 계층을 만들어서 텐서가 흐르기만 하면 학습의 대상이 되는 것을 응용한 것이다. 단순하게 0.0 ~ 1.0까지의 값을 내놓는 계층이지만 거기에 텐서가 흐르면 계산그래프가 형성되고 `backward()` 시키면 Loss가 감소되는 방향으로 알아서 가중치가 변경될거고, 그 결과로 얼만큼의 값을 내보낼 것인가를 학습한다.

h_t는 o와 tanh(c_t)곱으로 계산된다. 여기서 말하는 곱이란 원소별 곱이며, 이것을 아다마르 곱이라고도 한다.

![[Pasted image 20240221002358.png|]]

tanh의 출력은 -1.0 ~ 1.0의 실수이다. 이 -1.0 ~ 1.0의 수치를 그 안에 인코딩된 ‘정보’의 강약(정도)을 표시한다고 해석할 수 있다. 

한편 시그모이드 함수의 출력은 0.0 ~ 1.0의 실수이며, 데이터를 얼마만큼 통과시킬지를 정하는 비율을 뜻한다. 따라서 (주로) 게이트에서는 시그모이드 함수가, 실질적인 ‘정보’를 지니는 데이터에는 tanh 함수가 활성화 함수로 사용된다.


### 3. forget 게이트
---
![[1. CNN LSTM 개념/image2/26.png|550]]

망각은 더 나은 진전을 낳는다는 점에서 착안한 것, 우리가 다음에 해야 할 일은 기억 셀에 ‘무엇을 잊을까’를 명확하게 지시하는 것이다. 

![[1. CNN LSTM 개념/image2/24.png]]
Forget 계층 게이트의 출력 f가 구해진다. 


### 3. 기억 셀
---
![[1. CNN LSTM 개념/image2/28.png|550]]

![[1. CNN LSTM 개념/image2/27.png]]

Forget 게이트만 존재한다면 기억 셀이 잊는 것밖에 하지 못한다. 그래서 새로 기억해야 할 정보를 기억셀에 추가해야 한다. 

이떄 주의해야 하는게 이건 실질적인 정보를 추가하는 것이므로 '셀' 이다. '게이트'가 아니다. 


### 4. input 게이트
---
![[1. CNN LSTM 개념/image2/29.png|500]]

![[1. CNN LSTM 개념/image2/30.png]]

Input 게이트는 g의 각 원소가 새로 추가되는 정보로써의 가치가 얼마나 큰지를 판단한다. 새 정보를 무비판적으로 수용하는 게 아니라, 적절히 취사선택하는 것이 이 게이트의 역할이다. 다른 관점에서 보면, input 게이트에 의해 가중된 정보가 새로 추가되는 셈


### 5. LSTM의 기울기 흐름
---
![[1. CNN LSTM 개념/image2/31.png]]

이떄 'x' 노드를 보자. 이 노드는 아다마르 곱을 행했기 때문에 곱셈의 효과가 누적되지 않아서 RNN의 고질적인 문제점인 `기울기 폭발`을 고려할 필요가 없어졌다. 

‘x’의 노드의 계산은 forget 게이트가 제어한다. (매 시각 다른 게이트 값을 출력한다.) 

그리고 forget 게이트가 ‘잊어야 한다’고 판단한 기억 셀의 원소에 대해서는 그 기울기가 작아지는 것이다. 한편 forget 게이트가 ‘잊어서는 안 된다’고 판단한 원소에 대해서는 그 기울기가 약화되지 않은 채로 과거 방향으로 전해진다. 따라서 기억 셀의 기울기가 (오래 기억해야 할 정보일 경우) 소실 없이 전파되리라 기대할 수 있다.


### 6. 구현
---
![[1. CNN LSTM 개념/image2/32.png]]

이런식으로 묶어 처리하겠다. 4개의 가중치 (또는 편향)를 하나로 모을 수 있고, 그렇게 하면 원래 개별적으로 총 4번을 수행하던 아핀 변환을 단 1회의 계산으로 끝마칠 수 있다.

![[1. CNN LSTM 개념/image2/33.png]]

처음 4개분의 아핀 변환을 한꺼번에 수행한다. 그리고 slice 노드를 통해 그 4개의 결과를 꺼낸다. Slice는 아핀 변환의 결과(행렬)를 균등하게 네 조각으로 나눠서 꺼내주는 단순한 노드이다. Slice 노드 다음에는 활성화 함수(시그모이드 함수 또는 tanh 함수)를 거쳐 앞 절에서 설명한 계산을 수행한다.

