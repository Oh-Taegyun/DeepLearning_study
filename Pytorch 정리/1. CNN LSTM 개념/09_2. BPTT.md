RNN의 역전파에 대한 이론

물론 파이토치 쓸거라 의미가 없지만 이론정도는 예전에 배웠고 정리했으니까 그냥 둘러 보자

![[79.png]]

순환 구조를 펼친 후의 RNN에는 (일반적인) 오차역전파법을 적용할 수 있다. 즉, 먼저 순전파를 수행하고, 이어서 역전파를 수행하여 원하는 기울기를 구할 수 있다. 여기서의 오차역전파법은 ‘시간 방향으로 펼친 신경망의 오차역전파법’이란 뜻으로 BPTT (Backpropagation Through Time)라고 한다.

이 BPTT를 이용하면 RNN을 학습할 수 있을 듯 보인다. 하지만 그전에 해결해야 할 문제가 하나 있답니다. 그것은 바로 긴 시계열 데이터를 학습할 때의 문제이다. 이것이 왜 문제가 되는가 하면, 시계열 데이터의 시간 크기가 커지는 것에 비례하여 BPTT가 소비하는 컴퓨팅 자원도 증가하기 때문이다. 또한 시간 크기가 커지면 역전파 시의 기울기가 불안정해지는 것도 문제이다.

BPTT를 이용해 기울기를 구하려면, 매 시각 RNN계층의 중간 데이터를 메모리에 유지해두지 않으면 안 된다. (RNN 계층의 역전파는 나중에 설명). 따라서 시계열 데이터가 길어짐에 따라 계산량뿐 아니라 메모리 사용량도 증가하게 된다.


### 0. 한눈에 보는 예시
---

![[78.png|500]]

이것처럼 첫번째 단어 "h"에 관해 다음에 원하는 값이 "e"이다. 그럼 이 순간에 정답이 "e"가 맞는지 확인하고 역전파 한다. 

이 순서대로 계속 진행한다

순간순간이 문자일 경우 이렇단 거지, 만약 추론 대상이 단어벡터이면 추론 값이 단어가 된다. 단어 알파벳 하나하나씩 저런식으로 작동하지 않는단 이야기...

### 1. Truncated BPTT
---
큰 시계열 데이터를 취급할 때는 흔히 신경망 연결을 적당한 길이로 ‘끊는다’. 시간축 방향으로 너무 길어진 신경망을 적당한 지점에서 잘라내어 작은 신경망 여러 개로 만든다는 아이디어. 

그리고 이 잘라낸 작은 신경망에서 오차역전파법을 수행한다. 이것이 바로 Truncated BPTT라는 기법이다.

Truncated BPTT에서는 신경망의 연결을 끊지만, 제대로 구현하려면 ‘역전파’의 연결만 끊어야 한다. 순전파의 연결은 반드시 그대로 유지해야 한다.

그럼 Truncated BPTT를 구체적인 예를 가져와 살펴보자. 길이가 1,000인 시계열 데이터가 있다고 하자. 자연어 문제에서라면 단어 1,000개짜리 말뭉치에 해당한다. 덧붙여서, 우리가 지금까지 다룬 PTB 데이터셋에서는 여러 문장을 연결한 것을 하나의 큰 시계열 데이터로 취급했다. 여기에서도 마찬가지로 여러 문장을 연결할 것을 하나의 시계열 데이터로 취급하겠다.

![[81.png]]

그런데 길이가 1,000인 시계열 데이터를 다루면서 RNN 계층을 펼치면 계층이 가로로 1,000개나 늘어선 신경망이 된다. 물론 계층이 아무리 늘어서더라도 오차역전파법으로 기울기를 계산할 수는 있다. 하지만 너무 길면 계산량과 메모리 사용량 등이 문제가 된다. 또한, 계층이 길어짐에 따라 신경망을 하나 통과할 때마다 기울기 값이 조금씩 작아져서, 이전 시각 t까지 역전파되기 전에 0이 되어 소멸할 수도 있다.

이처럼 역전파의 연결을 잘라버리면, 그보다 미래의 데이터에 대해서는 생각할 필요가 없어진다. 따라서 각각의 블록 단위로, 미래의 블록과는 독립적으로 오차역전파법을 완결시킬 수 있다.

여기서 반드시 기억할 점은 역전파의 연결을 끊어지지만, 순전파의 연결은 끊어지지 않는다는 점이다. 그러므로 RNN을 학습시킬 때는 순전파가 연결된다는 점을 고려해야 한다. 데이터를 ‘순서대로’ 입력해야 한다는 뜻.



### 2. 어떻게 이게 가능한가?
---
뭔가 굉장히 어려워 보이는데 다른 예시를 들어주면 지극히 당연한 말이 된다. 

![[82.png]]

이렇게 잘라서 이 블록 내에서만 오차역전파법이 완결된다고 나와있다. 그럼 도대체 그 이전에 dh_9는 어디서 오는지 궁금할 것이다. 역전파를 위해서는 손실 함수의 값이 필요하고, 손실 함수의 값이 필요하다면 강화학습 같은 특수한 상황이 아니고서야 반드시 정답 레이블이 필요하다. 그럼 저 블록 까지만 정답 레이블을 주는 걸까?

역전파가 끊어짐에도 불구하고 학습이 잘 되는 이유를 알려면 가중치들이 갱신되는 원리와 학습 과정 일부를 이해한다면 알 수 있다.

위의 RNN 계층 그림도 사실 하나의 계층에 대해서 시간적으로 나타낸 것이기 때문에 본질이 하나이므로 가중치들(Wx, Wh, b)은 모두 같다.

하지만 Truncated 기법을 도입하면서 역전파가 끊어지게 되므로 가중치 조건에서 어느 정도 변화가 생긴다. 그 변화는 같은 블럭 내부의 계층들 마다의 가중치 조건은 모두 같지만 서로 다른 블럭들의 가중치 조건은 다르다는 것이다. (여기서 가중치 조건이랑 가중치 값 그 자체를 말함)

가중치 갱신은 첫 번째 블록이 블록 내부에서 순전파를 실행하고 역전파 과정을 거쳐서 기울기를 바탕으로 갱신 된 가중치 조건들이 두 번째 블록의 가중치 초기 조건으로 사용된다.

마찬가지로 두 번째 블록의 계층들이 해당 초기 가중치 조건으로 순전파를 진행한 후 다시 끝에서 처음까지(블록 안 기준) 역전파를 해서 갱신된 가중치 조건들이  세 번째 블록의 가중치 초기 조건으로 사용된다. 

![[83.png]]

예를 들어 5분짜리 영상이 있을때 이 5분짜리 영상은 큰 시계열 데이터이다. 본래 5분짜리를 통으로 학습시켜야 하는데 이걸 메모리에 그대로 올려놓으면 메모리가 그대로 죽어버린다.

따라서 이를 1분씩 끊어서 1분짜리 영상 5개로 만든다. 물론 학습시킬때는 1분을 순서대로 붙여서 5분을 학습시킨것처럼 하겠다만, 역전파의 경우 다르게 해도 좋다.

1분짜리 영상 5개 (Video1, Video2, Video3, Video4, Video5)가 있다고 하자 그럼 젤 첫번째로 Video1를 학습시키는거다. 학습시켰다는 것은 다시 말해 정답 레이블을 활용해 역전파까지 완료한 것이다!

그렇게 다시 순서대로 Video2를 학습시키고 Video3, Video4, Video5를 순차적으로 학습시켜도 문제가 없단 것이다. 

이렇게 **기존 블록의 갱신된 가중치가 다음 블록으로 넘어가서 실행되고 갱신되므로 역전파를 끊어서 사용해도 학습에 크게 지장이 없다는 것을 알 수 있다.** 

그래서 Truncated BPTT는 일반 BPTT에서 역전파 부분을 끊어주어서 계산비용을 줄여주고, 과거에 대한 정보를 기억하기 위해서 순전파는 유지를 시켜주는 형태로 구성된 것이다.


### 3. Truncated BPTT의 미니배치 학습
---
일단 잘라서 학습한다는 것은 알았다. 하지만 5분이 아닌 한 20년의 데이터라면 어떻게 해야 할까? 자르는 것도 자르는것인데 학습을 효율적으로 할려면 역시 배치처리를 해야한다.

그럼 배치 처리를 어떻게 해야할까?

![[85.png]]

정답은 데이터를 주는 시각 위치를 각 미니 배치의 시작 위치로 "옮겨주면" 된다. 

'옮긴다'라는 뜻을 보다 쉽게 이해하기 위해서 예시를 들어 보자

 길이가 1000인 시계열 데이터에 대해서 시각의 길이를 10개 단위로 잘라 Truncated BPTT로 학습하는 경우를 예로 설명하자. 이때 미니 배치의 수를 2개로 구성한다고 하면,

1. 첫 번째 미니 배치 때는 처음(0번)부터 순서대로 499번 까지 10개씩 묶는다.
2. 500번째 데이터를 시작 위치로 정하고 999번까지 10개씩 묶는다

이렇게 한다면 첫 번째 배치와 두 번째 미니 배치를 합쳐서 비로소 배치의 수가 2개인 데이터가 되는 것이다. 

그리고 미니 배치별로 데이터를 제공하는 시작위치를 옮겨준다. 이처럼 미니배치 학습을 수행할 때는 각 미니 배치의 시작 위치를 오프셋으로 옮겨준 후 순서대로 제공하면 된다. 또한 데이터를 순서대로 입력하다가 끝에 도달하면 다시 처음부터 입력하도록 하자.


좀 더 자세한 설명은 아래에 서술하겠다

---
먼저 길이가 1000개인 시계열 데이터는 무엇을 의미하는 걸까? 어렵게 생각하지 않고 단순히 1000개가 순차적으로 연결된 데이터란 의미다. 

그런데 우리가 미니배치의 수를 두 개로 나눈다고 생각하면 0 ~ 499, 500 ~ 999개로 나뉠텐데 0 ~ 499까지의 시간적인 연속성이 그 뒤의 500~900까지에도 영향을 미치기 때문이다. 

즉 시간을 두 갈래로 나뉘었다고 해서 과거의 사건이 현재의 사건이랑 별개가 아니라 서로 연관되어 있을 거란 이야기다. 0~499까지의 시간을(과거의 사건을) 무시할 수 없기에 500~999개 또한 그 과거를 바탕으로 형성되었기 때문에 연속적으로 학습을 시켜야 하기 때문

---