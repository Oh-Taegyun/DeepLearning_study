다양한 LSTM의 기술에 관해서 메모하는 공간

### LSTM 계층 다층화
---
![[Pasted image 20240213103146 1.png]]

RNNLM으로 정확한 모델을 만들고자 한다면 많은 경우 LSTM 계층을 깊게 쌓아(계층을 여러겹 쌓아) 효과를 볼 수 있다.

이와 같은 요령으로 LSTM 계층을 몇 층이라도 쌓을 수 있으며, 그 결과 더 복잡한 패턴을 학습할 수 있게 된다. 그렇다면 몇 층이나 쌓아야 할까? 에 대한 답은 모른다 이다. 이것또한 하이퍼파라미터이기 때문

### 2. 드롭아웃
---
사실 LSTM의 고유 문제 해결법은 아니긴 하다. 하지만, LSTM의 경우 계층을 다층화하면 시계열 데이터의 복잡한 의존 관계를 해결할 수 있다. 다른 말로 하면 훈련 데이터를 과적합 할 가능성이 매우 높다.

따라서 이에 따른 해결책은 다른 계층과 동등하게 드롭아웃 훈련법이 있다.

![[Pasted image 20240213103654 1.png]]

하지만 RNN에서 시계열 방향으로 드롭아웃을 넣어버리면 (학습 시) 시간이 흐름에 따라 정보가 사라질 수 있다. 즉, 흐르는 시간에 비례해 드롭아웃에 의한 노이즈가 축적된다. 노이즈 축적을 고려하면, 시간축 방향으로의 드롭아웃은 그만두는 편이 좋을 것이다.

![[Pasted image 20240213110447 1.png]]

이렇게 구성하면 시간 방향(좌우 방향)으로 아무리 진행해도 정보를 잃지 않는다. 드롭아웃이 시간축과는 독립적으로 깊이 방향(상하 방향)에만 영향을 주는 것이다.


### 3. 변형 드롭아웃
---
![[Pasted image 20240213111303 1.png]]

같은 계층에 속한 드롭아웃들은 같은 마스크를 공유한다. 여기서 말하는 ‘마스크’란 데이터의 ‘통과/차단’을 결정하는 이진 형태의 무작위 패턴을 가리킨다.

위 그림에서 보듯 같은 계층의 드롭아웃끼리 마스크를 공유함으로써 마스크가 ‘고정’된다. 그 경로가 정보를 잃게 되는 방법도 ‘고정’되므로, 일반적인 드롭아웃 때와 달리 정보가 지수적으로 손실되는 사태를 피할 수 있다.


### 4. 가중치 공유
---
![[Pasted image 20240213124512 1.png]]

언어 모델을 개선하는 아주 간단한 트릭 중 가중치 공유가 있다.

그림처럼 Embedding 계층의 가중치와 Affine 계층의 가중치를 연결하는 기법이 가중치 공유이다. 두 계층이 가중치를 공유함으로써 학습하는 매개변수 수가 크게 줄어드는 동시에 정확도도 향상되는 일석이조의 기술이다. 

Embedding 계층의 가중치는 형상이 V x H이며 Affine 계층의 가중치 형상은 H x V가 된다. 이때 가중치 공유를 적용하려면 Embedding 계층의 가중치를 전치하여 Affine 계층의 가중치로 설정하기만 하면 된다.

가중치 공유가 효과가 있는 이유
1. 직관적으로는 가중치를 공유함으로써 학습해야 할 매개변수 수를 줄일 수 있고, 그 결과 학습하기가 더 쉬워지기 떄문이라고 생각할 수 있다. 
2. 게다가 매개변수 수가 줄어든다 함은 과적합이 억제되는 혜택으로 이어질 수 있다.

