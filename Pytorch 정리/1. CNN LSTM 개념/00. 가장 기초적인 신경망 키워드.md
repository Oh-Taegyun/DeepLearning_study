
### 1. 신경망의 처리 과정 
---
![[Pasted image 20240220184157.png|200]]

1. 먼저 가중치와 편향을 기준으로 입력 신호의 총합을 구한다.
2. 활성화 함수에 넣는다 h(a)
3. 결과를 출력한다. 이 결과는 다음 뉴런의 입력값이 될 수 있다.


### 2. 활성화 함수
---
입력 신호의 총합이 활성화를 일으키는지 정하는 역할을 한다.

1. 시그모이드 함수
	![[Pasted image 20240220184312.png|400]]

2. ReLU 함수
	![[Pasted image 20240220184334.png|400]]

활성화 함수는 비선형이어야 하는데 선형 함수일 경우 층을 아무리 깊게 해도 은닉층이 없는 네트워크로 형성되기 때문이다.


### 3. 행렬
---
![[Pasted image 20240220184444.png|500]]

행렬의 장점이 나타나는 부분이다. 계산을 여러 번 해야 하는 것을 (각각 y1, y2, y3을 구했어야 함) 한번에 해결했다.


### 4. 출력층
---
회귀 – 항등 함수 (가장 큰 점수를 가진 친구를 출력)
분류 – 소프트맥스 함수 (확률적으로 교체 후 확률상 가장 높은 친구를 결정)

![[Pasted image 20240220184553.png|500]]

1) 소프트 맥스 함수의 장점
 - 소프트 맥스 함수의 출력은 항상 0에서 1사이의 값이다.
 - 소프트 맥스 함수 출력의 총합은 1이다
 - 소프트 맥스 함수 적용해도 각 원소의 대소 관계는 변하지 않는다.
 - 이러한 성질로 인해서 함수의 출력을 ‘확률’로 계산할 수 있다.

### 5. 손실함수
---
신경망 성능의 ‘나쁨’을 나타내는 지표로 현재의 신경망이 훈련 데이터를 얼마나 잘 처리하지 ‘못’하느냐를 나타낸다.

1) 오차제곱합
	
	![[Pasted image 20240220213617.png]]
	y는 신경망 출력, k는 데이터의 차원 수, t는 정답 레이블

2) 교차 엔트로피 오차
	
	![[Pasted image 20240220214342.png]]


![[스크린샷 2024-02-20 213954.png]]
![[스크린샷 2024-02-20 214016.png]]
![[스크린샷 2024-02-20 214209.png]]


### 6. 경사법
---
![[Pasted image 20240220214506.png]]

일반적으로 기울기가 가리키는 쪽은 각 장소에서 함수의 출력값을 가장 크게 줄이는 방향이다.

경사법은 현 위치에서 기울어진 방향으로 일정 거리만큼 이동한다. 그런 다음 이동한 곳에서도 마찬가지로 기울기를 구하고, 또 그 기울어진 방향으로 나아가기를 반복한다. 이렇게 해서 함수의 값을 점차 줄이는 것이 경사법이다.

경사법은 최솟값을 찾느냐 최대값을 찾느냐에 따라서 경사 하강법, 경사 상승법이라고 나타낸다.


### 7. 하이퍼파라미터
---
학습률 같은 매개변수를 하이퍼파라미터라고 한다. 가중치와 편향은 자동으로 휙득되는데 학습률과 같은 하이퍼파라미터는 사람이 직접 설정해야 하는 매개변수입니다.


### 8. 다중 분류, 이진 분류
---
1. 다중 분류의 경우, 출력층에서는 소프트맥스 함수를 손실 함수로는 교차 엔트로피 오차를 사용합니다.
2. 이진 분류의 경우, 출력층에서는 시그모이드 함수를 손실 함수로는 교차 엔트로피 오차를 사용합니다.



### 9. 신경망의 흐름
---
##### 1. 퍼셉트론

![[Pasted image 20240220222139.png|300]]

신경에 있는 뉴런의 형상을 구현화 한 것. 뉴런도 임계치가 넘어가는 전기신호를 받으면 1과 0으로 나타나고 그런 뉴런이 수백개 모여 신경을 이룬다.

##### 2. 신경망

![[Pasted image 20240220222212.png]]
퍼셉트론의 구조적 문제를 해결하기 위해 다중 퍼셉트론을 구현했다. 그리고 거기서 가중치와 활성화 함수를 적절하게 넣어 만든 것이 신경망이다. 그리고 이러한 계산을 행렬의 곱을 이용한다면

![[Pasted image 20240220222301.png|350]]
출력층은 입력층 은닉층과 다르게 회귀에는 항등 함수를 분류에는 소프트맥스 함수를 사용한다.

신경망의 순전파(추론) 하는데에는 행렬 계산이 들어간다. 근데 학습을 시키지 않으면 역시 정확도가 낮다!

##### 3. 신경망의 학습

그럼 어떤 기준으로 학습시킬까? 그에 대한 답은 손실함수를 이용한 가중치를 학습시키는 방법이다. 신경망도 하나의 지표를 기준으로 최적의 매개변수 값을 탐색해야 하는데. 신경망 학습에서 사용하는 지표는 손실함수이다.

그럼 신경망 가중치를 손실함수로 어떻게 최적화 할까? 그에 대한 답이 그레디언트에 있다. 그레디언트는 각 장소에서 함수가 가장 빨리 증가하는 방향벡터이다. 그래서 다음과 같이 학습이 가능하다.

![[Pasted image 20240220222345.png|200]]

즉 다음과 같이 손실함수에 대한 기울기를 구한다.

![[Pasted image 20240220222610.png|300]]

그 다음 이걸 토대로 가중치를 학습시키면 된다. 중요한 것은 흐름이다! 손실 함수를 흔한 오차제곱합으로 썼을 뿐

##### 4. 계산 그래프

역전파를 통해 미분을 효율적으로 계산이 가능해서 이 방법을 채택한다. 중요한 요점은 신경망의 디자인, 왜 행렬의 계산인지, 손실함수의 의미, 그리고 손실 함수를 이용한 학습방법이다.

가장 단순한 신경망 디자인은 다음과 같다.

| 신경망 디자인                          |
| -------------------------------------- |
| 1. 행렬식을 세워서 신경망을 디자인한다 |
| 2. 원하는 손실함수를 정의한다          |
| 3. 손실함수를 역전파를 이용해 학습한다 |
