Word2vec는 2개의 모델을 제안하고 있다.

하나는 CBOW 모델이고, 나머지 다른 하나는skip-gram 모델이다.

![[57.png]]

CBOW 모델은 맥락이 여러 개 있고, 그 여러 맥락으로부터 중앙의 단어(타깃)을 추측한다. 반면 Skip-gram 모델은 중앙의 단어 (타깃)로부터 주변의 여러 단어 (맥락)을 추측한다

Skip-gram 모델을 확률 표기로 나타내보

![[59.png|150]]

w_t가 주어졌을 때 w_(t-1)과 w_(t+1)이 동시에 일어날 확률을 뜻한다

![[60.png|350]]

위 식을 교차 엔트로피 오차에 적용하여 skip-gram 모델의 손실 함수를 유도하자

![[61.png|300]]

skip-gram 모델의 손실 함수는 맥락별 손실을 구한 다음 모두 더한다. 그리고 이번에도 셈플 하나짜리 skip-gram의 손실 함수이다. 이를 말뭉치 전체로 확장하면 skip-gram 모델의 손실 함수는 다음과 같다.

![[62.png|400]]

Skip-gram 모델은 맥락의 수만큼 추측하기 때문에 그 손실 함수는 각 맥락에서 구한 손실의 총합이어야 한다. 반면 CBOW 모델은 타깃 하나의 손실을 구한다.

그럼 뭐가 더 좋을까? 사실 둘 사이의 우열은 가릴 수 없는게 정답이다. 각자의 장점과 단점이 뚜렷하기 때문이다. 보통 단어 분산 표현의 정밀도 면에서 skip-gram모델의 결과가 더 좋은 경우가 많기 때문이죠

특히 말뭉치가 커질수록 저빈도 단어나 유추 문제의 성능 면에서 skip-gram 모델이 더 뛰어난 경향이 있다, 반면, 학습 속도 면에서는 CBOW모델이 더 빠르다

Skip-gram 모델은 하나의 단어로부터 그 주변 단어들을 예측한다. 이는 꽤 어려운 문제이다. 그리고 더 어려운 상황에서 단련하는 만큼 skip-gram 모델이 내어 주는 단어의 분산 표현이 더 뛰어날 가능성이 커지는 것이다

