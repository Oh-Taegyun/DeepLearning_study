책 `밑바닥부터 구현하는 딥러닝 (2)` 를 참고해서 작성 

굳이 이런것까지 알아야해? 라고 할 수 있지만, 의외로 작동방식을 아는 것은 추후 얼마든지 새로운 것을 응용해서 사용하거나, 이해속도가 아예 다른 차원의 이야기가 된다. 

### 1. 사전적 의미에서 관찰하는 RNN
---
`Recurrent` 는 순환하다 라는 라틴어이다. 즉 RNN은 순환하는 신경망이라는 것이다.

데이터가 순환하면서 정보가 끊임없이 갱신되는 아이디어를 착안한 결과물이 RNN이다. 이게 어떻게 가능할지에 대한 것은, 우리 몸을 생각해보면 된다.

우리 몸에서의 혈액은 계속 순환하면서 과거로부터 현재까지 끊임없이 갱신된다. 혹은 물은 자연 생태계를 순환하면서 과거로부터 현재까지 끊임없이 갱신된다. 혈액이 특정 구간에 고이면 썩고, 물 또한 특정 구간에 고이면 썩는다. 

CNN 계층에서 제일 중요한 점이 `3D 데이터의 올바른 처리법`이었다면 

RNN 계층에서 제일 중요한 점이 `직전 상황의 결과가 이후의 결과에 영향을 미친다` 이다. 

( 직전 상황인 `h_(t-1`)은 RNN의 결과값이고 이 결과값을 가중치와 곱해서 새로운 `h_t`를 만드는 것이다 )

직전 상황에 의해 영향을 받는다는 것은, 과거에 영향을 받는 현재라는 것이고, 이 아이디어가 RNN의 중요 아이디어이다. 따라서 RNN은 `기억하는 신경망`이 된 것이다.

### 2. RNN 계층 구현
---
![[94.png]]

![[92.png]]

![[95.png|500]]

``` python
class RNN:
    def __init__(self, Wx, Wh, b):
        self.params = [Wx, Wh, b]
        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]
        self.cache = None  # 역전파에 사용할 중간 데이터
        
    def forward(self, x, h_prev):
        Wx, Wh, b = self.params
        t = np.matmul(h_prev, Wh) + np.matmul(x, Wx) + b
        h_next = np.tanh(t) 
        
        self.cache = (x, h_prev, h_next)
        return h_next

    def backward(self, dh_next):
        Wx, Wh, b = self.params
        x, h_prev, h_next = self.cache

        dt = dh_next * (1 - h_next ** 2)  # tanh 미분
        db = np.sum(dt, axis=0)
        dWh = np.dot(h_prev.T, dt)  # shape: (H, N) x (N, H) = (H, H)
        dh_prev = np.dot(dt, Wh.T)  # shape: (N, H) x (H, H) = (N, H)
        dWx = np.dot(x.T, dt)  # shape: (D, N) x (N, H) = (D, H)
        dx = np.dot(dt, Wx.T)  # shape: (N, H) x (H, D) = (N, D)

        self.grads[0][...] = dWx
        self.grads[1][...] = dWh
        self.grads[2][...] = db

        return dx, dh_prev
```

### 3. Time RNN 계층 구현
---
![[97.png]]

``` python
class TimeRNN:
    def __init__(self, Wx, Wh, b, stateful=False):
        self.params = [Wx, Wh, b]
        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]
        self.layers = None  # RNN 계층을 리스트로 저장
        
        self.h, self.dh = None, None
        self.stateful = stateful # 은닉 상태를 유지할것인지에 대한 불리언 값

    def set_state(self, h):
        '''hidden state(h)를 설정하는 메서드'''
        self.h = h

    def reset_state(self):
        '''hidden state(h)를 초기화하는 메서드'''
        self.h = None

    def forward(self, xs):
        Wx, Wh, b = self.params
        N, T, D = xs.shape  # N(batch), T(time steps), D(input size)
        D, H = Wx.shape
        self.layers = []

        hs = np.empty((N, T, H), dtype='f')

        if not self.stateful or self.h is None: # 조건에 따라 은닉벡터를 0으로 만듦
            self.h = np.zeros((N, H), dtype='f')

        for t in range(T):
            layer = RNN(*self.params)
            self.h = layer.forward(xs[:, t, :], self.h)
            hs[:, t, :] = self.h
            self.layers.append(layer)

        return hs

    def backward(self, dhs):
        Wx, Wh, b = self.params
        N, T, H = dhs.shape
        D, H = Wx.shape

        dxs = np.empty((N, T, D), dtype='f')
        dh = 0
        grads = [0, 0, 0]

        for t in reversed(range(T)):
            layer = self.layers[t]
            dx, dh = layer.backward(dhs[:, t, :] + dh)  # 합산된 기울기
            dxs[:, t, :] = dx
            
            for i, grad in enumerate(layer.grads):
                grads[i] += grad
                
        for i, grad in enumerate(grads):
            self.grads[i][...] = grad
        self.dh = dh

        return dxs
```

`dxs = np.empty((N, T, D), dtype='f')` 에서 N은 미니배치 크기, T는 시계열 데이터의 크기, D는 입력벡터의 크기이다.

그럼 N과 T는 무슨 차이일까?

![[83.png]]

이거 보면 이해가 쉬운게...

총 데이터의 길이가 5분인데, N는 5이고 T는 1min 이다.

역전파시 주의할 점의 RNN 계층의 순전파에서는 출력이 2개로 분기된다는 것이다. 분기점도 하나의 계층이자 계산그래프가 활성화 되는 구간이라는 점을 주의하자. 

![[99.png]]

순전파때 분기했으므로 역전파에서는 각 기울기가 합산되어서 전해지는게 당연하다. 따라서 역전파 시 RNN 계층에서는 합산된 기울기 (dh_t + dh_next)가 입력된다. 

바로 여기서 다음과 같이 각 dh를 입력받아야 할 이유가 생긴 것이다.

![[98.png]]

이를 이용해서 다음과 같이 신경망을 구현할 수 있다. 

![[100.png]]
![[1. CNN LSTM 개념/image2/02.png|400]]



