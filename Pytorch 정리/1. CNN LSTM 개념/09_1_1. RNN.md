RNN(Recurrent Neural Network)의 ‘Recurrent’는 라틴어에서 온 말로, ‘몇 번이나 반복해서 일어나는 일’을 뜻한다. 그래서 RNN을 직역하면 순환하는 신경망이 되는 것이다

![[75.png|250]]

x_t 를 입력받는데 ,t는 시각을 뜻한다.

시계열 데이터(x0, x1, … , xt, …)가 RNN 계층에 입력되고 그 입력에 대응하여 (h0, h1, … , ht, … )가 출력된다.

각 시각에 입력되는 x_t 는 벡터라고 가정하자. 문장(단어 순서)을 다루는 경우를 예로 든다면 각 단어의 분산 표현(단어 벡터)이 x_t가 되며, 이 분산 표현이 순서대로 하나씩 RNN계층에 입력되는 것이다. 

위 그림에서는 출력이 2개로 분기하고 있다.

보다 자세한 작동 방식은
[[09_1_2. 밑바닥부터 구현하는 RNN]] 참고


### 0. 제일 중요한 사항
---
CNN 계층에서 제일 중요한 점이 `3D 데이터의 올바른 처리법`이었다면 

RNN 계층에서 제일 중요한 점이 `직전 상황의 결과가 이후의 결과에 영향을 미친다` 이다. 직전 상황에 의해 영향을 받는다는 것은, 과거에 영향을 받는 현재라는 것이고, 이 아이디어가 RNN의 중요 아이디어이다. 따라서 RNN은 `기억하는 신경망`이 된 것이다.

### 1. 순환 구조
---

![[76.png]]

![[1. CNN LSTM 개념/image2/09.png]]

시계열 데이터는 시간 방향으로 데이터가 나열된다. 따라서 시계열 데이터의 인덱스를 가리킬 때는 ‘시각’이라는 용어를 사용한다. (예컨대 시각 t의 입력 데이터 x_t등) 자연어의 경우에도 ‘t번째 단어’나 ‘t번째 RNN계층’이라는 표현도 사용하지만, ‘시각 t의 단어’나 ‘시각 t의 RNN 계층’처럼 표현하기도 한다.

![[77.png|250]]

RNN에서는 가중치가 2개 있다. 하나는 입력 x를 출력 h로 변환하기 위한 가중치 Wx이고, 다른 하나는 1개의 RNN출력을 다음 시각의 출력으로 변환하기 위한 가중치 Wh이다. 또한 편향 b도 있다. 참고로 ht-1과 xt는 행벡터이다.

행렬 곱을 계산하고, 그 합을 tanh함수를 이용해 변환한다. 그 결과가 시각 t의 출력 h_t가 된다. 이 h_t는 다른 계층을 향해 위쪽으로 출력되는 동시에, 다음 RNN 계층(자기 자신)을 향해 오른쪽으로 출력된다.

현재의 출력(h_t)은 한 시각 이전 출력(h_t-1)에 기초해 계산됨을 알 수 있다. 다른 관점으로 보면, RNN은 h라는 ‘상태’를 가지고 있으며, 위 식의 형태로 갱신된다고 해석할 수 있다. 그래서 RNN 계층을 ‘상태를 가지는 계층’ 혹은 ‘메모리(기억력)가 있는 계층’이라고 한다.

많은 문헌에서 RNN의 출력 h_t를 **은닉 상태 혹은 은닉 상태 벡터**라고 한다. 

RNN 계층은 시계열 데이터인 x_t를 입력하면 h_t를 출력한다. 이 ht는 RNN계층의 은닉상태라고 하며, 과거 정보를 저장한다.

![[78.png|500]]

위 그림에서 출력값이 다시 은닉벡터로 들어가는 과정이 처음 그림과 다르므로 헷갈리지 말것 둘 다 같은 의미이다.

그런데 이렇게 하면 직전의 데이터만 아는거 아닌가요? 하는데 맞다. 장기기억을 위해서는 다른 장치가 필요하고 그 장치가 반영된 것이 LSTM이다. 


### 2. 구현
---
![[87.png]]

우리가 다룰 신경망은 길이가 T인 시계열 데이터를 받는다. 그리고 각 시각의 은닉 상태를 T개 출력한다.

![[88.png]]

(x0, x1, … , xT-1)을 묶은 xs를 입력하면 (h0, h1, … , hT-1)을 묶은 hs를 출력하는 단일 계층으로 볼 수 있다. 

이때 Time RNN계층 내에서 한 단계의 작업을 수행하는 계층을 ‘RNN 계층’이라 하고, T개 단계분의 작업을 한꺼번에 처리하는 계층을 ‘Time RNN계층’이라 하자. RNN 클래스를 이용해 T개 단계의 처리를 한꺼번에 수행하는 계층을 TimeRNN이란 이름의 클래스로 완성시다.

![[89.png|250]]

데이터를 미니배치로 모아 처리한다. 따라서 xt(와 ht)에는 각 샘플 데이터를 행 방향에 저장한다. 미니배치 크기가 N, 입력 벡터의 차원 수가 D, 은닉 상태 벡터의 차원 수가 H라면, 지금 계산에서의 형상 확인은 다음과 같다.

![[90.png]]
