어텐션이라는 메커니즘 덕분에 seq2seq는 (우리 인간처럼) 필요한 정보에만 주목할 수 있게 된다. 

### 1. seq2seq의 문제점
---
Seq2seq에서는 encoder가 시계열 데이터를 인코딩한다. 그리고 인코딩된 정보를 decoder로 전달한다. 문제는 encoder의 출력은 ‘고정 길이의 벡터’ 이다. 바로 여기서 고정 길이의 벡터가 문제점이 된다.

![[Pasted image 20240222040935.png]]

아무리 긴 문장이라도 고정 길이의 벡터로 변환하는 것은 한계가 찾아올 것은 분명하다.


### 2. Encoder 개선
---
지금까지 우리는 LSTM 계층의 마지막 은닉 상태만을 Decoder에 전달했다. 그러나 Encoder 출력의 길이는 입력 문장의 길이에 따라 바꿔주는 게 좋다. 이 점이 Encoder의 개선 포인트다. 구체적으로는 시각별 LSTM 계층의 은닉 상태 벡터를 모두 이용하는 것이다.

입력 문장의 길이에 따라서 출력 길이가 자동으로 변환된다는 것이다. 이게 어떻게 가능할까? 

![[fig 8-2.png]]

각 시각(각 단어)의 은닉 상태 벡터를 모두 이용하면 입력된 단어와 같은 수의 벡터를 얻을 수 있다. 위 그림의 예에서는 5개의 단어가 입력되었고, 이때 Encoder는 5개의 벡터를 출력한다. 이것으로 Encoder는 ‘하나의 고정 길이 벡터’라는 제약으로부터 해방된다. 


![[Pasted image 20240222041709.png]]

우리는 이전까지 입력 길이를 고정시켰다. 장난감 문제만 봐도 알 수 있듯이, 일부러 빈칸까지 만들어가면서 입력길이를 고정시킨것 기억하면 된다. 그래서 은닉 벡터의 길이를 아예 고정까지 시켰다.

위랑 무슨 차이가 있냐면 은닉 벡터를 한 문장의 입력에 의한 출력으로 표시하는게 아니라, 단어 하나하나마다 은닉벡터로 뽑으면 어떤가? 이다.

그런데 주목할 것은 LSTM 계층의 은닉 상태의 ‘내용’이다. 시각별 LSTM 계층의 은닉 상태에는 어떠한 정보가 담겨 있을까? 한 가지 말할 수 있는 것은 각 시각의 은닉 상태에는 직전에 입력된 단어에 대한 정보가 많이 포함되어 있다는 사실이다. (물론 게이트로 다른 부분도 영향을 받긴 받지만 역시 제일 큰 비중을 차지하는 것은 직전 상황이다.)


![[fig 8-3.png]]

예컨대 고양이 단어를 입력했을 때의 LSTM 계층의 출력(은닉 상태)은 직전에 입력한 ‘고양이’라는 단어의 영향을 가장 크게 받는다. 따라서 이 은닉 상태 벡터는 ‘고양이’의 ‘성분’이 많이 들어간 벡터라고 생각할 수 있다. 다시 말해, Encoder가 출력하는 hs행렬은 각 단어에 해당하는 벡터들의 집합이라고 볼 수 있다.

Encoder는 왼쪽에서 오른쪽으로 처리하므로, 방금 전의 ‘고양이’ 벡터에는 정확히 총 3개 단어(‘나’, ‘는’, ‘고양이)의 정보가 담겨 있다. 그런데 전체의 균형을 생각하여 ‘고양이’ 단어의 주변 정보를 균형 있게 담아야 할 때도 있을 텐데, 그런 경우엔 시계열 데이터를 양방향 처리하는 양방향 RNN(양방향 LSTM)이 효과적이다. 


### 3. Decoder 개선
---
![[fig 8-4.png|400]]

Encoder는 각 단어에 대응하는 LSTM 계층의 은닉 상태 벡터를 hs로 모아 출력한다. 그리고 이 hs가 Decoder에 전달되어 시계열 변환이 이뤄진다.

![[fig 8-5.png]]

참고로, 장난감 문제에서 본 가장 단순한 seq2seq에서는 Encoder의 마지막 은닉 상태 벡터만을 Decoder에 넘겼다. 더 정확하게 말하면, Encoder의 LSTM 계층의 마지막 은닉 상태를 Decoder의 LSTM 계층의 첫 은닉 상태로 설정한 것이다.

그림에서 보듯 Decoder는 Encoder의 LSTM 계층의 마지막 은닉 상태만을 이용한다. hs에서 마지막 줄만 빼내어 Decoder에 전달한 것이다. 그럼 이 hs 전부를 활용할 수 있도록 Decoder를 개선하자


### 4. 어텐션의 기본 개념
---
예를 들어 영어 문장 "I am a student."을 프랑스어 문장 "Je suis étudiant."로 번역하는 상황을 생각해 보자. 출력 시퀸스의 단어 "étudiant"(프랑스어로 학생을 뜻하는 단어)는 입력 시퀸스의 단어 "i", "am", "a", "student" 중 "student"(영어로 학생을 뜻하는 단어)와 연관이 깊다. 이때 어텐션 메커니즘은 디코더가 "étudiant"를 출력하기 직전의 은닉 상태는 인코더가 "student"를 입력받은 직후의 은닉 상태와 유사할 것이라 가정한다. 

따라서 인코더가 "student"를 입력받은 직후의 은닉 상태에 조금 더 '집중'하면, 훨씬 더 품질 높은 번역을 만들 수 있을 것이다.

이 가정에 따라, Seq2Seq + 어텐션 모델에서의 디코더는 다음과 같은 순서로 다음 단어를 예측하게 된다.

1. 어느 시점의 인코더 은닉 상태에 조금 더 '집중'해야 하는지 찾기 위해, 현재 디코더의 은닉 상태와 매 시점 인코더의 은닉 상태들 간 '유사도'를 계산한다.
2. 이 유사도를 확률의 형태로 바꾸고, 그 값에 따라 인코더 은닉 상태들의 가중합(weighted sum)을 구해 '보정된 컨텍스트 벡터'를 구한다.
3. '보정된 컨텍스트 벡터'을 이용해 다음 단어를 예측한다.

이렇게 하면 Seq2Seq 모델의 두 가지 문제점이 모두 해결된다. 인코더의 마지막 은닉 상태 뿐만 아니라 인코더의 매 시점 은닉 상태들이 모두 디코더로 넘어가므로, 시퀸스의 길이가 길어지더라도 손실되는 정보가 거의 없다. 또 초기 시점의 인코더 은닉 상태와 나중의 인코더 은닉 상태가 동등하게 확률의 형태로 전달되므로, gradient vanishing/exploding 현상을 줄일 수 있다.

##### 예시: 기계 번역
어텐션 메커니즘을 도입하면, 번역 과정에서 디코더가 현재 생성하고 있는 단어에 가장 관련된 입력 단어에 "주의"를 기울일 수 있다. 예를 들어, "Le chat est sur la chaise"라는 프랑스어 문장을 "The cat is on the chair"로 번역할 때, 디코더가 "cat"이라는 단어를 생성하고 있을 때, 어텐션 메커니즘은 "chat"이라는 단어에 더 높은 가중치를 할당한다. (정확히는 cat이라는 단어가 나오게 된 은닉벡터에 초점을 맞춰서...) 이는 디코더가 해당 단어의 정확한 번역을 생성하는 데 도움이 된다고 한다.

### 5. 개선점
---
![[fig 8-6.png]]

위 그림처럼 여기에서 새롭게 어떤 계산을 수행하는 계층을 추가할 것이다. 이 어떤 계산이 받는 입력은 두가지로 하나는 Encoder로부터 받는 hs고 다른 하나는 시각별 LSTM계층의 은닉 상태이다. 그리고 여기에서 필요한 정보만 골라 위쪽의 Affine 계층으로 출력한다. 지금까지와 동일하게 Encoder의 마지막 은닉 상태 벡터는 Decoder의 첫 번쨰 LSTM계층이 전달한다.

그런데 위의 신경망으로 하고 싶은 일은 단어의 얼라인먼트 추출이다. ( 얼라인먼트 추출이란 고양이 = cat과 같은 단어의 대응 관계를 나타내는 정보이다. )

각 시각에서 Decoder에 입력된 단어와 대응 관계인 단어의 벡터를 hs에서 골라내겠다는 뜻이다. Decoder가 ‘I’를 출력할 때, hs에서 “나”에 대응하는 벡터를 선택하면 된다. 그리고 이러한 ‘선택’ 작업을 ‘어떤 계산’으로 해내겠다는 것이다. 하지만 여기서 문제가 발생한다. 바로 선택하는 작업(여러 대상으로부터 몇 개를 선택하는 작업)은 미분할 수 없다는 점이다. (임베딩 계층에서 왜 파이토치가 자동으로 계산그래프를 형성하도록 했는지 생각해봐라.)

신경망의 학습은 일반적으로 오차역전파법으로 이뤄진다. 따라서 미분 가능한 연산으로 신경망을 구축하면 오차역전파법의 틀 안에서 학습을 수행할 수 있다. 반대로 미분 가능한 연산을 이용하지 않으면 기본적으론 오차역전파법을 사용할 수 없다.

‘선택한다’ 라는 작업을 미분 가능한 연산으로 대체할 수 없을까? 사실 이 문제를 해결하는 아이디어는 간단한데 바로 하나를 선택하는게 아니라 ‘모든 것을 선택’한다는 것이다.

![[fig 8-7.png]]

여기에서는 각 단어의 중요도를 나타내는 ‘가중치’ a를 이용한다. A는 확률분포처럼 각 원소가 0.0 ~ 1.0사이의 스칼라(단일 원소)이며, 모든 원소의 총합은 1이 된다. 그리고 각 단어의 중요도를 나타내는 가중치 a와 각 단어의 벡터 hs로부터 가중합을 구하여, 우리가 원하는 벡터를 얻는다.

![[fig 8-8.png]]

그림처럼 단어 벡터의 가중합을 계산한다. 여기에서는 그 결과를 ‘맥락 벡터’라고 부르고 기호로는 c로 표기한다. 그런데 이 그림을 보면 ‘나’에 대응하는 가중치가 0.8이다. 이것이 의미하는 바는 맥락 벡터 c에는 ‘나’라는 벡터의 성분이 많이 포함되어 있다는 것이다. 즉, ‘나’벡터를 ‘선택’하는 작업을 이 가중합으로 대체하고 있다고 할 수 있다. 예컨대 ‘나’에 대응하는 가중치가 1이고 그 외에는 0이라면 ‘나’ 벡터를 ‘선택’ 한다고 해석할 수 있다.

``` python
import numpy as np

T, H = 5, 4
hs = np.random.randn(T, H)
a = np.array([0.8, 0.1, 0.03, 0.05, 0.02])

ar = a.reshape(5, 1).repeat(4, axis = 1)
print(ar.shape) # (5, 4)

t = hs * ar
print(t.shape) # (5, 4)

c = np.sum(t, axis = 0)
print(c.shape) # (4, )
```

![[fig 8-9.png]]

![[fig 8-11.png|300]]

``` python
import torch
import torch.nn as nn

class WeightSumPyTorch(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, hs, a):
        """
        hs: 입력 시퀀스, shape = (N, T, H)
        a: 주의 가중치, shape = (N, T)
        """
        # a의 shape을 (N, T, 1)로 변경
        ar = a.unsqueeze(-1)  # PyTorch에서는 unsqueeze를 사용하여 차원을 추가
        # 가중합 계산
        t = hs * ar
        c = torch.sum(t, dim=1)  # dim=1에 대해 합산

        return c

# 사용 예시
N, T, H = 10, 20, 30  # 가정: 배치 크기=10, 시퀀스 길이=20, 히든 크기=30
hs = torch.randn(N, T, H)  # 입력 시퀀스 랜덤 초기화
a = torch.rand(N, T)  # 주의 가중치 랜덤 초기화

model = WeightSumPyTorch()
c = model(hs, a)
print(c.shape)  # 결과 확인, shape = (N, H)
```


### 6. 어텐션 계층
---
각 단어의 중요도를 나타내는 가중치 a가 있다면, 가중합을 이용해 ‘맥락 벡터’를 얻을 수 있다. 그런데 이 a는 어떻게 구해야 할까? 물론, 사람이 손수 지정하는 일은 하지 않아야 한다. 그럼 각 단어의 가중치 a를 구하는 방법을 살펴보자.

![[fig 8-12.png|500]]

Decoder의 LSTM계층의 은닉 상태 벡터를 h라 했다. 지금 목표는 h가 hs의 각 단어 벡터와 얼마나 ‘비슷한가’를 수치로 나타내는 것이다. 방법은 여러 가지가 있지만, 여기에서는 가장 단순한 방법인 벡터의 ‘내적’을 이용하고자 한다.

내적의 의미는 ‘두 벡터가 얼마나 같은 방향을 향하고 있는 가’ 이다. 따라서 두 벡터의 ‘유사도’를 표현하는 척도로 내적을 이용하는 것은 자연스러운 선택이라 할 수 있다.

![[fig 8-13.png|500]]

벡터의 내적을 이용해 h와 hs의 각 단어 벡터와의 유사도를 구한다. 그리고 s는 그 결과다. 참고로 s는 정규화 하기 전의 값이며, 점수라고도 한다. 계속해서 s를 정규화 하기 위해서는 일반적으로 소프트맥스 함수를 적용한다.

![[fig 8-14.png|500]]

``` python
N, T, H = 10, 5, 4
hs = np.random.randn(N, T, H)
h = np.random.randn(N, H)
hr = h.reshape(N, 1, H).repeat(T, axis = 1)
# hr = h.reshape(N, 1, H)

t = hs * hr
print(t.shape) # (10, 5, 4)

s = np.sum(t, axis = 2)
print(s.shape) # (10, 5)

softmax = Softmax()
a = softmax.forward(s)
print(a.shape) # (10, 5)
```

![[fig 8-15.png|400]]

![[fig 8-16.png]]

``` python
import torch
import torch.nn as nn
import torch.nn.functional as F

class AttentionWeightPyTorch(nn.Module):
    def __init__(self):
        super().__init__()
        # Softmax는 PyTorch의 nn.functional에서 제공하는 함수를 사용할 수 있습니다.

    def forward(self, hs, h):
        """
        hs: 입력 시퀀스, shape = (N, T, H)
        h: 특정 타임 스텝에서의 히든 상태, shape = (N, H)
        """
        N, T, H = hs.shape

        # h의 shape을 (N, 1, H)로 변경
        hr = h.unsqueeze(1)  # PyTorch에서는 unsqueeze를 사용하여 차원을 추가
        # 주의 스코어 계산
        t = hs * hr
        s = torch.sum(t, dim=2)  # dim=2에 대해 합산
        a = F.softmax(s, dim=1)  # Softmax는 시퀀스 길이에 대해 적용됩니다.

        # 캐시는 필요 없습니다. PyTorch가 자동으로 그래디언트를 관리하기 때문입니다.
        return a

# 사용 예시
N, T, H = 10, 20, 30  # 가정: 배치 크기=10, 시퀀스 길이=20, 히든 크기=30
hs = torch.randn(N, T, H)  # 입력 시퀀스 랜덤 초기화
h = torch.randn(N, H)  # 특정 타임 스텝에서의 히든 상태 랜덤 초기화

model = AttentionWeightPyTorch()
a = model(hs, h)
print(a.shape)  # 결과 확인, shape = (N, T)
```

여태껏 구현했던 계층을 합치면 그것이 바로 어텐션 계층이다.

![[fig 8-17.png]]


``` python
import torch
import torch.nn as nn

class Attention(nn.Module):
    def __init__(self):
        super().__init__()
        self.attention_weight_layer = AttentionWeightPyTorch()  # 앞서 정의한 클래스 사용
        self.weight_sum_layer = WeightSumPyTorch()  # 앞서 정의한 클래스 사용
        # 주의 가중치를 저장하기 위한 변수

    def forward(self, hs, h):
        """
        hs: 입력 시퀀스, shape = (N, T, H)
        h: 특정 타임 스텝에서의 히든 상태, shape = (N, H)
        """
        # 주의 가중치 계산
        a = self.attention_weight_layer(hs, h)
        # 가중합을 통해 컨텍스트 벡터 계산
        out = self.weight_sum_layer(hs, a)
        return out

# 사용 예시
N, T, H = 10, 20, 30  # 가정: 배치 크기=10, 시퀀스 길이=20, 히든 크기=30
hs = torch.randn(N, T, H)  # 입력 시퀀스 랜덤 초기화
h = torch.randn(N, H)  # 특정 타임 스텝에서의 히든 상태 랜덤 초기화

model = AttentionPyTorch()
out = model(hs, h)
print(out.shape)  # 결과 확인, shape = (N, H)
```

![[fig 8-18.png]]

