---

---
파이토치에서는 `torch.nn.LSTM` 클래스를 통해 사용할 수 있다.

![[Pasted image 20240221004950.png]]


``` python
rnn = nn.LSTM(10, 20, 2)
input = torch.randn(5, 3, 10)
h0 = torch.randn(2, 3, 20)
c0 = torch.randn(2, 3, 20)
output, (hn, cn) = rnn(input, (h0, c0))

# ---------------------------------------------------------------------------------

torch.nn.LSTM(_self_, _input_size_, _hidden_size_, _num_layers=1_, _bias=True_, _batch_first=False_, _dropout=0.0_, _bidirectional=False_, _proj_size=0_, _device=None_, _dtype=None_)
```

- `N`은 배치 크기(batch size),
- `L`은 시퀀스 길이(sequence length), 내 방식으로는 `T`
- `D`는 양방향일 경우 2, 단방향일 경우 1,
- `H_in`은 입력 크기(input size),
- `H_out`은 숨겨진 크기(hidden size)를 의미합니다.

##### 입력 값
- **`input`**: 시퀀스 데이터입니다.
    - `batch_first=False` 일 때의 크기: `(L, N, H_in)`
    - `batch_first=True` 일 때의 크기: `(N, L, H_in)`
- **`h_0`**: 초기 hidden state입니다.
    - `batch_first=False` 일 때의 크기: `(D * num_layers, N, H_out)`
    - `batch_first=True` 일 때도 동일: `(D * num_layers, N, H_out)`
    - 제공되지 않으면 0으로 설정됩니다.
- `C_0`: 초기 기억셀
    - `batch_first=False` 일 때의 크기: `(D * num_layers, N, H_cell)`
    - `batch_first=True` 일 때도 동일: `(D * num_layers, N, H_cell)`
    - 제공되지 않으면 0으로 설정됩니다.
    
##### 출력 값
- **`output`**: 모든 시간 단계에서의 hidden states입니다.
    - `batch_first=False`일 때의 크기: `(L, N, D * H_out)`
    - `batch_first=True` 일 때의 크기: `(N, L, D * H_out)`
- **`h_n`**: 마지막 시간 단계에서의 hidden states입니다.
    - `batch_first=False`일 때의 크기: `(D * num_layers, H_out)`
    - `batch_first=True` 일 때도 동일: `(D * num_layers, N, H_out)`
- **`c_n`**: 마지막 시간 단계에서의 기억셀 상태입니다.
    - `batch_first=False`일 때의 크기: `(D * num_layers, H_cell)`
    - `batch_first=True` 일 때도 동일: `(D * num_layers, N, H_cell)`


### 1. 자세히 살피기
---
``` python
torch.nn.LSTM(_self_, _input_size_, _hidden_size_, _num_layers=1_, _bias=True_, _batch_first=False_, _dropout=0.0_, _bidirectional=False_, _proj_size=0_, _device=None_, _dtype=None_)
```

##### 주요 파라미터
1. **input_size**: 입력 텐서의 특징(feature)의 수입니다. 예를 들어, 단어 임베딩 벡터의 차원이 이에 해당합니다.

2. **hidden_size**: LSTM 셀의 hidden state의 크기입니다. 이는 LSTM 셀 내부에서의 메모리 용량을 결정합니다.

3. **num_layers**: LSTM 층의 수입니다. 여러 개의 LSTM 층을 쌓아 더 복잡한 표현을 학습할 수 있습니다.

4. **bias**: Boolean 값으로, bias 항을 사용할지 여부를 결정합니다. 기본값은 `True`입니다.

5. **batch_first**: Boolean 값으로, 입력 및 출력 텐서의 배치 차원이 첫 번째 차원인지 여부를 결정합니다. 기본값은 `False`이며, 이 경우 입력의 형태는 `(seq_len, batch, input_size)`가 됩니다. `True`로 설정하면 `(batch, seq_len, input_size)` 형태로 입력해야 합니다.

6. **dropout**: 0과 1 사이의 값으로, 다중 LSTM 층 사이에 적용할 드롭아웃의 비율을 결정합니다. 과적합을 방지하는 데 도움이 됩니다.

7. **bidirectional**: Boolean 값으로, 양방향 LSTM을 사용할지 여부를 결정합니다. 양방향 LSTM은 과거와 미래의 정보를 모두 학습할 수 있습니다.

##### 출력값
1. **output**: 모든 시간 단계에서의 hidden state를 포함하는 텐서입니다. 이 출력값의 형태는 `(seq_len, batch, num_directions * hidden_size)` 또는 배치가 첫 번째 차원인 경우 `(batch, seq_len, num_directions * hidden_size)`가 됩니다. 여기서 `num_directions`는 단방향 LSTM의 경우 1이고, 양방향 LSTM의 경우 2입니다.

2. **(h_n, c_n)**: 튜플 형태로, 각각 마지막 시간 단계에서의 hidden state와 cell state를 나타냅니다. `h_n`과 `c_n`의 형태는 각각 `(num_layers * num_directions, batch, hidden_size)`입니다.

