RNN의 경우 시계열 데이터의 장기 기억이 어렵다. 생각해보면 당연하다. 아무리 이전 상황의 결과가 현재에 미친다고 한들 그것은 직전의 이야기이고 시간이 지나면 지날수록 한참 예전의 상황이 바로 직전의 상황에 가려질 테니까

이걸 해결한것이 [[11_1. LSTM]]이다.


### 1. 기울기 소실 문제
---
RNN의 문제라기보다 `tanh(x)`의 고질적인 문제점

![[1. CNN LSTM 개념/image2/12.png|500]]

이 경우의 문제는 단순하게 기울기 소실 문제 대책을 꺼내들면 된다. 그렇다 그냥 `ReLU` 쓰란 이야기


### 2. 행렬 곱 문제
---
행렬 곱 자체가 문제가 되진 않지만 RNN 같은 경우는 문제가 된다.

![[1. CNN LSTM 개념/image2/13.png]]

가중치 `W_h`가 지속적으로 같은 값이 쓰이는게 문제다. 일반적인 계층의 경우 각 계층별로 가중치를 소유하고 있어서 문제가 되진 않지만, RNN 계층의 경우 가중치를 공유하는 형태이기 떄문에 문제가 된다. (물론 아예 서로 다른 RNN끼리는 문제가 되지 않는데 RNN을 여러개 쓰면 RNN을 쓰는 이유가....?)

![[1. CNN LSTM 개념/image2/14.png]]

결국 이런식으로 기울기가 기하급수적으로 증가하거나 감소한다. (기울기 폭발)

증가, 감소의 판단은 행렬의 `특잇값`이 좌우하게 된다. 

행렬의 특잇값이란, 간단히 말하면 데이터가 얼마나 퍼져 있는지를 나타낸다. 이 특잇값의 값(더 정확하게는 여러 특잇값 중 최댓값)이 1보다 큰지 여부를 보면 기울기 크기가 어떻게 변할지 예측할 수 있다.


### 3. 클리핑 - 기울기 폭발 대책
---
![[1. CNN LSTM 개념/image2/16.png]]

클리핑이라는 전통적인 기법으로 기울기 폭발 예방이 가능하다.

g는 신경망에서 사용되는 모든 매개변수의 기울이이다. 예컨대 두 개의 가중치 W1과 W2 매개변수를 사용하는 모델이 있다면, 이 두 매개변수에 대한 기울기 dW1과 dW2를 결합한 것을 g이라 하는 것)

그리고 threshold를 문턱값으로 설정한다. 이때 기울기의 L@노름이 문턱값을 초과하면 두 번쨰 줄의 수식과 같이 기울기를 수정한다.

![[1. CNN LSTM 개념/image2/17.png]]

파이토치에는 이렇게 존재한다

1. `clip_grad_norm_`:
   - 이 함수는 네트워크의 파라미터들에 대한 그래디언트의 L2 노름이 주어진 임계값을 초과하지 않도록 클리핑합니다. 클리핑은 모든 그래디언트에 대해 동시에 적용되므로, 모든 파라미터의 그래디언트를 유지하면서 비례적으로 크기를 줄입니다.
   - `clip_grad_norm_` 함수는 파라미터에 대해 in-place로 작동하여 원본 파라미터의 그래디언트를 직접 수정합니다.

2. `clip_grad_norm`:
   - 이 함수는 PyTorch의 이전 버전에서 사용되었던 함수이며, `clip_grad_norm_` 함수의 기능과 유사합니다. 그러나 최신 버전의 PyTorch에서는 `clip_grad_norm_` 함수를 사용하는 것이 권장됩니다.
   - PyTorch의 일관성 있는 네이밍 컨벤션에 따라, 뒤에 `_`가 붙는 함수는 in-place 연산을 수행함을 나타냅니다.

3. `clip_grad_value_`:
   - 이 함수는 파라미터의 그래디언트가 주어진 최대 절대값을 초과하지 않도록 클리핑합니다. 즉, 개별 그래디언트 값이 지정된 최대 절대값 이상이거나 이하로 설정되지 않도록 각각의 그래디언트 값을 독립적으로 클리핑합니다.
   - `clip_grad_value_` 또한 파라미터에 대해 in-place로 작동하여 원본 파라미터의 그래디언트를 직접 수정합니다.

이라고 한다.

``` python
torch.nn.utils.clip_grad_norm_(parameters, clip_value, norm_type=2)
```
##### 파라미터
- `parameters` (Iterable[Tensor] 또는 Tensor): 클리핑을 적용할 파라미터들을 포함하는 iterable 또는 단일 텐서입니다. 일반적으로 이는 `model.parameters()`를 통해 얻을 수 있습니다.
- `max_norm` (float 또는 int): 그래디언트의 최대 L2 노름입니다. 이 값은 그래디언트의 최대 허용 크기를 정의합니다.
- `norm_type` (float 또는 int, optional): L2 노름을 계산하는데 사용되는 노름의 종류입니다. 기본값은 2입니다, 즉 L2 노름입니다. `norm_type`이 1이면 L1 노름, 'inf'면 무한 노름을 사용합니다.

##### 출력값
- `clip_grad_norm_` 함수는 클리핑된 그래디언트의 총 노름을 반환합니다. 이 값은 클리핑 연산 전에 그래디언트의 노름을 계산한 것입니다.

``` python
import torch
from torch import nn
from torch.nn.utils import clip_grad_norm_

# 임의의 모델 정의
model = nn.Linear(10, 2)

# 모델의 파라미터에 대한 임의의 그래디언트 설정
for p in model.parameters():
    p.grad = torch.randn_like(p)

# 클리핑할 최대 노름 설정
max_norm = 2.0

# 그래디언트 클리핑 적용
total_norm = clip_grad_norm_(model.parameters(), max_norm)

# 클리핑 후의 그래디언트 노름 확인
print("Total gradient norm after clipping:", total_norm)
```



