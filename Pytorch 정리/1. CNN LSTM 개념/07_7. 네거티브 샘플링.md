![[63.png]]

은닉층 이후에서 계산이 오래 걸리는 곳은 다음의 두 부분이다. 

1. 은닉층의 뉴련과 가중치 행렬 (W_out)의 곱

2. Softmax 계층의 계산

은닉층의 벡터 크기가 100이고, 가중치 행렬의 크기가 100 x 100만 이다. 

![[64.png|150]]

이 식에서는 어휘 수를 100만 개로 가정했으므로 분모의 값을 얻으려면 exp 계산을 100만 번 수행해야 한다

### 1. 다중 분류에서 이진 분류로
---
다중 분류를 이진 분류로 근사하는 것이 네거티브 샘플링을 이해하는 데 중요한 포인트이다. 

이진 분류는 “Yes/No”로 답하는 문제를 다룬다. 우리가 생각해야 할 것은 ‘다중 분류’ 문제를 ‘이진 분류’ 방식으로 해결하는 것이다. 

그러기 위해서는 “yes/No”로 답할 수 있는 질문을 생각해내야 한다. 이렇게 하면 출력층에는 뉴런을 하나만 준비하면 된다.

![[65.png]]

출력층의 뉴런은 하나뿐이다. 따라서 은닉층과 출력 측의 가중치 행렬의 내적은 “say”에 해당하는 열(단어 벡터)만을 추출하고, 그 추출된 벡터와 은닉층 뉴런과의 내적을 계산하면 끝이다.

![[66.png]]

그림처럼 출력 측 가중치 W_(out)에서는 각 단어 ID의 단어 벡터가 각각의 열로 저장되어 있다. 이 예에서는 “say”에 해당하는 단어 벡터를 추출한다. 그리고 그 벡터와 은닉층 뉴런과의 내적을 구한다. 이렇게 구한 값이 최종 점수가 되는 것이다.

이전까지의 출력층은 모든 단어를 대상으로 계산을 수행했다. 하지만 여기에서는 “say”라는 단어 하나에 주목하여 그 점수만을 계산하는 게 차이이다.


### 2. 시그모이드 함수와 교차 엔트로피 오차
---
이진 분류 문제를 신경망으로 풀려면 점수에 시그모이드 함수를 적용해 확률로 변환하고, 손실을 구할 때는 손실 함수로 ‘교차 엔트로피 오차’를 사용한다.

다중 분류의 경우, 출력층에서는 (점수를 확률로 변환할 때) ‘소프트 맥스 함수’를 손실 함수로는 ‘교차 엔트로피 오차’를 이용한다. 이진 분류의 경우, 줄력층에서는 ‘시그모이드 함수’를, 손실 함수로는 ‘교차 엔트로피 오차’를 이용한다.

![[67.png|300]]

y는 시그모이드 함수의 출력이고, t는 정답 레이블이다. 이 정답 레이블의 값은 0 혹은 1이다.


### 3. 모델 
---
![[69.png]]

생각을 해봐야 하는게 W_out에서 say를 뽑는건 타당하다. (앞서 데이터를 보면...) 하지만 이 선택을 누가 하는가?

신경망이 스스로 했으면 좋겠지만 그 경우 단어의 분산 표현을 얻는다는 목적과는 거리가 멀어진다.

따라서 이 경우 사용자가 직접 정답 레이블을 넣어준 셈이다. 정답 레이블을 신경망 순전파에 2번이나 넣은 셈

### 3. 네거티브 샘플링
---
지금까지 배운 것으로 주어진 문제를 ‘다중 분류’에서 ‘이진 분류’로 변환할 수 있다. 하지만 안타깝게도 이것만으로는 문제가 다 해결되지 않는다. 

지금까지는 긍정적인 예(정답)에 대해서만 학습했기 때문이다. 다시 말해 부정적인 예(오답)를 입력하면 어떤 결과가 나올지 확실하지 않는다.

![[68.png]]

현재의 신경망에서는 긍정적 예인 ‘say’에 대해서만 학습하게 된다. 그러나 부정적 예 (‘say’이외의 단어)에 대해서는 어떠한 지식도 휙득하지 못했다

긍정적 예에 대해서는 sigmoid 계층의 출력을 1에 가깝게 만들고, 부정적 예에 대해서는 sigmoid 계층의 출력을 0에 가깝게 만드는 것이 중요하다.

![[70.png]]

모든 부정적 예를 대상으로 하는 방법은 어휘 수가 늘어나면 감당할 수 없기 때문에(어휘 수 증가에 대처하는 것이 이번 장에서의 목적이었다) 근사적인 해법으로, 부정적 예를 몇 개 (5개라든지, 10개라든지) 선택한다. 

즉, 적은 수의 부정적 예를 샘플링해 사용한다. 이것이 바로 ‘네거티브 샘플링’ 기법이 의미하는 바이다. 네거티브 샘플링 기법은 긍정적 예를 타깃으로 한 경우의 손실을 구한다. 그와 동시에 부정적 예를 몇 개 샘플링(선별)하여, 그 부정적 예에 대해서도 마찬가지로 손실을 구한다. 그리고 각각의 데이터(긍정적 예와 샘플링된 부정적 예)의 손실을 더한 값을 최종 손실로 한다.

![[71.png]]

부정적 예에 대해서는 sigmoid with Loss 계층에 정답 레이블로 0을 입력한다.

### 4. 네거티브 샘플링의 샘플링 기법
---
부정적 예를 어떻게 샘플링 할까? 단순히 무작위로 샘플링하는 것보다 말뭉치의 통계 데이터를 기초로 샘플링하는 방법이 좋다. 구체적으로 말하면, 말뭉치에서 자주 등장하는 단어를 많이 추출하고 드물게 등장하는 단어를 적게 추출하는 것이다. 말뭉치에서의 단어 빈도를 기준으로 샘플링하려면, 먼저 말뭉치에서 각 단어의 출현 횟수를 구해 ‘확률분포’로 나타낸다. 그런 다음 그 확률분포대로 단어를 샘플링 하면 된다.

![[72.png|500]]

확률 분포대로 샘플링 하므로 말뭉치에서 자주 등장하는 단어는 선택될 가능성이 높다. 같은 이유로, ‘희소한 단어’는 선택되기가 어렵다. 드문 단어를 잘 처리하는 일은 중요도가 낮다. 그보다는 흔한 단어를 잘 처리하는 편이 좋은 결과로 이어질 것이다.

그런데 word2vec의 네거티브 샘플링에서는 앞의 확률분포에서 한 가지를 수정하라고 권고하고 있다. 기본 확률분포에 0.75를 제곱하는 것이다

![[73.png|200]]

수정 후에도 확률의 총합이 1이 되어야 하므로, 분모로는 ‘수정 후 확률 분포의 총합’이 필요하다.

이렇게 수정하는 이유는 출현 확률이 낮은 단어를 아예 버리지 않기 위해서이다

참고로 0.75라는 수치에는 이론적인 의미는 없으므로 다른 값으로 수정해도 좋다.



