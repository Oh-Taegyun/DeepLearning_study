통계 기반 기법에서는 주변 단어의 빈도를 기반으로 단어를 표현한다. 구체적으로는 단어의 동시발생 행렬을 만들고, 그 행렬에 SVD를 적용해 밀집벡터(단어의 분산 표현)를 얻는다. 하지만 이 방식은 대규모 말뭉치를 다룰 때 문제가 발생한다.

현업에서는 말뭉치의 어휘 수가 100만을 넘기도 한다. 어휘가 100만개라면 통계 기반 기법에서는 '100만 x 100만'이라는 거대한 행렬을 만들어야 하고, 이 거대한 행렬을 SVD에 넣어야 한다. SVD를 n x n 행렬에 적용하는 비용은 O(n³)이기 때문에, 심지어 슈퍼컴퓨터를 사용해도 처리하기 어려운 수준이다.

통계 기반 기법은 말뭉치 전체의 통계(동시발생 행렬과 PPMI 등)를 이용해 단 한 번의 처리(SVD 등)로 단어의 분산 표현을 얻는다. 반면, 추론 기반 기법에서는 예를 들어 신경망을 사용하는 경우, 미니배치로 학습하는 것이 일반적이다. 미니배치 학습에서는 신경망이 한 번에 소량(미니배치)의 학습 샘플씩 반복해서 학습하며 가중치를 갱신해간다.

![[43.png]]

통계 기반 기법은 학습 데이터 전체를 한꺼번에 학습한다(배치 학습). 

![[44.png|400]]

반면, 추론 기반 기법은 학습 데이터의 일부를 사용해 순차적으로 학습한다(미니배치 학습).

이는 말뭉치의 어휘 수가 많아 SVD와 같은 계산량이 큰 작업을 처리하기 어려운 경우에도 신경망을 학습시킬 수 있다는 것을 의미한다. 

데이터를 작게 나누어 학습하기 때문이다. 또한 여러 머신과 여러 GPU를 이용한 병렬 계산도 가능해져 학습 속도를 높일 수 있다. 이러한 이유로 추론 기반 기법은 큰 힘을 발휘하는 영역이다.

간단히 모든 말뭉치를 그냥 처리해서 단어의 분산 표현을 얻는 경우가 통계 기반 기법
신경망을 활용해서 단어의 분산 표현을 얻는 경우가 추론 기반 기법이다.


### 1. 신경망에서의 단어 처리를 하는 법
---
신경망은 "you", "say" 등의 단어를 그대로 처리할 수 없기 때문에, 단어를 '고정 길이의 벡터'로 변환해야 한다. 이를 위해 사용하는 대표적인 방법이 단어를 원핫 표현(또는 원핫 벡터)으로 변환하는 것이다. 원핫 표현이란 벡터의 원소 중 하나만 1이고 나머지는 모두 0인 벡터를 말한다. 이 방식을 통해 각 단어는 고유한 벡터로 표현되며, 이 벡터는 신경망에서 입력으로 사용될 수 있다.

![[45.png|500]]
![[46.png|500]]

이렇게 된다면 신경망은 이제 벡터를 처리할 수 있고 그 벡터는 단어이다.
이러한 기법을 word2vec라고 한다.

![[47.png|600]]


### 2. 단순한 woed2vec
---
CBOW 모델은 맥락으로부터 타깃을 추측하는 용도의 신경망이다. (‘타깃’은 중앙 단어이고 그 주변 단어들이’맥락’이다) 

CBOW의 모델의 입력은 맥락이다. 맥락은 “you”와 “goodbye”같은 단어들의 목록이다

![[48.png|500]]

이 그림에서 입력층이 2개인 이유는 맥락으로 고려할 단어를 2개로 정했기 때문이다. 즉, 맥락에 포함시킬 단어가 N개라면 입력층도 N개가 된다.

은닉층의 뉴런은 입력층의 완전연결계층에 의해 변환된 값이 되는데, 입력층이 여러 개이면 전체를 ‘평균’하면 된다. 

왼쪽의 경우 완전연결계층에 의한 첫 번째 입력층이 h1으로 변환되고, 두번째 입력층이 h2로 변환되었다고 한다면, 은닉층 뉴런은 1/2(h1 + h2)가 되는 것

![[49.png|400]]

Win의 각 행에는 해당 단어의 분산 표현이 담겨 있다고 볼 수 있다. 따라서 학습을 진행할수록 맥락에서 출현하는 단어를 잘 추측하는 방향으로 이 분산 표현들이 갱신될 것이다. 그리고 놀랍게도 이렇게 해서 얻은 벡터에는 ‘단어의 의미’도 잘 녹아들어 있다

은닉층의 뉴런 수를 입력층의 뉴런 수보다 적게 하는 것이 중요한 핵심이다. 이렇게 해야 은닉층에는 단어 예측이 필요한 정보를 ‘간결하게’ 담게 되며, 결과적으로 밀집벡터 표현을 얻을 수 있다.

이때 입력층 측의 가중치는 서로 공유한다는 점에 주의

참고로 정답 레이블을 썼으므로 손실 함수는 `교차 엔트로피 오차` 이다.

![[50.png]]

### 3. word2vec의 가중치와 분산 표현
---

![[51.png]]

최종적으로 이용하는 단어의 분산 표현으로는 어느 쪽 가중치를 선택해도 무관하다 선택지는 다음과 같다.

A 입력 측의 가중치만 이용한다
B 출력 측의 가중치만 이용한다
C 양쪽 가중치를 모두 이용한다

Word2vec(특히 skip-gram모델) 에서는 A안인 ‘입력 측 가중치만 이용한다’가 가장 대중적인 선택이다
