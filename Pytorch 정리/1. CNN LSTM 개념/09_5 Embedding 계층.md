거대한 행렬 계산의 문제점을 해결하기 위해 나온 계층이다. 만약 원 핫 벡터 표현을 처리하기 위해서 다음과 같은 과정을 거쳐야 한다고 생각해보자 

![[fig 4-3.png]]

행렬 계산을 해보면 알겠지만, 그냥 단순하게 '1'이라고 표시된 부분을 추출하는 것 뿐이다. 그런데 문제는 컴퓨터는 직접 다 계산을 한다는 것이 문제이다. 이러한 문제를 해결하기 위해 존재하는 계층이다. 

단순하게 가중치 매개변수로부터 "입력 원 핫 벡터 ID에 해당하는 행(벡터)"를 추출하는 계층이란 것이다. 즉, 이 임베딩 계층에는 어떤 원 핫 벡터에 대한 가중치 표현이 담겨져 있고 이를 컨트롤 하는 계층이 임베딩 계층이다. 


### 1. 파이토치의 임베딩 계층
---
PyTorch의 `Embedding` 계층은 주로 자연어 처리(NLP) 작업에서 단어, 문자 또는 기타 유형의 토큰을 저차원의 연속 벡터로 매핑하는 데 사용된다. 이러한 벡터를 임베딩이라고 하며, 비슷한 의미를 가진 단어는 비슷한 벡터 표현을 갖도록 학습된다. 간단하게 말해서 단어의 분산 표현을 얻는것을 말한다 이 단어의 분산 표현으로 우리는 비슷한 단어를 유추하는 기능을 얻을 수 있었다. 

`Embedding` 계층은 사전 훈련된 임베딩을 사용하거나, 특정 작업에 대해 처음부터 학습될 수 있다.


### 2. 기본 사용법
---
``` python
import torch
import torch.nn as nn

# 임베딩 계층 생성
embedding = nn.Embedding(num_embeddings=10, embedding_dim=3)
```

이 계층을 초기화할 때 주요 인자로는 임베딩을 수행할 단어의 총 개수(`num_embeddings`)와 임베딩 벡터의 차원(`embedding_dim`)이 필요하다. 당연히 분산 표현을 관리하는 계층이니, 단어가 총 몇개있는지는 매우 중요하다. 

##### 입력
임베딩 계층의 입력은 일반적으로 정수 인덱스로 구성된 텐서입니다. 이 인덱스는 보통 단어의 ID나 위치를 나타내며, 각 단어는 사전에 정의된 어휘집(vocabulary) 내의 유니크한 정수에 매핑됩니다. 입력 텐서의 형태는 `[*,]`로, 여기서 `*`는 어떤 차원도 될 수 있으며, 이는 배치 크기나 시퀀스 길이를 포함할 수 있습니다.

##### 출력
`nn.Embedding` 계층의 출력은 다음과 같은 특징을 가집니다:

- **출력 크기**: `[*, embedding_dim]`.
- 여기서 `*`은 입력 텐서의 형태를 나타내며, `embedding_dim`은 생성자에서 지정한 임베딩 벡터의 차원입니다. 입력이 단일 정수 인덱스인 경우, 출력은 단일 임베딩 벡터가 됩니다. 입력이 정수 인덱스의 시퀀스(예: 단어 인덱스의 리스트)인 경우, 출력은 해당 인덱스에 매핑된 임베딩 벡터의 시퀀스가 됩니다.



##### 주요 파라미터

1. **`num_embeddings` (int)**: 임베딩을 수행할 범주형 데이터의 고유한 항목 수, 즉 임베딩 테이블에 있는 항목의 총 개수입니다. 예를 들어, 단어 집합의 크기가 10,000개라면 `num_embeddings`는 10,000이 됩니다.
    
2. **`embedding_dim` (int)**: 각 범주형 항목을 매핑할 임베딩 벡터의 차원입니다. 예를 들어, 각 단어를 300차원 벡터로 변환하고 싶다면 `embedding_dim`은 300이 됩니다.
    
3. **`padding_idx` (int, optional)**: 선택적으로 지정할 수 있는 매개변수로, 패딩(padding)에 사용될 인덱스를 지정합니다. 이 인덱스에 해당하는 임베딩 벡터는 0으로 초기화되며, 모델 학습 과정에서 업데이트되지 않습니다. 주로 시퀀스 데이터를 배치로 처리할 때 길이를 맞추기 위해 사용됩니다.
    
4. **`max_norm` (float, optional)**: 임베딩 벡터의 최대 노름을 지정합니다. 이 값을 설정하면, `max_norm`보다 큰 노름을 가진 임베딩 벡터는 재조정되어 이 값을 초과하지 않게 됩니다.
    
5. **`norm_type` (float, optional)**: `max_norm` 옵션을 사용할 때, 노름의 종류를 지정합니다. 기본값은 2.0으로 L2 노름을 의미합니다.
    
6. **`scale_grad_by_freq` (boolean, optional)**: `True`로 설정되면, 그라디언트를 단어의 빈도에 비례하여 스케일링합니다. 데이터셋에서 자주 등장하는 단어의 그라디언트가 너무 커지는 것을 방지합니다.
    
7. **`sparse` (bool, optional)**: `True`로 설정되면, 내부적으로 스파스 텐서를 사용하여 임베딩 그라디언트를 저장하고 업데이트합니다. 이는 메모리 사용량을 줄이지만, 성능에 영향을 줄 수 있습니다.


### 3. 예시
---
``` python
# an Embedding module containing 10 tensors of size 3
embedding = nn.Embedding(10, 3)
# a batch of 2 samples of 4 indices each
input = torch.LongTensor([[1, 2, 4, 5], [4, 3, 2, 9]]) # (2,4)
embedding(input).shape # torch.Size([2, 4, 3])
embedding(input)
tensor([[[-0.0251, -1.6902,  0.7172],
         [-0.6431,  0.0748,  0.6969],
         [ 1.4970,  1.3448, -0.9685],
         [-0.3677, -2.7265, -0.1685]],

        [[ 1.4970,  1.3448, -0.9685],
         [ 0.4362, -0.4004,  0.9400],
         [-0.6431,  0.0748,  0.6969],
         [ 0.9124, -2.3616,  1.1151]]])
```


### 4. 참고
---
참고로 임베딩 계층에 들어가는 것은 정수인데, `.requries_grad()`를 설정을 어떻게 하느냐? 실수가 아닌데 이게 되느냐? 라고 할 수 있는데, 다행히도 이를 위해 파이토치에서는 굳이 입력값에 `.requries_grad()`를 설정하지 않아도 자동으로 중간 텐서부터 `.requries_grad()`를 설정해준다. 

임베딩 계층은 입력값에 대한 계산그래프보다는, 임베딩 케이블 내의 가중치가 중요하다. 파이토치에서는 다행이 이 가중치에 대해서 자동으로 `.requries_grad()`가 설정된다. 따라서 임베딩 계층을 앞에서 사용하는 경우 그냥 정수형태의 텐서만 알아서 흘러보내면 자동으로 계산그래프를 형성해준다. 굳이굳이 우리가 계산그래프를 설정해야 할 필요가 없어서 편한 장점이 있다.

