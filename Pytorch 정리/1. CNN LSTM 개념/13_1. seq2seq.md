시계열 데이터를 다른 시계열 데이터로 변환하는 모델로 seq2seq를 살펴보자. 이를 위한 기법으로 2개의 RNN을 이용하는 방법을 사용한다.

![[Pasted image 20240213132900.png|400]]

Seq2seq를 encoder-Decoder 모델이라고도 한다. 이름이 말해주듯 여기에는 2개의 모듈, encoder와 decoder가 등장한다.


### 1. Encoder
---
Encoder가 인코딩한 정보에는 번역에 필요한 정보가 조밀하게 응축되어 있다.

![[Pasted image 20240213133014.png]]

Encoder가 출력하는 벡터 h는 LSTM 계층의 마지막 은닉 상태이다. 이 마지막 은닉 상태 h에 입력 문장(출발어)을 번역하는 데 필요한 정보가 인코딩된다. 여기서 중요한 점은 LSTM의 은닉 상태 h는 고정 길이 벡터라는 사실이다. 그래서 인코딩한다라 함은 결국 임의 길이의 문장을 고정 길이 벡터로 변환하는 작업이 된다.

![[Pasted image 20240213154902.png|400]]


### 2. Decoder
---
decoder는 이 인코딩된 벡터를 해석해 도착어 문장을 생성한다.

![[Pasted image 20240213155030.png]]

`<eos>`라는 구분 기호(특수 문자)는 구분자이며 decoder의 문장의 시작, 문장의 종료 신호로 이용된다.

Decoder는 일반 LSTM과 같은 모델이다. 단지 바로 LSTM 계층이 벡터 h를 입력받는다는 점이 다르다. 참고로, LSTM 언어 모델에서는 LSTM 계층이 아무것도 입력 받지 않았다. 이처럼 단 하나의 사소한 차이가 평범한 언어 모델을 번역도 가능하게 하는 decoder로 탈바꿈 시킨다.

![[Pasted image 20240213155217.png]]

이때 LSTM 계층의 은닉 상태가 encoder와 decoder를 이어주는 ‘가교’가 된다. 순전파 때는 encoder에서 인코딩된 정보가 LSTM 계층의 은닉 상태를 통해 decoder에 전해진다. 그리고 seq2seq의 역전파 떄는 이 ‘가교’를 통해 기울기가 decoder로부터 encoder로 전해진다.

