
### 3. 파이토치에서의 RNN
---
![[90.png]]

``` python
rnn = nn.RNN(10, 20, 2)  # input_size=10, hidden_size=20, num_layers=2
input = torch.randn(5, 3, 10)  # seq_len=5, batch=3, input_size=10
h0 = torch.randn(2, 3, 20)  # num_layers=2, batch=3, hidden_size=20
output, hn = rnn(input, h0)
```

- `N` = 배치 크기
- `L` = 시퀀스 길이
- `D` = 양방향(bidirectional=True)이면 2, 그렇지 않으면 1
- `H_in` = 입력 크기
- `H_out` = 숨겨진 크기

##### 입력 값:
1. `input`
	- batch_first=False 일 때 (L, N, H_in)
	- batch_first=True 일 때 (N, L, H_in)

2. `h_0` 
	- batch_first=False - (D * num_layers, N, H_out)
	- batch_first=True - (D * num_layers, N, H_out)
	- 제공되지 않으면 기본적으로 0으로 설정됩니다.

##### 출력 값:
1. `output` : 모든 시간 단계에서의 hidden states입니다.
	- batch_first=False일 때 (L, N, D * H_out)
	- batch_first=True 일 때 (N, L, D * H_out) 

3. `h_n` : 마지막 시간 단계에서의 hidden states입니다.
	- batch_first=False일 때 (D * num_layers, N, H_out)
	- batch_first=True 일 때 (D * num_layers, N, H_out)
	- 배치의 각 요소에 대한 최종 숨겨진 상태를 포함한다.
	- 왜 배치별로 숨겨진 상태를 저장해야하죠? -> 그래야 계산 그래프 따라 역전파를 하지...


---

입력 시퀀스에 tanh 또는 ReLU 비선형성을 적용한 다층 RNN을 적용한다. 입력 시퀀스의 각 요소에 대해, 각 층은 다음 함수를 계산한다

![[91.png]]

여기서 h_t는 시간 t에서의 은닉 상태이고, x_t는 시간 t에서의 입력이며, ℎ_(t-1)은 시간 t-1에서 이전 층의 은닉 상태 또는 시간 0에서의 초기 은닉 상태이다. 

비선형성이 'relu'인 경우, tanh 대신 ReLU가 사용된다.

---
##### 주요 파라미터:
1. `input_size`: 입력 특성의 개수입니다. 예를 들어, 단어 임베딩 벡터의 크기 또는 한 시간 단계에 입력되는 특성의 수를 의미합니다.

2. `hidden_size`: RNN 셀의 출력 및 숨겨진 상태의 차원입니다. 이는 RNN 셀 내부의 숨겨진 레이어의 크기를 결정합니다.

3. `num_layers`: RNN의 층 수입니다. 기본적으로 1층이지만, 여러 층을 쌓아 더 복잡한 모델을 만들 수 있습니다. 일반적으로 5나 3을 사용합니다. 

4. `nonlinearity`: 비선형 활성화 함수의 종류입니다. 기본적으로 `tanh`가 사용되지만, `ReLU` 등 다른 함수로 변경할 수 있습니다(단, 일부 RNN 변형에서만 해당됩니다).

5. `bias`: 레이어에 편향(bias)을 사용할지 여부를 결정합니다. 기본값은 `True`입니다.

6. `batch_first`: 입력 및 출력 텐서의 배치 차원이 첫 번째 차원으로 오게 할지 여부를 결정합니다. 기본값은 `False`이며, 이 경우 입력의 형태는 `(seq_len, batch, input_size)`가 됩니다. `True`로 설정하면 입력의 형태는 `(batch, seq_len, input_size)`가 됩니다.

7. `dropout`: 0이 아닌 경우, 출력 레이어 사이에 드롭아웃을 추가하여 과적합을 방지합니다. 기본값은 0이며, 이는 드롭아웃이 없음을 의미합니다.

8. `bidirectional`: 양방향 RNN을 사용할지 여부를 결정합니다. 기본값은 `False`입니다. `True`로 설정하면, 양방향 RNN이 생성되어, 각 시간 단계에서 앞뒤 방향의 정보를 모두 학습할 수 있습니다.

---


### 형상 맞추기
---
``` python
rnn = nn.RNN(10, 20, 2)  # input_size=10, hidden_size=20, num_layers=2
input = torch.randn(5, 3, 10)  # seq_len=5, batch=3, input_size=10
h0 = torch.randn(2, 3, 20)  # num_layers=2, batch=3, hidden_size=20
output, hn = rnn(input, h0)
```

![[Pasted image 20240221001959.png]]

PyTorch의 `nn.RNN` 모듈을 사용하여 RNN을 구현할 때
1. 입력 텐서의 형상은 `(seq_len, batch, input_size)
2. 숨겨진 상태 텐서의 형상은 `(num_layers * num_directions, batch, hidden_size)`

여기서 `seq_len`은 시퀀스의 길이, `batch`는 배치 크기, `input_size`는 입력의 특징의 수, `hidden_size`는 숨겨진 상태의 특징의 수, 그리고 `num_layers`는 RNN 층의 수를 의미한다.

num_layers=2는 RNN을 2번 연속으로 쌓았다는 것이다. 즉 이렇게 

![[84.jpg]]

보면 입력 1이 RNN을 2번동안 거쳐서 출력된다. 같은 시간 t에서 RNN을 순차적으로 반복하는 것은 학습능력이 상승한다고 알려져 있다. (같은 5분에서 단어를 1번 보는것과 10번 보는것 어느게 더 학습이 좋을진 알지 않은가??)

각 시간 단계에서의 계산은 다음과 같이 표현될 수 있다.

![[Pasted image 20240221000301.png]]

1. `output`의 형상은 `(seq_len, batch, num_directions * hidden_size)` = `(5, 3, 20)`
2. `hn`의 형상은 `(num_layers * num_directions, batch, hidden_size)` = `(2, 3, 20)`

`num_directions`는 단방향 RNN의 경우 1이고, 양방향 RNN의 경우 2가 된다.

![[Pasted image 20240221002108.png]]
