근데 생각해보자

![[1. CNN LSTM 개념/image2/09.png]]

우리가 Embedding 계층을 사용한 이유는 단어의 분산 표현을 얻고자 함이다. (정확히는 단어의 분산 표현을 관리하는 계층이다) 그래서 우리는 word2vec를 제작했다.

그런데 뜬금없이 RNN에 Embedding계층이 있는것을 확인할 수 있다. 이건 무슨 이유일까?

이걸 이해하려면 Embedding 계층을 뜯어봐야한다. 다음은 Embedding 계층의 설명이다. 

> PyTorch의 `Embedding` 계층은 주로 자연어 처리(NLP) 작업에서 단어, 문자 또는 기타 유형의 토큰을 저차원의 연속 벡터로 매핑하는 데 사용된다. 이러한 벡터를 임베딩이라고 하며, 비슷한 의미를 가진 단어는 비슷한 벡터 표현을 갖도록 학습된다.

즉, 어떠한 시계열 데이터를 벡터표현으로 만들고자 함이다. 이러면 단순히 RNN이 무작정 시계열 데이터를 입력받아서 처리하는 것보다, 어떠한 특징을 잡고 그에 따른 벡터를 입력받아서 처리하는게 좀 더 좋은 학습 방법이란 것이다.

생각해보자, 어떠한 단어를 외울때 그냥 단어 하나하나 무작정 외우는 것보다, 그 단어가 어떤 의미이고 다른 비슷한 단어가 뭐가 있는지 그 단어의 특징을 잡고 학습하는게 더 효율이 좋지 않겠는가?

### 1. 장점
---
1. **차원 축소**: 임베딩은 일반적으로 고차원의 원-핫 인코딩(one-hot encoding)을 저차원의 연속적인 벡터로 변환합니다. 이는 모델의 계산 효율성을 높이고, 필요한 매개변수의 수를 크게 줄여줍니다.

2. **의미적 정보 포착**: 임베딩 벡터는 단어 간의 의미적 관계를 학습하여 표현할 수 있습니다. 이는 단순한 원-핫 인코딩보다 훨씬 더 풍부한 정보를 제공하여, 모델이 문맥을 더 잘 이해하고 예측을 개선하는 데 도움을 줍니다.

3. **메모리 효율성**: 원-핫 인코딩은 매우 비효율적으로 메모리를 사용합니다. 대부분의 원소가 0이고 오직 하나의 원소만 1인 벡터는, 특히 단어 사전의 크기가 큰 경우 많은 메모리를 차지합니다. 임베딩은 이러한 고차원 벡터를 훨씬 작은 크기의 밀집 벡터로 변환함으로써 메모리 사용을 최적화합니다.

4. **연산적 효율성**: 원-핫 벡터를 사용할 때, 대부분의 연산은 0에 대해 수행되므로 연산 자체가 낭비될 수 있습니다. 반면, 임베딩을 사용하면 모든 원소가 연산에 기여하므로 연산이 훨씬 효율적입니다.

5. **미분 가능**: 원-핫 인코딩은 미분할 수 없지만, 임베딩 벡터는 미분 가능하므로, 역전파 알고리즘을 사용하여 임베딩을 학습하고 최적화할 수 있습니다.

6. **전이 학습**: 사전에 훈련된 임베딩(예: Word2Vec, GloVe 등)을 사용하면, 다른 데이터셋에서 학습된 언어의 일반적인 의미적 특성을 활용할 수 있습니다. 이는 모델이 새로운 데이터에 대해 빠르게 학습하고 더 나은 성능을 낼 수 있도록 도와줍니다.

7. **변수 길이 입력 처리**: 자연어 데이터는 일반적으로 가변 길이를 가지며, 임베딩 계층은 이러한 가변 길이의 시퀀스를 일정한 크기의 벡터로 변환하여 RNN 같은 순환 신경망이 처리할 수 있도록 합니다.


### 2. 조심해야 하는것
---
위 사진을 보면 임베딩 계층을 T개만큼 뭉쳐서 만든 TimeEmbedding을 쓴다. 근데 일반적으로 자연어 처리(NLP) 작업에서는 각 시간 단계(time step)에 대해 동일한 임베딩 계층(embedding layer)을 공유하여 사용한다. 

심지어 위의 소스코드를 뜯어보면 다음과 같다.

``` python
class TimeEmbedding:
    def __init__(self, W):
        self.params = [W]
        self.grads = [np.zeros_like(W)]
        self.layers = None
        self.W = W

    def forward(self, xs):
        N, T = xs.shape  # N(batch), T(timesteps)
        V, D = self.W.shape  # V(vocab_size), D(embedding_size)
        out = np.empty((N, T, D), dtype='f')
        self.layers = []
        for t in range(T):
            layer = Embedding(self.W)
            out[:, t, :] = layer.forward(xs[:, t])
            self.layers.append(layer)

        return out
        
    def backward(self, dout):
        N, T, D = dout.shape
        grad = 0
        for t in range(T):
            layer = self.layers[t]
            layer.backward(dout[:, t, :])
            grad += layer.grads[0]
            
        self.grads[0][...] = grad

        return None
```

보면 여러개의 임베딩 계층을 시계열 데이터에 맞게 설정한 것을 확인할 수 있다. 한 개의 데이터에 한개의 임베딩 계층을 사용하고 있다. 어느것이 더 뛰어난지는 잘 모르겠다.

---
1. **파라미터 공유**: 각 시간 단계에 동일한 임베딩 계층을 사용함으로써, 모델은 각 단어에 대해 하나의 벡터 표현만을 학습합니다. 이는 모델 파라미터의 수를 크게 줄여주고, 학습해야 할 가중치의 수를 감소시킵니다.

2. **일관성**: 동일한 단어는 문장 내에서 다른 위치에 등장할 수 있습니다. 공유 임베딩 계층을 사용하면, 해당 단어는 문맥에 상관없이 일관된 벡터 표현을 가지게 됩니다. 이는 단어의 의미가 일관되게 유지되어야 하는 대부분의 언어 처리 작업에 적합합니다.

3. **학습 효율성**: 하나의 임베딩 계층만을 사용함으로써, 모델이 데이터의 다양한 부분에서 같은 단어에 대해 학습할 수 있으므로, 훈련 데이터를 보다 효율적으로 사용합니다.

4. **메모리 및 계산 효율성**: 만약 각 시간 단계마다 별도의 임베딩 계층을 사용

한다면, 각 단계에 대해 별도의 파라미터 세트가 필요하게 되어 메모리 사용량이 증가하고, 이로 인해 계산 비용도 증가합니다. 반면에 하나의 임베딩 계층을 공유함으로써 리소스 사용을 최적화할 수 있습니다.

5. **전이 학습**: 사전 훈련된 임베딩을 사용하는 경우, 이러한 임베딩은 다양한 문맥에서 단어의 벡터 표현을 이미 학습했기 때문에, 모든 시간 단계에서 같은 임베딩을 사용하는 것이 효과적입니다.

따라서, 일반적으로 임베딩 계층은 각 시간 단계에 대해 하나만 사용하며, 이를 전체 시퀀스에 걸쳐 공유합니다. 이 접근 방식은 대부분의 순환 신경망(RNN), 게이트 순환 유닛(GRU), 장단기 기억(LSTM)과 같은 순환 아키텍처에서 표준적인 방법으로 받아들여지고 있습니다.

---

