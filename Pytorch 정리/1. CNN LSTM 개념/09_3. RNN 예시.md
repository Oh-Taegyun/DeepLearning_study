넷플릭스 주가의 하루 시가, 고가, 저가 정보를 입력받아 그날의 종가를 예측하는 간단한 RNN을 만들어보자


### 1. 데이터 정보
---
![[1. CNN LSTM 개념/image2/01.png|400]]

날짜(Date), 개장(Open), 최고가(High), 최저가(Low), 거래량(Volume), 종가(Close)

주가의 변동 추이를 분석해 다음날의 종가를 예측하기 원하므로, 입력 데이터로는 개장가, 최고가, 최저가이고 정답 레이블은 종가이다. 

### 1. 최소-최대 정규화
---
![[86.png]]

이거 보면 값의 범위가 100 ~ 400까지 넓게 형성되어 있다. 

딥러닝은 오차 역전파를 이용해 가중치를 학습하는데, 출력값의 범위가 커지면 오차의 범위도 커지게 되고 오차가 커지면 역전파 되는 기울기 또한 커지기 때문에 가중치 수렴에 안 좋은 영향을 미칠 수 있다. 따라서 값의 범위를 0부터 1사이로 정규화 하는 것이 좋다. 

이러한 0과 1 사이로 정규화 하는 것을 최소-최대 정규화라고 하는데 데이터에 이상치가 있을 때에는 적합하지 않지만 이 데이터에는 이상치가 없어서 적용할 수 있다.


### 2. 학습용 데이터 만들기
---
30일치씩 묶어서 데이터를 구성해주자

``` python
import numpy as np
from torch.utils.data.dataset import Dataset

class Netflix(Dataset):  # 1) 클래스 선언
    def __init__(self):
        self.csv = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/data/CH06.csv")         # 2) 데이터 읽기
        # 입력 데이터 정규화
        self.data = self.csv.iloc[:, 1:4].values   # 3) 종가를 제외한 데이터 담기
        self.data = self.data / np.max(self.data)  # 4) 제일 큰 데이터로 나눠서 0과 1 사이로 정규화

        # 5) 종가 데이터 정규화
        self.label = data["Close"].values        
        self.label = self.label / np.max(self.label)

    def __len__(self):
        return len(self.data) - 30 # 1) 사용 가능한 배치 개수

    def __getitem__(self, i):
        data = self.data[i:i+30] # 1) 입력 데이터 30일치 읽기
        label = self.label[i+30] # 2) 종가 데이터 30일치 읽기
        return data, label
```

데이터를 꺼낼때는 30일치씩 꺼내되 1일차가 나도록 꺼내겠다.

무슨 말이냐면

![[1. CNN LSTM 개념/image2/10.png]]

이런식으로 묶겠다는 뜻이다. 전체 데이터의 개수가 N이고, 배치 길이를 L이라고 하자.

이론적으로 N-L+1개를 만들 수 있는데 마지막 종가를 예측하는 것이므로 마지막 종사는 모르기 때문에 N-L이다.

### 3. RNN
---
![[84.jpg]]

위 그림이 살짝 이해하기 어렵다. 따라서 다음 그림을 활용하면서 이해해보자

![[93.png]]

![[99.png]]

헷갈리는 부분은 바로 `"어느것이 먼저 시작인거지?"` 일 것이다. 하지만 잘 생각해보자. 0초에 해당하는 작업이 완료가 되어야 1초에 해당하는 작업이 시작될 것이다. 즉, 분기하는 지점의 순간은 같으나, h_0가 나온 뒤에 x_1이 입력되고 h_1이 나온다는 것이다. 


``` python
import torch
import torch.nn as nn

class RNN(nn.Module):
   def __init__(self):
       super(RNN, self).__init__()
       self.rnn = nn.RNN(input_size=3, hidden_size=8, num_layers=5,
                         batch_first=True)  # 1) RNN층의 정의

       # 2) 주가를 예측하는 MLP층 정의
       self.fc1 = nn.Linear(in_features=240, out_features=64)
       self.fc2 = nn.Linear(in_features=64, out_features=1)
       self.relu = nn.ReLU() # 활성화 함수 정의

   def forward(self, x, h0):
       x, hn = self.rnn(x, h0)  # 1) RNN층의 출력 x: (32,30,3) hn: (32,30,8)
       
       # 2) MLP층의 입력으로 사용될 수 있도록 모양 변경
       x = torch.reshape(x, (x.shape[0], -1)) # (32, 240)
      
       # MLP 층을 이용해 종가를 예측
       x = self.fc1(x) # (32, 64)
       x = self.relu(x) # (32, 64)
       x = self.fc2(x) # (32, 1)

       # 예측한 종가를 1차원 벡터로 표현
       x = torch.flatten(x) # (1, 32) 파이토치 식으로 하자면 torch.Size([32])
       
       return x
```

![[1. CNN LSTM 개념/image2/11.png]]


### 4. 그 외 코드
---
``` python
import tqdm
from torch.optim.adam import Adam
from torch.utils.data.dataloader import DataLoader
import matplotlib.pyplot as plt

device = "cuda" if torch.cuda.is_available() else "cpu"
model = RNN().to(device)  # 모델의 정의
dataset = Netflix()  # 데이터셋의 정의

loader = DataLoader(dataset, batch_size=32)  # 배치 크기를 32로 설정
optim = Adam(params=model.parameters(), lr=0.0001) # 사용할 최적화를 설정

for epoch in range(200):
   iterator = tqdm.tqdm(loader)
   for data, label in iterator:
       optim.zero_grad()

       # ❶ 초기 은닉 상태
       h0 = torch.zeros(5, data.shape[0], 8).to(device)
       # ❷ 모델의 예측값
       pred = model(data.type(torch.FloatTensor).to(device), h0)
       # ❸ 손실의 계산
       loss = nn.MSELoss()(pred, label.type(torch.FloatTensor).to(device))
       loss.backward()  # 오차 역전파
       optim.step()  # 최적화 진행
       
       iterator.set_description(f"epoch{epoch} loss:{loss.item()}")
       
torch.save(model.state_dict(), "./rnn.pth")  # 모델 저장

# --------------------------------------------------------------------------------

loader = DataLoader(dataset, batch_size=1)  # 예측값을 위한 데이터 로더
preds = []  # 예측값들을 저장하는 리스트
total_loss = 0

with torch.no_grad():
   # 모델의 가중치 불러오기
   model.load_state_dict(torch.load("rnn.pth", map_location=device))
   
   for data, label in loader:
       h0 = torch.zeros(5, data.shape[0], 8).to(device)  # ➊ 초기 은닉상태 정의 (5, 32, 8)
       # 모델의 예측값 출력
       pred = model(data.type(torch.FloatTensor).to(device), h0)
       preds.append(pred.item())  # ➋ 예측값을 리스트에 추가
       loss = nn.MSELoss()(pred, label.type(torch.FloatTensor).to(device))  # 손실계산
       total_loss += loss/len(loader)  # ➌ 손실의 평균치 계산
     
total_loss.item()
```

초반에 제공하는 초기 은닉벡터는 기본적으로 값이 0인 텐서로 주는 것이 일반적이다. 

근데 뭐랄까 멋있게 마지막 종가는 몇일까요? 하는 코드는 아니고 그냥 얼마나 예측을 할 수 있는지만 파악하는 정도의 단순한 코드다. 