모델이 스스로 과대적합이 가능하고, 모델이 정상이라면 이제 일반화를 시작하자

### 1. 데이터셋 큐레이션
---
매니폴드 가설에 의해 데이터의 샘플 간격을 부드럽게 보간할 수 있다면 일반화 성능을 가진 모델을 훈련할 수 있을 것이다. 

다시말해서 모델을 디자인하는 것 만큼 데이터의 전처리에도 충분한 시간을 쏟아야 한다는 뜻

 - 데이터셋 큐레이션
	 1. 데이터가 충분한지 확인 
	 2. 데이터가 많을수록 좋은 모델이 만들어짐 - 대용량의 데이터셋으로 해결
	 3. 레이블 할당 에러를 최소화
	 4. 데이터를 정제하고 누락된 값을 처리
	 5. 특성 선택을 수행

### 2. 특성 공학
---
모델에 데이터를 주입하기 전에 변환을 적용하여 알고리즘이 더 잘 수행되도록 만들어 주어야 하고, 모델이 수월하게 작업할 수 있는 어떤 방식으로 데이터가 표현될 필요가 있다는 이론이다. 

간단하게 모델에 데이터를 주입하기 전에 (학습이 아닌) 하드코딩된 변환을 적용하여 알고리즘이 더 잘 수행하도록 만들어준다는 것

예를 들어 다음과 같은 예시를 들 수 있다.

![[Pasted image 20240414011420.png]]

이미지처럼 CNN으로 모든것을 접근해서 파악해야지! 했던 것이 CNN으로는 각도 계산을 하고 나머지 알고리즘으로 각도를 보고 시간을 읽어주는 식으로 해결한다면, 아예 차원이 다른 이야기가 된다.

딥러닝이 아무리 강력한 툴이긴 해도, 하나의 방법일뿐 기존의 알고리즘까지 싹 다 잡아먹을 정도로 대단한 것은 아니다. 위의 예시의 경우 컴퓨터 자원을 많이 잡아먹는 다는 것은 다시 말해 간단한 제품을 만들때 엄청난 제약을 가하게 된다. (만약 게임을 아무리 고퀄로 만들어도 집에 슈퍼컴퓨터가 아닌 이상 돌아가지 않는다는 단점이 있으면 그 게임을 누가 사겠는가...?)


### 3. 모델 규제하기
---
규제(regularization) 훈련 데이터에 완벽하게 맞추려는 모델의 능력을 적극적으로 방해하는 기법 

너무 작은 신경망은 과대적합되지 않는다. 과대적합을 완화하는 가장 간단한 방법은 신경망의 크기를 냅다 줄이면 된다. 근데 그러면 기억을 담고 있는 노드의 숫자가 줄어들어서(모델의 기억 용량에 제한이 생긴다는 것)

반대로 너무 큰 용량을 가지고 있으면 그냥 냅다 과대적합을 해버린다. 보통 첫 번째 에포크 이후 거의 바로 과대적합이 시작되어서 갈수록 심해지는 경우로 나타난다. 과대적합은 안해도 검증 손실 곡선이 고르지 않고 분산이 크게 나타낸다. 

![[Pasted image 20240414013638.png]]

검증 지표가 고르지 않다는 것은 신뢰할 수 있는 검증 과정을 사용하지 않는다는 징후일 가능성이 존재학 때문, 검증 세트가 너무 작은 경우에 발생한다. 

따라서 적합한 모델 크기를 결정하려면 먼저 적은 신경망부터 시작해서 천천히 모델 크기를 늘리는 식으로 극복해야 한다. ~~노가다~~


### 4. 가중치 규제 추가
---
오캄의 면도날이라는 이론으로부터 시작한다. 오캄의 면도날이란 것은 어떤 것에 대한 두 가지 설명이 있다면 더 적은 가정이 필요한 간단한 설명이 옳을 것이란 이론이다.

이를 딥러닝에 추가한다면, 간단한 모델이 복잡한 모델보다 덜 과대적합하게 될 가능성이 있다는 것이다. 그러므로 과대적합을 완화하기 위한 일반적인 방법은 모델의 복잡도에 제한을 둬서 가중치가 작은 값을 가지도록 강제해야한다. (물론 모델을 간단하게 하는 방법도 좋은데 이 방법엔 한계가 존재한다.)

가중치 규제는 일반적으로 작은 딥러닝 모델에서 사용한다. 대규모 딥러닝의 경우 모델은 파라미터가 너무 많기 때문에 가중치 값을 제약하는 것이 모델 용량과 일반화에 큰 영향을 미치지 않는 경향이 있기 때문이다. 이런 경우 드롭아웃이 더 선호된다. 

이를 통해 가중치의 값 분포가 더 균일하게 된다. 이를 가중치 규제라고 한다. L1 규제가 L2 규제에 비해 Outlier에 더 둔감 하다.

##### 4-1. L1 규제
---
![[Pasted image 20240414014317.png]]

가중치의 절댓값에 비례하는 비용이 추가(가중치의 L1 노름(norm))

##### 4-2. L2 규제
---
![[Pasted image 20240414014351.png]]

L2 규제(가중치 감쇠,weight decay) : 가중치의 제곱에 비례하는 비용이 추가(가중치의 L2 노름) 

이때 이러한 페널티항은 훈련할 때만 추가해야한다. 그래서 손실이 테스트보다 훈련할 때 더 높을 것이다. 


### 5. 드롭아웃
---
훈련하는 동안 무작위로 층의 출력 특성을 일부 제외시킨다(0으로 만든다.)

각 샘플에 대해 뉴런 일부를 무작위하게 제거하면 뉴런의 부정한 협업을 방지하고 결국 과대적합을 감소시킨다는 아이디어

뉴런은 누락된 뉴런의 존재에 의존하지 않고 스스로 작업을 수행하는 방법을 배워야 한다. 그 결과 모델이 더욱 견고해지고 특정 뉴런에 대한 의존도가 낮아져 새로운 데이터에 대한 일반화가 향상되기 때문이다. 노이즈가 없다면 모델이 이 패턴을 기억하기 시작할 것

여기에 재미난 이야기가 있는데 은행 업무는 행원들을 자주 바꾼다고 한다. 부정 행위를 할려면 직원들 사이에서 유대가 생겨나야 하는데 유대가 생기기 전에 바꿔서 방지한다는 것

