딥러닝에서 활성화 함수(Activation Function)의 사용은 필수적인 부분이다. 활성화 함수는 신경망의 각 레이어에서 입력 신호의 총합을 출력 신호로 변환하는데, 이 변환 과정을 비선형으로 만들기 때문이다. (Linear의 경우 행렬에서 선형 변환이 있다는 점을 기억하자) 비선형 활성화 함수가 없으면, 신경망은 아무리 많은 레이어를 쌓아도 결국 선형 함수와 같아져 복잡한 문제를 풀 수 없게 된다. 즉, 비선형 활성화 함수를 사용함으로써 신경망이 더 복잡한 패턴과 데이터의 특징을 학습할 수 있게 된다.

### 1. 고려할 사항
---
1. **문제의 종류**: 예를 들어, 회귀 문제와 분류 문제는 서로 다른 종류의 활성화 함수가 적합할 수 있다. 분류 문제에서는 종종 출력층에서 소프트맥스(Softmax) 함수를 사용하고, 회귀 문제에서는 일반적으로 활성화 함수를 사용하지 않거나 선형 활성화 함수를 사용한다. 참고로 실험 도중 출력층에서도 활성화 함수를 쓰면 추론을 아예 못하는 것을 확인할 수 있었다. (아마도 ReLU를 예시로 들면 음수의 값또한 0으로 전환해주기 때문에 많은 활성화 함수의 출력값을 이상하게 바꾸기 때문이 아닐까 싶다.)
