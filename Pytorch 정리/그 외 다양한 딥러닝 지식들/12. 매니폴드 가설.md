### 1. 차원의 저주
---
차원이 높아짐에 따라 데이터가 희소하게 분포하게 되는 문제이다. 

![[Pasted image 20240403021222.png]]

위의 그림처럼 1D에 데이터가 분포한다고 하면 4칸만 확인하면 데이터 분포를 확인할 수 있다. (1차원에 데이터가 존재한다는 것은 저렇게 직선 위에 데이터가 위치하는 것을 의미한다.)

2D의 경우에는 4x4(4^2) 칸을 모두 확인해야 데이터를 확인할 수 있다.

3D의 경우에는 맨 오른쪽 정육면체처럼 4^3개의 공간을 모두 살펴봐야한다. 하지만, 점이 안찍혀있는 빈 공간도 적은 차원일 때보다 많아졌다. 즉, 모든 구역을 살펴보는게 의미가 없다는 뜻이다. 이렇게 빈 공간이 많다는 것이 데이터가 희소하게 분포한다는 뜻이다.

차원이 높을수록 데이터는 희소하게 분포하여 학습이 어려워진다. (모든 점들을 학습하기 위해서 모든 구역들을 살펴보아야 하기 때문에)

다시 말해 희소성이 높을수록 모델링 난이도가 높아진다. 

그러므로 데이터의 특징(feature)을 더럽히지 않으면서 낮은 차원에서 표현해야 한다.

### 2. 매니폴드 가설
---
딥러닝에서의 차원축소에서 근본이 되는 가설 고차원의 데이터를 오류없이 잘 표현하는 subspace가 존재한다는 가설이다.

고차원 공간의 샘플들이 저차원 다양체(manifold)의 형태로 분포해 있다는 가정이다. 샘플들이 모두 균일하게 존재하는 것이 아니고 한 공간에 뭉쳐있을 확률이 높다. 

쓸데없는 차원을 날려버리면 더 낮은 차원의 공간에 데이터를 위치시킬 수 있을 것이다.

ex) 3차원 공간에 2차원 형태로 데이터가 존재할 수 있다.

![[Pasted image 20240403021411.png]]

예를 들면, 왼쪽 그림과 같이 고차원 공간에 데이터가 저렇게 존재한다고 하자. 이는 데이터가 모든 공간에 균등하게 퍼져있는 것이 아니라 저렇게 말려있는 형태로 데이터가 분포한다고 하자. (빈 공간이 있는 것)

이렇게 비어있는 공간이 많고 모든 데이터가 균일하게 분포한게 아니므로 저차원 공간에 말린 것을 펴서 표현할 수 있을 것이다. (실제로 고차원에 데이터가 분포한다고 했지만 사실은 저차원 형태로 존재하는 것)

= 이 manifold를 해당 차원의 공간에 mapping 할 수 있다

이때 고차원 공간에서의 두 점 사이의 거리는 저차원 공간으로의 맵핑 후 거리가 다르다. 하지만 저차원 공간으로 맵핑해서 거리를 재는 것이 더 의미가 있다.

고차원 공간에서 왼쪽 그림과 같이 두 점 사이의 거리(최단거리)를 잰다면, 검은색 화살표(↔) 사이를 나타낼 것이다. 하지만 그 직선 사이에는 데이터가 존재하지 않으므로 이 거리는 의미가 없다. 

하지만 저차원 공간에 맵핑한 후 거리를 재면 그 사이에 데이터들이 있기 때문에 의미가 있는 것이다.

이렇게 추출된 입력 공간 안에서 비교적 간단하고, 저차원적이며, 매우 구조적인 부분 공간(잠재 매니폴드)만 학습하면 고차원의 데이터를 통째로 들어내서 학습하는것보다 더 의미있다는 것이다. 


### 4. 샘플 사이를 보간하는 능력은 딥러닝에서 일반화하는 열쇠
---
샘플 사이를 보간하는 능력은 딥러닝에서 일반화를 이해하는 열쇠이란 것은 무슨 뜻일까?
  
매니폴드 이론은 고차원 데이터가 실제로는 더 낮은 차원의 매니폴드(manifold) 내에 존재할 수 있음을 제안한다는 것이다. 

복잡하게 보이는 데이터도 내부적으로는 더 간단한 구조를 가지고 있다는 것. 딥러닝에서 샘플 사이를 보간하는 능력은 이 매니폴드를 학습하고, 데이터가 존재하는 더 낮은 차원의 공간을 이해하는 데 중요하다. 이를 통해 모델은 보이지 않는 새로운 데이터에 대해 예측을 할 수 있으며, 이는 일반화의 핵심 요소이다.

간단하게 이미지 인식을 기반으로 예시를 들어보자

만약 우리는 고양이와 개의 이미지로 구성된 데이터셋을 가지고 있다. 이 데이터셋의 고차원 공간(예: 각 이미지가 수천 또는 수만 개의 픽셀로 구성된 공간)에서, 모든 고양이 이미지는 특정 매니폴드 상에 위치하고, 모든 개 이미지도 또 다른 매니폴드 상에 위치한다.

이 두 매니폴드는 서로 겹치지 않지만, 아마도 매우 가까이 있을 것이다. 딥러닝 모델이 이 두 매니폴드를 학습함으로써, 모델은 고양이와 개 이미지를 구분할 수 있게 된다. 

더 중요한 것은, 모델이 두 매니폴드 사이의 공간을 '이해'하게 되어, 고양이와 개의 '사이'에 있는 이미지가 어떻게 보일지를 추측할 수 있다는 것이다. 이는 새로운 고양이나 개의 이미지가 주어졌을 때, 모델이 해당 이미지가 각각의 매니폴드에 얼마나 가까운지를 파악하고, 그 결과로 고양이인지 개인지를 정확하게 예측할 수 있게 한다는 것.

이러한 보간 능력은 모델이 학습 데이터에 정확하게 맞지 않는 새로운 데이터에 대해서도 예측할 수 있게 해준다. 즉, 보지 못한 데이터에 대한 일반화를 가능하게 하며, 이는 딥러닝 모델의 성능을 실제 세계에서 평가할 때 매우 중요한 요소이다. 모델이 매니폴드 사이의 공간을 이해하고, 샘플을 적절히 보간할 수 있으면, 그 모델은 더 다양한 상황에서 정확하게 작동할 수 있게 된다.

![[Pasted image 20240403022524.png]]

하지만 일반적으로 잠재 매니폴드에서 보간은 부모 공간에서의 선형 보간과 완전히 다른 의미이다. 단순하게 픽셀을 평균하면 일반적으로 유요한 숫자가 만들어지지 않기 때문

이러한 보간법을 이용해서 어디에 더 가까운지를 판단해서 새로운 데이터들을 잘 판단할 수 있다. 

##### mnist의 예시
MNIST 숫자 데이터로 예를 들어 설명해주셨다. 고차원 공간에 존재했던 MNIST 데이터를 2차원에 맵핑하였다. 

MNIST 숫자 데이터가 784(28x28)차원의 벡터라고 하자 (이는 1D 벡터로 표현된다고 해서 1D 차원에 있는 것이 아니다. 784개의 숫자로 표현되어있기 때문에 784차원의 공간에 존재한다고 볼 수 있다. 우리는 이 784차원을 상상조차 하지 못한다.)

아래 그림은 원래 784차원에 존재하는 데이터를 2차원으로 인코딩한 것을 나타낸 것이다. 3과 6을 2차원으로 인코딩하고 그 값의 평균을 구하면 0에 해당하는 값이 나오는데 이는 3과 6 사이에 0이 위치한다는 것으로 해석할 수 있다.

2개의 MNIST 숫자 3과 6을 변형시키는 연속적인 중간 이미지가 있다는 의미이다.

![[Pasted image 20240403021719.png]]


### 5. 딥러닝이 작동하는 이유
---
딥러닝 모델은 근본적으로 엄청 고차원적인 곡선이다. 경사 하강법을 통해 이 곡선을 부드럽고 점진적으로 데이터 포인트에 맞춘다. 딥러닝은 본질적으로 크고 복잡한 곡선(매니폴드)을 선택하여 훈련 데이터 포인트에 맞을 때까지 파라미터를 점진적으로 조정하는 것이다.


### 6. 가장 중요한 훈련 데이터
---
딥러닝이 실제로 매니폴드 학습에 잘 맞지만 일반화의 능력은 모델의 어떤 속성때문이라기보다 그냥 데이터의 자연적인 구조로 인한 결과다.

그러니까 데이터가 보간할 수 있는 매니폴드를 형성하는 경우에만 일반화가 가능하다는 것이다. 다시 말해 특성이 유익하고 잡음이 적을수록 입력 공간이 더 간단하고 구조적이기 때문에 일반화 할 수 있다는 것이다.

그니까 한마디로 데이터의 전처리가 엄청 중요하단 뜻, 데이터 큐레이션과 특성공학이 일반화에 필수적인 이유이다.

---
##### 데이터 큐레이션(Data Curation)

데이터 큐레이션은 데이터를 수집, 정리, 관리, 통합하는 과정을 포함합니다. 이 과정은 대규모 데이터 세트에서 유용하고 관련성 높은 정보를 식별하고, 데이터의 품질을 보장하며, 데이터를 분석이나 학습에 적합한 형태로 조직하는 것을 목표로 합니다. 데이터 큐레이션의 주요 단계에는 다음이 포함될 수 있습니다:

- **수집**: 다양한 출처에서 데이터를 수집합니다.
- **정제**: 오류, 결측치, 중복 등을 수정하거나 제거하여 데이터의 정확성을 높입니다.
- **통합**: 서로 다른 데이터 소스를 통합하여 일관된 형식이나 구조를 만듭니다.
- **분류 및 태깅**: 데이터를 분류하고 메타데이터를 추가하여 검색과 분석을 용이하게 합니다.
- **보관**: 데이터를 안전하게 저장하고 접근을 관리합니다.

##### 특성 공학(Feature Engineering)

특성 공학은 기계 학습 모델의 성능을 향상시키기 위해 입력 데이터의 특성(또는 변수)을 선택, 수정, 생성하는 과정입니다. 이 과정은 모델이 데이터에서 패턴을 더 잘 이해하고 학습할 수 있도록 돕는 것을 목적으로 합니다. 특성 공학은 종종 도메인 지식과 데이터 이해를 바탕으로 이루어지며, 다음과 같은 활동을 포함할 수 있습니다:

- **특성 선택**: 가장 유용한 특성을 선택하여 모델의 복잡성을 줄이고 과적합을 방지합니다.
- **특성 생성**: 기존의 데이터에서 새로운 특성을 생성합니다. 예를 들어, 날짜에서 요일을 추출하거나 여러 특성을 결합하는 등입니다.
- **특성 변환**: 로그 변환, 정규화, 범주형 데이터의 원-핫 인코딩 등을 통해 데이터의 분포를 조정하거나 모델이 데이터를 더 잘 이해할 수 있도록 합니다.
- **차원 축소**: PCA(주성분 분석) 같은 기법을 사용하여 데이터의 차원을 줄이고, 중요한 정보를 유지하면서 계산 비용을 감소시킵니다.

데이터 큐레이션과 특성 공학은 서로 보완적으로 작용하여, 데이터를 정제하고 최적화하는 데 도움을 줍니다. 이 과정들을 통해 데이터 과학자와 기계 학습 엔지니어는 더 깊은 인사이트를 발견하고, 보다 효과적인 모델을 구축할 수 있습니다.

---

![[Pasted image 20240403024214.png]]

딥러닝이 곡선을 맞추는 것이기 때문에 모델이 이를 잘 수행하려면 입력 공간을 조밀하게 샘플링 해야한다. '조밀한 샘플링'은 입력 데이터 매니폴드 전체를 조밀하게 커버해야 한다는 의미이다. 결정 경계 근처는 더더욱 중요하다.

따라서 딥러닝 모델을 향상시키는 가장 좋은 방법은 더 좋고, 더 많은 데이터에서 훈련하는 기본 원칙을 증명하는 이유이다. 

