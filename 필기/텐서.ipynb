{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ae2631f-1b28-46be-b396-b3819a953fdb",
   "metadata": {},
   "source": [
    "# 텐서\n",
    "-----\n",
    "\n",
    "- 텐서의 정의\n",
    "1) torch.FloatTenser : 32bit float point (일반적인 텐서 형태)\n",
    "2) torch.LongTenser : 64bit signed integer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94e6b3b-af32-4862-ae57-9005895e445f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. 텐서의 생성 방법\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f80408b6-99e0-4e92-a81a-abb9ed2b484b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step1 : tensor([[1, 2],\n",
      "        [3, 4]]) \n",
      "\n",
      "step2 : tensor([[1, 2],\n",
      "        [3, 4]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 1. 리스트로 생성\n",
    "A = torch.tensor([[1,2], [3,4]])\n",
    "print ('step1 :',A,'\\n')\n",
    "# 넘파이랑 동일하게 torch.tenser([[1, 2], [3, 4]], device=“cuda:0“, dtype=“torch.float64“) 처럼 세부 조정이 가능하다. \n",
    "\n",
    "# 2. numpy 배열로부터 생성\n",
    "np_array = np.array([[1,2], [3,4]])\n",
    "x_np = torch.from_numpy(np_array)\n",
    "print('step2 :',x_np)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428b3302-94ac-4d21-9996-2199ab2acdf3",
   "metadata": {},
   "source": [
    "### 2. 다양한 텐서 관련 함수들\n",
    "---\n",
    "\n",
    "x_data = torch.tensor([[1,2],[3,4]])\n",
    "\n",
    "a, b는 그저 스칼라\n",
    "\n",
    "-  랜덤한 값을 가지는 텐서\n",
    "\n",
    "    1. torch.rand(a,b) : 0과 1 사이의 숫자를 균등하게 생성\n",
    "\n",
    "    2. torch.rand_like(x_data) : 사이즈를 튜플로 입력하지 않고 기존의 텐서로 정의\n",
    "\n",
    "    3. torch.randn(a,b) : 평균이 0이고 표준편차가 1인 가우시안 정규분포를 이용해 생성\n",
    "\n",
    "    4. torch.randn_like(x_data) :  사이즈를 튜플로 입력하지 않고 기존의 텐서로 정의\n",
    "\n",
    "    5. torch.randint(a,b,size=(n,m)) : 주어진 범위 내의 정수[a,b)를 균등하게 nxm 행렬로 생성, 자료형은 torch.float32\n",
    "\n",
    "    6. torch.randint_like(x_data) : 사이즈를 튜플로 입력하지 않고 기존의 텐서로 정의\n",
    "\n",
    "    7. torch.randperm(A) : 주어진 정수까지의(0부터 A-1까지) 정수를 랜덤하게 생성\n",
    "\n",
    "    8. 랜덤 생성에 사용되는 시드(seed)는 torch.manual_seed() 명령으로 설정한다.  \n",
    "        - seed() 괄호 안에 들어가는 숫자는 무슨 의미일까?\n",
    "        - seed value 숫자 자체는 중요하지 않고 서로 다른 시드를 사용하면 서로 다른 난수를 생성한다는 점만 알면 된다.  \n",
    "        #  \n",
    "        　\n",
    "        \n",
    "\n",
    "    9. 특정한 값으로 초기화를 하지 않는 행렬을 만들 때에는 torch.empty(a,b) 함수를 사용한다. \n",
    "        - 주어진 크기의 아무값으로도 초기화되지 않은 텐서를 만든다. 텐서 성분의 값들은 쓰레기값들이다. 0.00000e + 00 같은 값인데 의미 없다.\n",
    "        #  \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "- 특정한 값을 가지는 텐서 생성\n",
    "\n",
    "    1. torch.arange(a,b,step) : 주어진 범위 내([a,b))의 정수를 step만큼 간격을 두고 순서대로 생성\n",
    "\n",
    "    2. torch.ones(a,b) : 주어진 사이즈의 1로 이루어진 텐서 생성\n",
    "\n",
    "    3. torch.zeros(a,b) : 주어진 사이즈의 0으로 이루어진 텐서 생성\n",
    "\n",
    "    4. torch.ones_like(x_data) : 사이즈를 튜플로 입력하지 않고 기존의 텐서로 정의\n",
    "\n",
    "    5. torch.zeros_like(x_data) : 사이즈를 튜플로 입력하지 않고 기존의 텐서로 정의\n",
    "\n",
    "    6. torch.linspace(시작,끝,step) : 시작점과 끝점을 주어진 갯수만큼 균등하게 나눈 간격점을 행벡터로 출력\n",
    "\n",
    "    7. torch.logspace(시작,끝,step) : 시작점과 끝점을 주어진 갯수만큼 로그간격으로 나눈 간격점을 행벡터로 출력\n",
    "\n",
    "    8. torch.eye(a) : 사선 방향이 1인 a x a 텐서 생성\n",
    "\n",
    "    9. torch.cat((a, b, c ...), dim = 0 or 1) : a, b, c가 쌓이며 dim 값에 따라 a, b, c가 쌓이는 방향이 달라진다. \n",
    "\n",
    "    10. torch.stack((a , b, ...), dim = o or 1, out = None) : torch.cat와 같은 방식으로 쌓이지만 텐서의 크기가 같아야 사용할 수 있다.  \n",
    "\n",
    "    11. torch.reshape(input, (a, b)): input값의 shape를 a x b로 바꿔준다. \n",
    "\n",
    "    12. torch.clamp(input, a, b):  min값 a, max을 b로 설정해서 input 값을 바꾼다. \n",
    "\n",
    "    13. input.view(a, b) : input의 shape을 a x b로 변경\n",
    "\n",
    "    14. torch.norm(input) : 텐서의 모든 값들을 모두 제곱해서 더한 후 제곱근을 씌운다. \n",
    "\n",
    "    15. torch.masked_select(input, mask) : input를 mask영역을 참고해서 인덱싱한다. \n",
    "\n",
    "        Ex) x = torch.tensor([1,2,3],[4,5,6]) \n",
    "\n",
    "        mask = torch.ByteTensor([[0,0,1],[0,1,0]])\n",
    "        \n",
    "        torch.masked_select(x, mask) \n",
    "\n",
    "        출력 : tensor([3,5])\n",
    "\n",
    "    16. torch.chunk(tensor, chunks, dim=0) : 텐서의 데이터를 chunks만큼 나눈다. 마지막에 데이터가 부족한 경우 그냥 내보냄\n",
    "\n",
    "        자세하게 설명하자면 torch.chunk(tensor, 3, dim=0)  0번째 축을 기준으로 텐서의 데이터를 3개로 나눈다\n",
    "\n",
    "    17. torch.split(tensor, split_size, dim=0) : 텐서의 데이터를 나눈다. 다만 각 데이터를 split_size만큼 채워서 나눈다. 마지막에 데이터가 부족한 경우 그냥 내보냄\n",
    "    \n",
    "        자세하게 설명하자면 torch.split(tensor, 3, dim=0) 0번째 축을 기준으로 텐서의 데이터를 몇개로 나눌지는 모르겠으나, 반드시 각 데이터를 3개씩 가지게끔 나눔\n",
    "\n",
    "    18. torch.squeeze(input, dim) : 차원중에 1로 되어 있는 차원을 압축한다. 만약 dim을 설정하면 그 축을 가진 차원만 압축한다.\n",
    "\n",
    "        Ex) x = torch.zeros(2,1,2,1,2) # (2,1,2,1,2) 차원을 가진 행렬이 나온다.\n",
    "\n",
    "            torch.squeeze(x) # x의 차원이 (2,2,2)로 되었다\n",
    "            torch.squeeze(x,1) # x의 차원이 (2,2,1,2)가 되었다 \n",
    "\n",
    "    19. torch.unsqueeze(input, dim) : dim인 축을 압축한다. squeeze와 동일하나 dim을 반드시 입력받아야 한다는 차이가 존재한다.\n",
    "    #  \n",
    "\n",
    "\n",
    "- 텐서의 차원 조작\n",
    "    x1 = torch.tensor([[1,2], [3,4]]) 이라 가정\n",
    "    \n",
    "    1. x1.reshape(a,b) : x1행렬을 a x b로 변형합니다\n",
    "\n",
    "        추가적으로 view도 존재하긴 하나 reshape를 사용하는것을 권장합니다(아마 버그 때문인듯...?)\n",
    "\n",
    "    2. x1.t() : 행렬 전치\n",
    "\n",
    "    3. x.cuda() : GPU 타입의 텐서로 변환\n",
    "\n",
    "    4. x.cpu() : cpu 타입의 텐서로 변환\n",
    "\n",
    "    6. x.size() : 텐서 사이즈 확인\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18197408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([8, 5, 7, 3, 2, 4, 9, 0, 6, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x =  torch.randperm(10)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9476b0b",
   "metadata": {},
   "source": [
    "### 3. 텐서의 속성\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ffcadde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape : torch.Size([3, 4])\n",
      "dtype : torch.float32\n",
      "device : cpu\n",
      "shape[0] 3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tensor = torch.rand(3,4)\n",
    "print('shape :',tensor.shape) # 텐서의 모양 반환\n",
    "print('dtype :',tensor.dtype) # 텐서의 자료형 반환\n",
    "print('device :',tensor.device) # 텐서의 저장 장치 반환\n",
    "\n",
    "print('shape[0]',tensor.shape[0]) # 0번째 축을 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db52872e",
   "metadata": {},
   "source": [
    "이때 저 0번째 축을 반환은 어떤 원리로 돌아가느냐?\n",
    "\n",
    "x.shape 는 튜플을 반환하는데 그 튜플에서 인덱스를 지정한 것이다. \n",
    "\n",
    "x[0].shape와 x.shape[0]는 아예 다른 문장이다.\n",
    "\n",
    "첫번째 문장은 x에서 0번째에 해당하는 텐서의 shape를 반환하라는 것이고\n",
    "\n",
    "두번째 문장은 x에서 텐서의 shape중 0번째 인덱스를 반환하라는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e36526",
   "metadata": {},
   "source": [
    "### 4. 텐서 연산\n",
    "---\n",
    "\n",
    "전치, 인덱싱, 슬라이싱, 수학 계산, 선형 대수, 임의 샘플링 등등 엄청 많은 텐서의 연산 기능\n",
    "\n",
    "자세한 내용은 : https://pytorch.org/docs/stable/torch.html 참고\n",
    "\n",
    "이 밑에 기록할 내용들은 한번이라도 사용했으면 적는걸로 한다. \n",
    "\n",
    "1. tensor = tensor.to(\"cuda\")\n",
    "\n",
    "    기본적으로 텐서는 CPU에 생성됩니다. .to 메소드를 사용하면 (GPU의 가용성(availability)을 확인한 뒤) GPU로 텐서를 명시적으로 이동할 수 있습니다.\n",
    "\n",
    "2. t[:,2], t[t>3]\n",
    "\n",
    "    인덱스 조작은 여러가지가 있지만 마스크 배열을 이용한 인덱스 조작은 기억해두자\n",
    "    \n",
    "    Ex) t[t>5] = 20 # 마스크 배열을 사용하여 일괄 대입\n",
    "\n",
    "3. a @ b 는 행렬곱이고 a * b는 행렬의 각 원소별로 곱한다\n",
    "\n",
    "4. torch.exp(tensor,out=None), torch.log(input, out-None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588498a0",
   "metadata": {},
   "source": [
    "### 4.산술 연산\n",
    "---\n",
    "Dezero에서 만든것 그대로이다. 오버로딩이 되어있어서 단순하게 +, *, /, -로 연산이 가능\n",
    "\n",
    "자세한 것은 https://statisticsplaybook.tistory.com/6 참고\n",
    "\n",
    "1. 바꿔치기(in-place) 연산\n",
    "    - 연산 결과를 피연산자에 저장하는 연산이다. _ 접미사를 가진다 (Ex:x.copy_(y), x.t_())\n",
    "    - tensor.add_(5)\n",
    "\n",
    "      print(tensor) \n",
    "\n",
    "      tensor([[1., 0., 1., 1.],\n",
    "\n",
    "        [1., 0., 1., 1.],\n",
    "\n",
    "        [1., 0., 1., 1.],\n",
    "\n",
    "        [1., 0., 1., 1.]])\n",
    "\n",
    "      tensor([[6., 5., 6., 6.],\n",
    "\n",
    "        [6., 5., 6., 6.],\n",
    "\n",
    "        [6., 5., 6., 6.],\n",
    "        \n",
    "        [6., 5., 6., 6.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec44367",
   "metadata": {},
   "source": [
    "### 5.Numpy 변환\n",
    "---\n",
    "1. t = torch.ones(5)\n",
    "   n = t.numpy()\n",
    "\n",
    "2. 반대로 numpy에서 텐서로 바꿀 수 있다.\n",
    "    n = np.ones(5)\n",
    "    t = torch.from_numpy(n)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch_studing",
   "language": "python",
   "name": "pytorch_studing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
