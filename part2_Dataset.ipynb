{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset, DataLoader\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Dezero에서의 Dataset, DataLodaer\n",
    "---\n",
    "Daatset을 설정한 뒤에 DataLoder로 설정한 Dataset을 끌고 온다.\n",
    "\n",
    "Dataset 은 샘플과 정답(label)을 저장하고, DataLoader 는 Dataset 을 샘플에 쉽게 접근할 수 있도록 순회 가능한 객체(iterable)로 감싼다.\n",
    "\n",
    "> PyTorch의 도메인 특화 라이브러리들은 (FashionMNIST와 같은) 미리 준비해둔(pre-loaded) 다양한 데이터셋을 제공합니다.    \n",
    "> 데이터셋은 torch.utils.data.Dataset 의 하위 클래스로 개별 데이터를 특정하는 함수가 구현되어 있습니다.    \n",
    "> 이러한 데이터셋은 모델을 만들어보고(prototype) 성능을 측정(benchmark)하는데 사용할 수 있습니다.    \n",
    "> 1. 비디오 데이터셋 - https://pytorch.org/vision/stable/datasets.html  \n",
    "> 2. 텍스트 데이터셋 - https://pytorch.org/text/stable/datasets.html  \n",
    "> 3. 오디오 데이터셋 - https://pytorch.org/audio/stable/datasets.html  \n",
    "\n",
    "\n",
    "Dataset은 왜 만들었을까?\n",
    "\n",
    "데이터가 100만개라면, 거대한 데이터를 하나의 ndarray 인스턴스로 처리하면 모든 원소를 한꺼번에 메모리에 올려야 한다. \n",
    "\n",
    "당연히 메모리는 부담스러운 상황\n",
    "\n",
    "그렇기 때문에 이러한 문제에 대응이 가능하도록 Dataset 클래스를 만들었다.\n",
    "\n",
    "\n",
    "### 2. Dezero의 Dataset(복습)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Dataset: # Dezero의 Dataset\n",
    "    def __init__(self, train=True, transform=None, target_transform=None):\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "        if self.transform is None:\n",
    "            self.transform = lambda x: x\n",
    "        if self.target_transform is None:\n",
    "            self.target_transform = lambda x: x\n",
    "\n",
    "        self.data = None # 입력 데이터 보관\n",
    "        self.label = None # 레이블 보관\n",
    "        self.prepare()\n",
    "\n",
    "    def __getitem__(self, index): # 객체에 슬라이싱이 가능하게 하는 특수 메소드 \n",
    "        assert np.isscalar(index) # 만약에 받은 요소가 스칼라이면 실행\n",
    "        if self.label is None: # 레이블이 존재치 않는다면\n",
    "            return self.transform(self.data[index]), None # 값에 인덱싱해서 반환\n",
    "        else:\n",
    "            return self.transform(self.data[index]), self.target_transform(self.label[index])\n",
    "\n",
    "    def __len__(self): # 데이터셋 길이 반환\n",
    "        return len(self.data)\n",
    "\n",
    "    def prepare(self): # 유도 쿨래스에서 마저 정의해야 한다. \n",
    "        pass\n",
    "\n",
    "class Spiral(Dataset): # 이렇게 데이터셋에 각종 데이터를 가져올 수 있게끔 설정해 뒀을것이다. \n",
    "    def prepare(self): # 이런식으로 인스턴스 변수인 data와 label데이터를 설정한다\n",
    "        self.data, self.label = get_spiral(self.train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorch또한 위와 비슷한 형상을 띄고 있을것이다. \n",
    "\n",
    "여기서 중요한 것은 데이터셋은 이미 다 정의가 되어있다는 것\n",
    "\n",
    "파이토치가 정해준 데이터셋을 그냥 이용하면 끝이다. 물론 튜닝을 할 수 있지만... 굳이?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. pytorch의 dataset\n",
    "---\n",
    "\n",
    "기본적으로 제공되는 틀인 Dataset이라는 클래스가 존재한다.\n",
    "\n",
    "이 클래스를 상속받고 \n",
    "\n",
    "__init__(self) : 메서드는 객체를 생성할 때 실행되는 메서드, 즉 생성자입니다. 여기에는 모델에 사용할 데이터를 담아두는 등 어떤 인덱스가 주어졌을 때 반환할 수 있게 만드는 초기 작업을 수행합니다.\n",
    "\n",
    "__len__(self) : 학습에 사용할 데이터의 총 개수라고 볼 수 있는데, 즉 얼마만큼의 인덱스를 사용할지를 반환하는 메서드입니다.\n",
    "\n",
    "__getitem__(self, index) : 메서드는 어떤 인덱스가 주어졌을 때 해당되는 데이터를 반환하는 메서드입니다. numpy 배열이나 텐서 형식으로 반환합니다. 보통 입력과 출력을 튜플 형식으로 반환하게 됩니다.\n",
    "\n",
    "들을 정의해주면 사용자 전용 Dataset 완성\n",
    "\n",
    "찾아보니 __len__ 메서드와 __getitiem__ 메서드를 지닌 어떤 것도 죄다 Dataset이 될 수 있다고 한다. (물론 오류에 대한 책임은 작성자가 지어야 한다.)\n",
    "\n",
    "> 사용자 정의 Dataset 클래스는 반드시 3개 함수를 구현해야 합니다: __init__, __len__, and __getitem__. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __getitem__(self, index):\n",
    "        pass\n",
    "    def __len__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Dezero의 Dataloader\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, dataset, batch_size, shuffle=True, gpu=False):\n",
    "        self.dataset = dataset # 데이터 셋\n",
    "        self.batch_size = batch_size # 배치 크기\n",
    "        self.shuffle = shuffle # 에포크별로 셔플\n",
    "        self.data_size = len(dataset) # 데이터 사이즈 \n",
    "        self.max_iter = math.ceil(self.data_size / batch_size) # 최대 반복수\n",
    "        self.gpu = gpu # GPU\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.iteration = 0 # 반복자를 0으로 설정, 반복자는 데이터를 순차적으로 추출하는 기능을 제공한다.\n",
    "        if self.shuffle: # 만약에 셔플할 생각이거든\n",
    "            self.index = np.random.permutation(len(self.dataset)) # 그리 해주마\n",
    "        else: # 아니거든\n",
    "            self.index = np.arange(len(self.dataset)) #그냥 천천히 반환해라\n",
    "\n",
    "    def __iter__(self): # 클래스를 파이썬 반복자로 사용하기 위해 넣은 특수 메서드 \n",
    "        return self # 특수 메서드를 구현하고 자기 자신을 반환하도록 하는게 기본 문법.\n",
    "\n",
    "    def __next__(self): # 반복자에서 데이터를 순서대로 추출하기 위한 메서드\n",
    "        if self.iteration >= self.max_iter: #최대 반복자수보다 크거나 같다면\n",
    "            self.reset() # 아 ㅋㅋ 리셋해야지 뭐\n",
    "            raise StopIteration() # 다음 원소를 반환하도록 구현, 반환할 원소가 없으면 StopIteration()을 수행 \n",
    "\n",
    "        i, batch_size = self.iteration, self.batch_size\n",
    "        \n",
    "        batch_index = self.index[i * batch_size:(i + 1) * batch_size]\n",
    "        batch = [self.dataset[i] for i in batch_index]\n",
    "\n",
    "        xp = cuda.cupy if self.gpu else np\n",
    "        x = xp.array([example[0] for example in batch])\n",
    "        t = xp.array([example[1] for example in batch])\n",
    "\n",
    "        self.iteration += 1\n",
    "        return x, t # 데이터와 레이블을 반환\n",
    "\n",
    "    def next(self):\n",
    "        return self.__next__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset과는 달리 DataLoader는 그닥 사용자가 수정할 이유가 없다.\n",
    "\n",
    "DataLoader는 그저 Dataset를 기반으로 데이터를 꺼내는 것일뿐 그 이상의 기능을 할 이유가 없다.\n",
    "\n",
    "### 5. pytorch의 DataLoader\n",
    "---\n",
    "정의는 다음과 같다.\n",
    "\n",
    "DataLoader(dataset, batch_size=1, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=None,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=None)\n",
    "\n",
    "\n",
    "parameter 설명\n",
    "\n",
    "1. dataset는 필수 입력 인자다.\n",
    "\n",
    "2. bath_size = 1 : 배치 크기이다.\n",
    "\n",
    "3. shuffle = False : 에포크별로 데이터셋을 뒤섞을건지 여부를 결정하는 것\n",
    "\n",
    "나머지는 많이 사용하지는 않는다. 그 외 설명은 다음 글을 참고하길 바란다.\n",
    "> DataLoader parameter 별 용도 : https://subinium.github.io/pytorch-dataloader/\n",
    "\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "\n",
    "``` py\n",
    "# DataLoader 사용 예제\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    for x, t in train_dataloader:\n",
    "        print(x.shape, t.shape)\n",
    "\n",
    "        ...\n",
    "        \n",
    "```\n",
    "\n",
    "> https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader 데이터로더에 대한 자세한 파라미터 값"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 직접 데이터셋 만들기\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, t):\n",
    "        # 데이터셋을 처음 선언할 때, 즉 데이터셋 오브젝트가 생길 때 자동으로 불리는 함수이고, 여기에 우리가 몇 가지 인수들을 입력받도록 만들 수 있다 (path, transform 같은 것들).\n",
    "\n",
    "    def __len__(self):\n",
    "        # 데이터셋의 길이다. 만약 dataset을 선언하고 나서 len(어떤 dataset)을 하면 내부적으로는 이 len 함수가 불리는 것이다. \n",
    "        # 이 len은 나중에 데이터셋을 선언하고 데이터로더를 사용할 때 또 내부적으로 사용된다. \n",
    "        # (데이터셋의 len을 알아야 데이터로더가 미니 배치 샘플링을 하면서 지금 다 돌았는지 아닌지를 알 수 있으니까)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 이름에서 알 수 있듯이 데이터셋의 본분인 데이터 하나씩 뽑기이다. \n",
    "        # idx는 index를 말하는데, 몇 번째 데이터를 뽑을 건지에 대한 변수이다. 이는 데이터로더에서 또 사용될 것이다.\n",
    "        # return으로 몇개의 데이터를 반환 할건지 정할 수 있다"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "밑바닥부터 시작하는 딥러닝2 의 CBOW에서 사용할 데이터셋을 만들어 보겠다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트를 넣으면 자동적으로 말뭉치와 word_to_id, id_to_word를 만들어 준다\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace('.', ' .')\n",
    "    words = text.split(' ')\n",
    "\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    for word in words:\n",
    "        if word not in word_to_id:\n",
    "            new_id = len(word_to_id)\n",
    "            word_to_id[word] = new_id\n",
    "            id_to_word[new_id] = word\n",
    "\n",
    "    corpus = np.array([word_to_id[w] for w in words])\n",
    "\n",
    "    return corpus, word_to_id, id_to_word\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "def create_contexts_target(corpus, window_size=1):\n",
    "    '''맥락과 타깃 생성\n",
    "\n",
    "    :param corpus: 말뭉치(단어 ID 목록)\n",
    "    :param window_size: 윈도우 크기(윈도우 크기가 1이면 타깃 단어 좌우 한 단어씩이 맥락에 포함)\n",
    "    :return:\n",
    "    '''\n",
    "    target = corpus[window_size:-window_size]\n",
    "    contexts = []\n",
    "\n",
    "    for idx in range(window_size, len(corpus)-window_size):\n",
    "        cs = []\n",
    "        for t in range(-window_size, window_size + 1):\n",
    "            if t == 0:\n",
    "                continue\n",
    "            cs.append(corpus[idx + t])\n",
    "        contexts.append(cs)\n",
    "\n",
    "    return np.array(contexts), np.array(target)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "def convert_one_hot(corpus, vocab_size):\n",
    "    '''원핫 표현으로 변환\n",
    "\n",
    "    :param corpus: 단어 ID 목록(1차원 또는 2차원 넘파이 배열)\n",
    "    :param vocab_size: 어휘 수\n",
    "    :return: 원핫 표현(2차원 또는 3차원 넘파이 배열)\n",
    "    '''\n",
    "    N = corpus.shape[0]\n",
    "\n",
    "    if corpus.ndim == 1:\n",
    "        one_hot = np.zeros((N, vocab_size), dtype=np.int32)\n",
    "        for idx, word_id in enumerate(corpus):\n",
    "            one_hot[idx, word_id] = 1\n",
    "\n",
    "    elif corpus.ndim == 2:\n",
    "        C = corpus.shape[1]\n",
    "        one_hot = np.zeros((N, C, vocab_size), dtype=np.int32)\n",
    "        for idx_0, word_ids in enumerate(corpus):\n",
    "            for idx_1, word_id in enumerate(word_ids):\n",
    "                one_hot[idx_0, idx_1, word_id] = 1\n",
    "\n",
    "    return one_hot    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리가 만들 데이터셋은 다음과 같은 기능을 해아한다\n",
    "\n",
    "1. 텍스트만 넣었는데 추론 기반 기법으로 학습할 수 있게끔 데이터들을 추출할 수 있어야 한다! 즉 다음과 같은 변환을 자동적으로 해줘야 한다.\n",
    "</br> </br>\n",
    "\n",
    "![1](./%EC%9D%B4%EB%AF%B8%EC%A7%80/image_20.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "text = 'Yoy say goodbye and I say hello.'\n",
    "\n",
    "class Corpus_Dataset(Dataset):\n",
    "    def __init__(self, text, window_size = 1):\n",
    "        self.window_size = window_size\n",
    "        self.corpus, self.word_to_id, self.id_to_word = preprocess(text) # 먼저 텍스트를 말뭉치와 word_to_id로 바꾼다\n",
    "        self.vocab_size = len(self.word_to_id)\n",
    "\n",
    "        contexts, target = create_contexts_target(self.corpus, self.window_size)\n",
    "        self.contexts = convert_one_hot(contexts, self.vocab_size)\n",
    "        self.target = convert_one_hot(target, self.vocab_size)\n",
    "        self.target = target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.contexts) # 데이터의 갯수 반환 형상이 (6,2,7)이면 데이터의 갯수는 6이다.\n",
    "\n",
    "    def __getitem__(self, idx): # 데이터를 몇개씩 뽑을거냐 인데 저렇게 해두면 데이터가 올바르게 추출된다\n",
    "        input = torch.tensor(self.contexts[idx], dtype = torch.float32)\n",
    "        label = torch.tensor(self.target[idx], dtype = torch.long) # 손실함수 구할때 라벨은 long로 해야 한다고 하더라...\n",
    "        return input, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 7])\n",
      "torch.Size([1, 2, 7])\n",
      "torch.Size([1, 2, 7])\n",
      "torch.Size([1, 2, 7])\n",
      "torch.Size([1, 2, 7])\n",
      "torch.Size([1, 2, 7])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = corpus_Dataset(text)\n",
    "dataLoader = DataLoader(dataset)\n",
    "\n",
    "for input, label in dataLoader:\n",
    "    print(input.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('Pytorch_studing')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d060f0828fc2b117241290c294bd7cf3e183c40a44f56e1a2627712ee0c7af9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
