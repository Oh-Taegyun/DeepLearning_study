{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN\n",
    "---\n",
    "> https://pytorch.org/docs/stable/generated/torch.nn.RNN.html#torch.nn.RNN\n",
    "torch.nn.RNN()\n",
    "\n",
    "### 1. RNN 파라미터\n",
    "---\n",
    "\n",
    "![](./%EC%9D%B4%EB%AF%B8%EC%A7%80/image_27.png)\n",
    "\n",
    "1. 파라미터\n",
    "\n",
    "    1. input_size \n",
    "\n",
    "        입력해 주는 특성 값의 개수 입니다. 만약 feature의 개수가 1이라면 input_size=1, 입력 feature 개수가 7개면 input_size=7을 입력합니다.\n",
    "\n",
    "    2. hidden_size\n",
    "\n",
    "        hidden state의 개수를 지정합니다. 보통 arbitrary 합니다.\n",
    "\n",
    "    3. num_layers\n",
    "\n",
    "        RNN 레이어를 겹겹이 쌓아올릴 수 있습니다. RNN 레이어를 쌓아 올리는 것을 stacking RNN이라고도 합니다. 만약, 2개층을 겹겹이 쌓아올린다면 num_layers=2 로 설정하면 됩니다. 기본 값: 1\n",
    "\n",
    "    4. nonlinearity\n",
    "\n",
    "        사용할 비선형성입니다. 'tanh' 또는 'relu'일 수 있습니다. 기본값: 'tanh'\n",
    "\n",
    "    5. bias\n",
    "\n",
    "        False인 경우 레이어는 바이어스 가중치 b_ih와 b_hh를 사용하지 않습니다. 기본값: True\n",
    "\n",
    "    6. batch_first\n",
    "\n",
    "        입력으로 받는 데이터의 shape중 첫 번째 차원을 batch로 간주할 것인지를 설정합니다. 일반적으로 pytorch에서 데이터 전처리시 batch를 첫번째 차원으로 지정하기 때문에 많은 케이스에서 batch_firtst=True 로 지정함을 볼 수 있습니다. True이면 입력 및 출력 텐서가 (seq, batch, feature) 대신 (batch, seq, feature)로 제공됩니다. 이는 hidden 또는 cell state에는 적용되지 않습니다. 기본값: False\n",
    "\n",
    "    7. dropout\n",
    "\n",
    "        0이 아닌 경우, 마지막 레이어를 제외한 각 RNN 레이어의 출력에 드롭아웃 레이어를 도입하며, 드롭아웃 확률은 dropout과 같습니다. 기본값: 0\n",
    "        \n",
    "    8. bidirectional\n",
    "\n",
    "         True이면 양방향 RNN이 됩니다. 기본값: False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 사용 예제\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` py\n",
    ">>> rnn = nn.RNN(10, 20, 2)\n",
    ">>> input = torch.randn(5, 3, 10)\n",
    ">>> h0 = torch.randn(2, 3, 20)\n",
    ">>> output, hn = rnn(input, h0)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(1, 10, num_layers=2, batch_first=True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = 1  # 입력 데이터 특성 차원\n",
    "hidden_dim = 10 # hidden state 차원\n",
    "n_layers = 2    # stacking layer 개수\n",
    "\n",
    "rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)\n",
    "rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20,)\n",
      "(20, 1)\n",
      "Input size:  torch.Size([1, 20, 1])\n"
     ]
    }
   ],
   "source": [
    "# 20개의 시퀀스 생성\n",
    "seq_length = 20\n",
    "\n",
    "time_steps = np.linspace(0, np.pi, seq_length*input_size)\n",
    "print(time_steps.shape)\n",
    "# 출력\n",
    "# (20,)\n",
    "\n",
    "data = np.sin(time_steps)\n",
    "data.resize((seq_length, 1))\n",
    "print(data.shape)\n",
    "# 출력\n",
    "# (20, 1)\n",
    "\n",
    "# 배치 차원 추가(0번째)\n",
    "input_data = torch.Tensor(data).unsqueeze(0)\n",
    "print('Input size: ', input_data.shape)\n",
    "# Input size:  torch.Size([1, 20, 1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  torch.Size([1, 20, 10])\n",
      "Hidden State:  torch.Size([2, 1, 10])\n"
     ]
    }
   ],
   "source": [
    "# RNN 출력(output, hidden_state)\n",
    "output, hidden_state = rnn(input_data, None)\n",
    "# rnn output, hidden_state 차원\n",
    "\n",
    "print('Output: ', output.size())\n",
    "print('Hidden State: ', hidden_state.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 20])\n",
      "torch.Size([2, 3, 20])\n"
     ]
    }
   ],
   "source": [
    "rnn = nn.RNN(10, 20, 2) \n",
    "input = torch.randn(5, 3, 10) # 기본적으로 (seq, batch, feature)\n",
    "h0 = torch.randn(2, 3, 20) # 기본적으로 (seq, batch, feature)\n",
    "\n",
    "output, hn = rnn(input, h0)\n",
    "print(output.shape)\n",
    "print(hn.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. RNN 만들어보기\n",
    "---\n",
    "아래 코드는 어떻게 작동되는 걸까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 20])\n",
      "torch.Size([2, 3, 20])\n"
     ]
    }
   ],
   "source": [
    "rnn = nn.RNN(10, 20, 2) \n",
    "input = torch.randn(5, 3, 10) # 기본적으로 (seq, batch, feature)\n",
    "h0 = torch.randn(2, 3, 20) # 기본적으로 (seq, batch, feature)\n",
    "\n",
    "output, hn = rnn(input, h0)\n",
    "print(output.shape) # [5, 3, 20]\n",
    "print(hn.shape) # [2, 3, 20]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일반적으로 3차원의 행렬을 계산하는 방법은 없다. 우리가 합성곱을 할때도 img2col이라는 특수 함수를 이용해서 다차원의 행렬을 2차원으로 바꿔서 행렬 계산을 했던것을 떠올려보라.\n",
    "\n",
    "그럼 RNN은 어떻게 구현이 되었을까? 파이토치의 RNN과 비슷하게 구현해보겠다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./%EC%9D%B4%EB%AF%B8%EC%A7%80/image_28.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Base(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.W_x = nn.Linear(input_size, hidden_size) \n",
    "        self.W_h = nn.Linear(input_size, hidden_size) \n",
    "        self.h = None # 은닉 상태 벡터\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.h = None # 은닉 상태를 초기화\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.h is None: #현재 가진 은닉 상태가 없다면?\n",
    "            h_new = F.tanh(self.W_x(x)) # 그냥 입력값으로만 계산해서 첫번째 은닉 상태를 추출 tanh( x * W_x )\n",
    "        else: # 은닉 상태가 있다면\n",
    "            h_new = F.tanh(self.W_x(x) + self.W_h(self.h)) # tanh( (x * W_x) + (h * W_h + b) )\n",
    "            self.h = h_new\n",
    "        return h_new"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. RNN의 입력과 출력에 관해서\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 참고하면 좋은 링크 : https://wegonnamakeit.tistory.com/52\n",
    "\n",
    "![](./%EC%9D%B4%EB%AF%B8%EC%A7%80/image_29.png)\n",
    "\n",
    "N : 미니배치 크기\n",
    "\n",
    "D : 입력 벡터 차원 수\n",
    "\n",
    "H : 은닉 상태 차원 수\n",
    "\n",
    "T : 길이가 T인 시계열 데이터 (간단히 말하자면 문장정도?)\n",
    "\n",
    "F : bidirectional ( 양방향일경우 : 2, 단방향일경우 : 1 ) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN에 들어가는 input은 총 2가지이다. input (위 사진에선 x<sub>t</sub>) 그리고 은닉상태인 h (위 사진에선 h<sub>t-1</sub>)\n",
    "\n",
    "1. input의 형상 \n",
    "\n",
    "    1. 배치가 없는 input의 형상 : (Sequence length, input_size) \n",
    "\n",
    "        간단히 표기하면 (T, D)\n",
    "\n",
    "    2. 배치가 있으면서 batch_first=False 일때 input의 형상 : (Sequence length, batch_size, input_size)\n",
    "\n",
    "        간단히 표기하면 (T, N, D)\n",
    "\n",
    "    3. 배치가 있으면서 batch_first=True 일때 input의 형상 : (batch_size, Sequence length, input_size)\n",
    "\n",
    "        간단히 표기하면 (N, T, D)\n",
    "\n",
    "2. h의 형상\n",
    "\n",
    "    1. 배치가 없을 경우 : ( bidirectional ( 양방향일경우 : 2, 단방향일경우 : 1 ) * num_layers, hidden_size)\n",
    "\n",
    "        간단히 표기하면 (F * num_layers, H) \n",
    "\n",
    "        F = bidirectional ( 양방향일경우 : 2, 단방향일경우 : 1 ) * num_layers 인데 단방향일 경우 통과한 RNN의 횟수정도로 생각하면 편하다\n",
    "\n",
    "        RNN을 겹겹이 쌓아 올렸다는 것은 다시 말해 밑의 사진처럼 쌓아 올린것(여러번 통과시킨 것)\n",
    "        \n",
    "        ![](./%EC%9D%B4%EB%AF%B8%EC%A7%80/image_33.png)\n",
    "        \n",
    "        즉, 단어가 RNN을 통과한 횟수라고 생각하면 편하다 (아니면 내가 통과하도록 설정한 레이어의 갯수정도)\n",
    "\n",
    "        단방향일 경우 그저 (T, H) 인것 \n",
    "\n",
    "        아마 추측하건데 h의 정보를 담고 있는것 같다, 무슨 뜻인지는 잘은 모르겠다....\n",
    "\n",
    "\n",
    "\n",
    "    2. 배치가 있을 경우 : ( bidirectional ( 양방향일경우 : 2, 단방향일경우 : 1 ) * num_layers, batch_size, hidden_size)\n",
    "\n",
    "        간단히 표기하면 (F * num_layers, N, H)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN에서 나오는 output은 총 2가지이다.\n",
    "\n",
    "1. output의 형상\n",
    "\n",
    "    1. 배치가 없는 output의 형상 : (Sequence length, bidirectional ( 양방향일경우 : 2, 단방향일경우 : 1 ) * num_layers * hidden_size) \n",
    "\n",
    "        간단히 표기하면 (T, F * H)\n",
    "\n",
    "    2. 배치가 있으면서 batch_first=False 일때 output의 형상 : (Sequence length, batch_size, bidirectional ( 양방향일경우 : 2, 단방향일경우 : 1 ) * num_layers * hidden_size)\n",
    "\n",
    "        간단히 표기하면 (T, N, F * H)\n",
    "\n",
    "    3. 배치가 있으면서 batch_first=True 일때 output의 형상 : (batch_size, Sequence length, bidirectional ( 양방향일경우 : 2, 단방향일경우 : 1 ) * num_layers * hidden_size)\n",
    "\n",
    "        간단히 표기하면 (N, T, F * H)\n",
    "\n",
    "2. h의 형상\n",
    "\n",
    "    1. 배치가 없을 경우 : ( bidirectional ( 양방향일경우 : 2, 단방향일경우 : 1 ) * num_layers, hidden_size)\n",
    "\n",
    "        간단히 표기하면 (F * num_layers, H)\n",
    "\n",
    "    2. 배치가 있을 경우 : ( bidirectional ( 양방향일경우 : 2, 단방향일경우 : 1 ) * num_layers, batch_size, hidden_size)\n",
    "\n",
    "        간단히 표기하면 (F * num_layers, N, H)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Sequence length가 무엇일까?\n",
    "---\n",
    "\n",
    "![](./%EC%9D%B4%EB%AF%B8%EC%A7%80/image_30.png)\n",
    "    \n",
    "Sequence length는 Input data의 길이를 뜻한다. PyTorch에서는 값을 자동으로 계산하기 때문에 input_size 값만 잘 입력해주면 된다.\n",
    "\n",
    "이거는 '밑바닥부터 시작하는 딥러닝 2'의 RNN과 TimeRNN의 차이라고 생각하면 편하다.\n",
    "\n",
    "TimeRNN 계층은 RNN 계층을 T개 연결한 신경망이다. \n",
    "\n",
    "![](./%EC%9D%B4%EB%AF%B8%EC%A7%80/image_31.png)\n",
    "\n",
    "![](./%EC%9D%B4%EB%AF%B8%EC%A7%80/image_32.png)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Base(nn.Module): # 딱 1개의 단어 벡터만 입력 가능\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.W_x = nn.Linear(input_size, hidden_size) \n",
    "        self.W_h = nn.Linear(input_size, hidden_size) \n",
    "        self.h = None # 은닉 상태 벡터\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.h = None # 은닉 상태를 초기화\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.h is None: #현재 가진 은닉 상태가 없다면?\n",
    "            h_new = F.tanh(self.W_x(x)) # 그냥 입력값으로만 계산해서 첫번째 은닉 상태를 추출 tanh( x * W_x )\n",
    "        else: # 은닉 상태가 있다면\n",
    "            h_new = F.tanh(self.W_x(x) + self.W_h(self.h)) # tanh( (x * W_x) + (h * W_h + b) )\n",
    "            self.h = h_new\n",
    "        return h_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module): # T개의 시계열 데이터 입력 가능 \n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.RNN = RNN_Base(input_size, hidden_size)\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.RNN.reset_state()\n",
    "\n",
    "    def forward(self, xs, h): \n",
    "        N, T, D = xs.shape # (batch_size, Sequence length, input_size) \n",
    "        H = self.hidden_size\n",
    "        # N : 미니배치 크기, T: 길이가 T인 시계열 데이터, D : 입력 벡터 차원 수, H : 은닉 상태 차원 수\n",
    "\n",
    "        hs = torch.tensor.zeros(N,T,H) # 출력 hs\n",
    "\n",
    "        for t in range(T): # RNN_Base은 단어 1개뿐이 입력이 안되니까 하나씩 꺼내서 입력하자\n",
    "            h = self.RNN(xs[:,t,:]) \n",
    "            hs[:,t,:] = H\n",
    "\n",
    "        return hs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 헷갈리네요... 왜 파이토치의 RNN은 2개를 반환하죠??\n",
    "---\n",
    "정확하게 무엇을 반환하는지 알아야 한다.\n",
    "\n",
    "RNN 셀은 두 개의 입력을 리턴하는데, 첫번째 리턴값은 모든 시점(timesteps)의 은닉 상태들이며, 두번째 리턴값은 마지막 시점(timestep)의 은닉 상태입니다. \n",
    "\n",
    "![](../image/image_34.jpg)\n",
    "\n",
    "![](../image/image_36.jpg)\n",
    "\n",
    "![](../image/image_35.jpg)\n",
    "\n",
    "이렇게 암기하면 된다!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m rnn \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mRNN(\u001b[39m10\u001b[39m, \u001b[39m20\u001b[39m) \n\u001b[0;32m      2\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39m5\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m10\u001b[39m) \u001b[39m# 기본적으로 (seq, batch, feature)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m h0 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m20\u001b[39m) \u001b[39m# 기본적으로 (seq, batch, feature)\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "rnn = nn.RNN(10, 20) \n",
    "input = torch.randn(5, 3, 10) # 기본적으로 (seq, batch, feature)\n",
    "h0 = torch.randn(2, 3, 20) # 기본적으로 (seq, batch, feature)\n",
    "\n",
    "output, hn = rnn(input, h0)\n",
    "print(output.shape) # [5, 3, 20]\n",
    "print(hn.shape) # [2, 3, 20]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch_studing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d060f0828fc2b117241290c294bd7cf3e183c40a44f56e1a2627712ee0c7af9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
