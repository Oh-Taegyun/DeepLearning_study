{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 제작 순서에 따른 이론 한눈에 보기\n",
    "---\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. 라이브러리 호출\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd \n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms # 데이터 전처리를 위해 사용하는 라이브러리\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. device 설정\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지금 사용하는 device : cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") #  torch.cuda.is_available() GPU를 사용가능하면 True, 아니라면 False를 리턴\n",
    "\n",
    "print(\"지금 사용하는 device :\",device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 데이터셋 준비\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 신경망 생성\n",
    "---\n",
    "\n",
    "보통 출력층으로 회귀일 경우 항등함수, 분류일 경우 소프트맥스 함수로 해둔다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5)\n",
    "        self.conv2 = nn.Conv2d(20, 20, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        return F.relu(self.conv2(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 모델 설정 완료\n",
    "---\n",
    "\n",
    "1. 신경망을 모델로 설정하기\n",
    "\n",
    "2. 손실함수 설정 (criterion)\n",
    "    - 오차 제곱합, 교차 엔트로피 오차 등등...\n",
    "    </br></br>\n",
    "\n",
    "3. 옵티마이저 설정 (optimizer)\n",
    "    신경망 학습의 목적은 손실 함수의 값을 가능한 한 낮추는 매개변수를 찾는 것이다. 이는 곧 매개변수의 최적값을 찾는 문제이며, 이러한 문제를 푸는 것이 최적화(optimization)이라 한다. \n",
    "\n",
    "    최적화 기법은 여러가지가 있다. (SGD, 모멘텀, Adam 등등..) 이러한 최적화를 해주는 클래스가 옵티마이저(Optimizer)이다. 옵티마이저를 한국어로 해석하면 '최적화를 행하는 자'라는 뜻이다.\n",
    "\n",
    "---\n",
    "\n",
    "+ 경사법 설정 이론\n",
    "    - 경사법 : 보통 그레디언트가 가리키는 쪽은 각 장소에서 함수의 출력 값을 가장 크게 줄이는 방향이다. 우리는 손실함수를 가지고 모델의 정확도를 평가한다.\n",
    "\n",
    "        ![1](./%EC%9D%B4%EB%AF%B8%EC%A7%80/image_13.png)\n",
    "\n",
    "        입력 데이터(x)를 넣어서 손실 함수(y)가 나왔으니, 손실 함수에서 역전파를 흘려주면 \n",
    "\n",
    "        ![2](./%EC%9D%B4%EB%AF%B8%EC%A7%80/image_15.png)\n",
    "\n",
    "        손실 함수에 대한 기울기가 나오게 된다. (dx/dy)\n",
    "\n",
    "        ![3](./%EC%9D%B4%EB%AF%B8%EC%A7%80/image_14.png)\n",
    "    \n",
    "        따라서 손실 함수를 줄이는 방향으로 경사법을 사용해야 한다. \n",
    "\n",
    "        신경망이 최적의 매개변수를 가지고 있다면? 손실 함수가 줄어들것이다. 즉, 최적이란 손실 함수가 최솟값이 될 때의 매개변수 값이다. \n",
    "        \n",
    "        그러나 일반적인 문제의 손실 함수는 매우 복잡하다. 매개변수 공간이 광대하여 어디가 최솟값이 되는 곳인지를 짐작할 수 없다.\n",
    "\n",
    "        이런 상황에서 기울기를 잘 이용해 함수의 최솟값(또는 가능한 한 작은 값)을 찾으려는 것이 경사법이다.\n",
    "\n",
    "        > 경사법 : 현 위치에서 기울어진 방향으로 일정 거리만큼 이동한다. 그런 다음 이동한 곳에서도 기울기를 구한 뒤 또 기울어진 방향으로 나아간다 이렇게 해서 함수의 값을 줄이는 것이 경사법이다\n",
    "\n",
    "        ![4](./%EC%9D%B4%EB%AF%B8%EC%A7%80/image_16.png)\n",
    "\n",
    "        ```param.data -= self.lr * param.grad.data```\n",
    "\n",
    "        보통의 파라미터(매개변수)는 이렇게 업데이트가 된다. \n",
    "\n",
    "        ![5](./%EC%9D%B4%EB%AF%B8%EC%A7%80/image_17.jpg)\n",
    "\n",
    "        이렇게 모든 계층을 돌면서 각 계층마다 입력값을 줄이는 벡터(기울기)가 한번에 구해진다!\n",
    "        \n",
    "        입력값을 줄이는 것은 알겠는데 그래서 파라미터는 어떻게 줄이나요? 라고 한다면 계산 그래프에 대한 정의를 다시 생각해보자\n",
    "\n",
    "        ![6](./%EC%9D%B4%EB%AF%B8%EC%A7%80/image_18.png)\n",
    "\n",
    "        위는 완전연결 계층의 계산 그래프이다. 계산 그래프의 입력값에 가중치(매개변수)가 있음을 까먹지 말라! \n",
    "\n",
    "        이쁘게 정리하면 다음과 같다.\n",
    "\n",
    "        1. 입력에 따른 손실 함수를 구한다.\n",
    "        \n",
    "        2. 손실 함수를 줄이는 방법으로 경사법을 사용하자\n",
    "\n",
    "        3. 이때 손실 함수까지 형성된 계산 그래프를 역전파 시키면 손실 함수(dL/dL)를 줄이기 위한 각 매개변수(가중치일 경우 dL/dW)의 기울기가 나온다!!\n",
    "\n",
    "        4. 나온 기울기를 바탕으로 경사법을 이용해 손실 함수를 줄인다\n",
    "\n",
    "        ![7](./%EC%9D%B4%EB%AF%B8%EC%A7%80/image_19.jpg)\n",
    "        \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "model = FashionCNN()\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # 손실함수는 교차 엔트로피로 함\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate) # 아담으로 옵티마이저 설정\n",
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. 모델 학습하는 함수 만들기\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, crtierion, optimizer, device, save_path, num_epochs=5):\n",
    "    since = time.time()\n",
    "    acc_history = [] # 각 에폭마다 정확도 저장\n",
    "    loss_history = [] # 각 에폭마다 손실함수값 저장\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        running_loss = 0.0 # 손실 함수\n",
    "        running_corrects = 0 # 정답갯수\n",
    "\n",
    "        for inputs, labels in dataloaders:\n",
    "            inputs.requires_grad_(True)\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            model.to(device)\n",
    "\n",
    "            outputs = model(inputs) # 순전파\n",
    "            loss = crtierion(outputs, labels) # 손실 함수 구하기\n",
    "            optimizer.zero_grad() # 기울기를 0으로 설정\n",
    "            _, preds = torch.max(outputs, 1) # 결과값 추출\n",
    "            loss.backward() # 역전파 실행 이때 requires_grad = True가 된 완전연결층만 역전파가 됨\n",
    "            optimizer.step() # 기울기 업데이트\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0) # 출력 결과와 레이블의 오차를 계산한 결과를 누적하여 저장한다\n",
    "            # loss.item() 으로 손실이 갖고 있는 스칼라 값을 가져올 수 있습니다.\n",
    "            running_corrects += torch.sum(preds == labels.data) # 출력 결과와 레이블이 동일한지 확인한 결과를 누적하여 저장\n",
    "\n",
    "        epoch_loss = running_loss / len(dataloaders.dataset)\n",
    "        epoch_acc = running_corrects.double() / len(dataloaders.dataset)\n",
    "\n",
    "        print('Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss,epoch_acc)) # 참고) {:.소수점 자리수f} 포맷 코드 : {} 내에 실수의 소수점 자리수(.소수점 자리수f)를 지정해 줄 수 있음 소수점 4자리 까지 표시함\n",
    "        if epoch_acc > best_acc: # 만약에 어떤 에폭에서의 정확도가 최고 정확도보다 높을 경우 업데이트\n",
    "            best_acc = epoch_acc\n",
    "        \n",
    "        acc_history.append(epoch_acc.item())\n",
    "        loss_history.append(epoch_loss)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since # 실행 시간 계산\n",
    "    print(f'실행 시간 : {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'이 모델의 최고 정확도: {best_acc:4f}') # 최고 정확도\n",
    "\n",
    "    torch.save(model.state_dict(),save_path) # 모든 에폭을 소진할 때 모델의 상태를 저장 \n",
    "\n",
    "    return acc_history, loss_history\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. 모델 평가하는 함수 만들기\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model,dataloaders,device,save_path):\n",
    "    since = time.time()\n",
    "    acc_history = []\n",
    "    best_acc = 0.0\n",
    "\n",
    "    model.load_state_dict(torch.load(save_path))\n",
    "    model.eval() # 모델을 train에서 evaluation으로 변경 https://bluehorn07.github.io/2021/02/27/model-eval-and-train.html\n",
    "    model.to(device)\n",
    "    running_corrects = 0\n",
    "\n",
    "    for inputs, labels in dataloaders:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        with torch.no_grad(): # 자동미분을 사용하지 않음 (학습 목적이 아닌 테스트 목적)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "        epoch_acc = running_corrects.double() / len(dataloaders.dataset)\n",
    "        print('Acc: {:.4f}'.format(epoch_acc))\n",
    "\n",
    "        if epoch_acc > best_acc: # 만약에 어떤 에폭에서의 정확도가 최고 정확도보다 높을 경우 업데이트\n",
    "            best_acc = epoch_acc\n",
    "        \n",
    "        acc_history.append(epoch_acc.item())\n",
    "        print()\n",
    "    \n",
    "    time_elapsed = time.time() - since # 실행 시간 계산\n",
    "    print(f'Validation complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f}') # 최고 정확도\n",
    "\n",
    "    return acc_history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. 내가 나 쓸려고 만든 모델 관리자!!\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "라고 거창하게 시작하긴 했는데.. 에폭마다 저장하는 모델의 경우도 구현할려니 시간이 너무 오래 걸린다.\n",
    "\n",
    "나중에 한가하면 만들어보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_Manager:\n",
    "    def __init__(self, model, dataloader, crtierion, optimizer, device, model_name=\"model\"):\n",
    "        self.model = model\n",
    "        self.dataloader = dataloader\n",
    "        self.crtierion = crtierion\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.model_name = model_name\n",
    "\n",
    "        print(\"모델 매니저 사용 방법\")\n",
    "        print(\"(필수) 1. set_model_save(path, how_to_save = None)를 실행해서 저장 방법을 선택합니다\")\n",
    "        print(\"train_model(num_epochs = 5) : 모델을 평가합니다.\")\n",
    "        print(\"test_model() : 모델을 테스트합니다\")\n",
    "        print(\"train_model_result_graph() : 모델의 훈련 정확도 그래프를 그려줍니다\")\n",
    "        print(\"test_model_reult_graph() : 모델의 테스트 정확도 그래프\")\n",
    "\n",
    "\n",
    "    def set_model_save(self, path, how_to_save = None): # 모델을 저장하는 함수\n",
    "        if how_to_save == None:\n",
    "            print(\"모델을 저장하지 않습니다.\")\n",
    "            self.how_to_save = how_to_save\n",
    "        elif how_to_save == \"save_model\":\n",
    "            print(\"모델을 저장합니다\")\n",
    "            self.how_to_save = how_to_save\n",
    "            self.path = path\n",
    "        elif how_to_save == \"save_model_state_dict\":\n",
    "            print(\"모델의 상태를 내부 상태 사전(model_state_dict)으로 저장합니다\")\n",
    "            self.how_to_save = how_to_save\n",
    "            self.path = path\n",
    "        elif how_to_save == \"save_epoch_model\":\n",
    "            print(\"에폭마다 모델을 내부 상태 사전(model_state_dict)으로 저장하겠습니다.\")\n",
    "            print(\"모델을 훈련할때 epoch_train_model 로 실행해 주세요\")\n",
    "            print(\"모델을 테스트할때 epoch_test_model 로 실행해 주세요\")\n",
    "            self.how_to_save = how_to_save\n",
    "            self.path = path\n",
    "        else:\n",
    "            print(\"모델의 저장 방법이 틀렸습니다\")\n",
    "            print(\"모델에 저장 방법은 3개가 있습니다\")\n",
    "            print(\"다음의 모델 저장 방식을 따라주세요\")\n",
    "            print(\"----------------------------------\")\n",
    "            print(\"1.None : 모델을 저장하지 않습니다. 디폴트 값입니다.\")\n",
    "            print(\"2.save_model : 모델을 저장합니다.\")\n",
    "            print(\"3.save_model_state_dict : 모델의 상태를 내부 상태 사전(model_state_dict)으로 저장합니다.\")\n",
    "            print(\"(미구현).save_epoch_model : 에폭마다 모델을 내부 상태 사전(model_state_dict)으로 저장하겠습니다.\")\n",
    "\n",
    "            \n",
    "    def save_model(self):\n",
    "        torch.save(model, self.path + f\"/{self.model_name}_state_dict.pt\")\n",
    "        path = self.path + f\"/{self.model_name}_state_dict.pt\"\n",
    "        self.path = path\n",
    "\n",
    "    def save_model_state_dict(self):\n",
    "        torch.save(model.state_dict(), self.path + f\"/{self.model_name}.pt\")\n",
    "        path = self.path + f\"/{self.model_name}.pt\"\n",
    "        self.path = path\n",
    "\n",
    "    def save_epoch_model(self, epoch, best_acc, train_acc_history, train_loss_history):\n",
    "        torch.save(\n",
    "                {\n",
    "                    \"epoch\" : epoch,\n",
    "                    \"model_state_dict\" : self.model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    \"best_acc\" : best_acc,\n",
    "                    \"train_acc_history\" : train_acc_history,\n",
    "                    \"train_loss_history\" : train_loss_history,\n",
    "                    \"description\": f\"CustomModel 체크포인트 - {epoch}\",\n",
    "                },\n",
    "                self.path + f\"/chekpoint - {epoch}.pt\"\n",
    "            )\n",
    "        path = self.path + f\"/chekpoint - {epoch}.pt\"\n",
    "        self.path = path\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "    def train_model(self, num_epochs = 5):\n",
    "        since = time.time()\n",
    "        self.train_acc_history = [] # 각 에폭마다 정확도 저장\n",
    "        self.train_loss_history = [] # 각 에폭마다 손실함수값 저장\n",
    "        best_acc = 0.0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "            print('-' * 10)\n",
    "\n",
    "            running_loss = 0.0 # 손실 함수\n",
    "            running_corrects = 0 # 정답갯수\n",
    "\n",
    "            for inputs, labels in self.dataloader:\n",
    "                inputs.requires_grad_(True)\n",
    "                inputs = inputs.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                self.model.to(self.device)\n",
    "\n",
    "                outputs = self.model(inputs) # 순전파\n",
    "                loss = self.crtierion(outputs, labels) # 손실 함수 구하기\n",
    "                self.optimizer.zero_grad() # 기울기를 0으로 설정\n",
    "                _, preds = torch.max(outputs, 1) # 결과값 추출\n",
    "                loss.backward() # 역전파 실행 이때 requires_grad = True가 된 완전연결층만 역전파가 됨\n",
    "                self.optimizer.step() # 기울기 업데이트\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0) # 출력 결과와 레이블의 오차를 계산한 결과를 누적하여 저장한다\n",
    "                # loss.item() 으로 손실이 갖고 있는 스칼라 값을 가져올 수 있습니다.\n",
    "                running_corrects += torch.sum(preds == labels.data) # 출력 결과와 레이블이 동일한지 확인한 결과를 누적하여 저장\n",
    "\n",
    "            epoch_loss = running_loss / len(self.dataloaders.dataset)\n",
    "            epoch_acc = running_corrects.double() / len(self.dataloaders.dataset)\n",
    "\n",
    "            print('Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss,epoch_acc)) # 참고) {:.소수점 자리수f} 포맷 코드 : {} 내에 실수의 소수점 자리수(.소수점 자리수f)를 지정해 줄 수 있음 소수점 4자리 까지 표시함\n",
    "            if epoch_acc > best_acc: # 만약에 어떤 에폭에서의 정확도가 최고 정확도보다 높을 경우 업데이트\n",
    "                best_acc = epoch_acc\n",
    "        \n",
    "            self.train_acc_history.append(epoch_acc.item())\n",
    "            self.train_loss_history.append(epoch_loss)\n",
    "\n",
    "            print()\n",
    "\n",
    "        time_elapsed = time.time() - since # 실행 시간 계산\n",
    "        print(f'실행 시간 : {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "        print(f'이 모델의 최고 정확도: {best_acc:4f}') # 최고 정확도\n",
    "\n",
    "        if self.how_to_save == \"save_model\":\n",
    "            self.save_model()\n",
    "        elif self.how_to_save == \"save_model_state_dict\":\n",
    "            self.save_model_state_dict()\n",
    "\n",
    "\n",
    "    def test_model(self):\n",
    "        since = time.time()\n",
    "        self.test_acc_history = []\n",
    "        best_acc = 0.0\n",
    "\n",
    "        if self.how_to_save == \"save_model\":\n",
    "            model.load_state_dict(torch.load(self.path))\n",
    "        elif self.how_to_save == \"save_model_state_dict\":\n",
    "            model = torch.load(self.path)\n",
    "\n",
    "\n",
    "        model.eval() # 모델을 train에서 evaluation으로 변경 https://bluehorn07.github.io/2021/02/27/model-eval-and-train.html\n",
    "        model.to(self.device)\n",
    "        running_corrects = 0\n",
    "\n",
    "        for inputs, labels in self.dataloaders:\n",
    "            inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "\n",
    "            with torch.no_grad(): # 자동미분을 사용하지 않음 (학습 목적이 아닌 테스트 목적)\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "            epoch_acc = running_corrects.double() / len(self.dataloaders.dataset)\n",
    "            print('Acc: {:.4f}'.format(epoch_acc))\n",
    "\n",
    "            if epoch_acc > best_acc: # 만약에 어떤 에폭에서의 정확도가 최고 정확도보다 높을 경우 업데이트\n",
    "                best_acc = epoch_acc\n",
    "        \n",
    "            self.test_acc_history.append(epoch_acc.item())\n",
    "            print()\n",
    "    \n",
    "        time_elapsed = time.time() - since # 실행 시간 계산\n",
    "        print(f'Validation complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "        print(f'Best val Acc: {best_acc:4f}') # 최고 정확도\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "'''\n",
    "    def epoch_train_model(self, num_epochs = 5):\n",
    "        since = time.time()\n",
    "        self.train_acc_history = [] # 각 에폭마다 정확도 저장\n",
    "        self.train_loss_history = [] # 각 에폭마다 손실함수값 저장\n",
    "        best_acc = 0.0\n",
    "        \n",
    "        checkpoint = torch.load(self.path)\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "        num_epoch = checkpoint[\"epoch\"]\n",
    "        self.train_acc_history = checkpoint[\"train_acc_history\"]\n",
    "        self.train_loss_history = \n",
    "        \n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "            print('-' * 10)\n",
    "\n",
    "            running_loss = 0.0 # 손실 함수\n",
    "            running_corrects = 0 # 정답갯수\n",
    "\n",
    "            for inputs, labels in self.dataloaders:\n",
    "                inputs.requires_grad_(True)\n",
    "                inputs = inputs.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                self.model.to(self.device)\n",
    "\n",
    "                outputs = self.model(inputs) # 순전파\n",
    "                loss = self.crtierion(outputs, labels) # 손실 함수 구하기\n",
    "                self.optimizer.zero_grad() # 기울기를 0으로 설정\n",
    "                _, preds = torch.max(outputs, 1) # 결과값 추출\n",
    "                loss.backward() # 역전파 실행 이때 requires_grad = True가 된 완전연결층만 역전파가 됨\n",
    "                self.optimizer.step() # 기울기 업데이트\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0) # 출력 결과와 레이블의 오차를 계산한 결과를 누적하여 저장한다\n",
    "                # loss.item() 으로 손실이 갖고 있는 스칼라 값을 가져올 수 있습니다.\n",
    "                running_corrects += torch.sum(preds == labels.data) # 출력 결과와 레이블이 동일한지 확인한 결과를 누적하여 저장\n",
    "\n",
    "            epoch_loss = running_loss / len(self.dataloaders.dataset)\n",
    "            epoch_acc = running_corrects.double() / len(self.dataloaders.dataset)\n",
    "\n",
    "            print('Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss,epoch_acc)) # 참고) {:.소수점 자리수f} 포맷 코드 : {} 내에 실수의 소수점 자리수(.소수점 자리수f)를 지정해 줄 수 있음 소수점 4자리 까지 표시함\n",
    "            if epoch_acc > best_acc: # 만약에 어떤 에폭에서의 정확도가 최고 정확도보다 높을 경우 업데이트\n",
    "                best_acc = epoch_acc\n",
    "        \n",
    "            self.train_acc_history.append(epoch_acc.item())\n",
    "            self.train_loss_history.append(epoch_loss)\n",
    "\n",
    "            self.save_epoch(epoch,best_acc,self.train_acc_history,self.train_loss_history)\n",
    "\n",
    "            print()\n",
    "\n",
    "        time_elapsed = time.time() - since # 실행 시간 계산\n",
    "        print(f'실행 시간 : {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "        print(f'이 모델의 최고 정확도: {best_acc:4f}') # 최고 정확도\n",
    "\n",
    "'''\n",
    "\n",
    "def train_model_result_graph(self):\n",
    "    print(\"모델의 훈련 정확도 그래프\")\n",
    "    plt.plot(self.train_acc_history)\n",
    "    print()\n",
    "    print(\"모델의 훈련 손실함수 그래프\")\n",
    "    plt.plot(self.train_loss_history)\n",
    "    \n",
    "\n",
    "def test_model_reult_graph(self):\n",
    "    print(\"모델의 테스트 정확도 그래프\")\n",
    "    plt.plot(self.test_acc_history)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "근데 모든 모델이 동일한 구조를 가질거란 보장도 없는데\n",
    "\n",
    "이렇게 틀을 짜는게 얼마나 의미없다고 안 것은\n",
    "\n",
    "코드를 작성한 5시간 뒤의 일이다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch_studing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0 (default, Nov 15 2020, 08:30:55) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d060f0828fc2b117241290c294bd7cf3e183c40a44f56e1a2627712ee0c7af9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
